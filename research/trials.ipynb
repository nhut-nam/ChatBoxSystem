{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3608fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2496c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93391bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"C:\\\\Users\\\\namnh\\\\Downloads\\\\data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18628c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='9c180a63-e4a8-47eb-b65a-34a3be5da606', embedding=None, metadata={'page_label': '1', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56cceacc-8912-44d4-8eca-69767d94b956', embedding=None, metadata={'page_label': '2', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5630079f-8fe7-42a6-914b-923cc01ba51e', embedding=None, metadata={'page_label': '3', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6b8cb7e0-bf6f-443e-9d56-f8d63b6c8209', embedding=None, metadata={'page_label': '4', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d669ea1f-f069-4032-a21a-8feff943db20', embedding=None, metadata={'page_label': '5', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='baf317d0-3b8b-4851-bb5c-6f14558f072b', embedding=None, metadata={'page_label': '6', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d24337ca-6b5b-453c-ae4a-a2ce820dd4c9', embedding=None, metadata={'page_label': '7', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a15403ea-c785-495c-bebe-25d2f76d218a', embedding=None, metadata={'page_label': '8', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='28881339-563d-4ae8-866d-ac18e5a9e855', embedding=None, metadata={'page_label': '9', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9afa4f33-5a38-46cb-a8e4-8f375d770231', embedding=None, metadata={'page_label': '10', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='42ed2e6d-9d53-47e3-98bc-568014ff7006', embedding=None, metadata={'page_label': '11', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e76485dd-1982-4051-b97a-26eaacea0b5e', embedding=None, metadata={'page_label': '12', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4bffee2a-0335-4de8-851e-676eea8b6d71', embedding=None, metadata={'page_label': '13', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='06539f59-fa50-49b1-b50c-f3cd93ee2a87', embedding=None, metadata={'page_label': '14', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='07d1373f-e3cc-488b-9a80-91dbbeba73aa', embedding=None, metadata={'page_label': '15', 'file_name': 'Transformer.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\Transformer.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9ad1a38b-a232-4594-b331-6ee45ca32d43', embedding=None, metadata={'page_label': '1', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Accepted: 2 May 2025\\n© The Author(s) 2025\\nExtended author information available on the last page of the article\\nYOLO advances to its genesis: a decadal and comprehensive \\nreview of the You Only Look Once (YOLO) series\\nRanjan\\xa0Sapkota1 \\xa0· Marco\\xa0Flores-Calero2\\xa0· Rizwan\\xa0Qureshi3\\xa0· Chetan\\xa0Badgujar4\\xa0· \\nUpesh\\xa0Nepal5\\xa0· Alwin\\xa0Poulose6\\xa0· Peter\\xa0Zeno7\\xa0· Uday Bhanu Prakash\\xa0Vaddevolu8\\xa0· \\nSheheryar\\xa0Khan9\\xa0· Maged\\xa0Shoman10\\xa0· Hong\\xa0Yan11,12\\xa0· Manoj\\xa0Karkee1,13\\nArtificial Intelligence Review          (2025) 58:274 \\nhttps://doi.org/10.1007/s10462-025-11253-3\\nAbstract\\nThis review systematically examines the progression of the You Only Look Once (YOLO) \\nobject detection algorithms from YOLOv1 to the recently unveiled YOLOv12. Employ -\\ning a reverse chronological analysis, this study examines the advancements introduced \\nby YOLO algorithms, beginning with YOLOv12 and progressing through YOLO11 (or \\nYOLOv11), YOLOv10, YOLOv9, YOLOv8, and subsequent versions to explore each \\nversion’s contributions to enhancing speed, detection accuracy, and computational effi -\\nciency in real-time object detection. Additionally, this study reviews the alternative ver -\\nsions derived from YOLO architectural advancements of YOLO-NAS, YOLO-X, YOLO-\\nR, DAMO-YOLO, and Gold-YOLO. Moreover, the study highlights the transformative \\nimpact of YOLO models across five critical application areas: autonomous vehicles and \\ntraffic safety, healthcare and medical imaging, industrial manufacturing, surveillance and \\nsecurity, and agriculture. By detailing the incremental technological advancements in sub -\\nsequent YOLO versions, this review chronicles the evolution of YOLO, and discusses the \\nchallenges and limitations in each of the earlier versions. The evolution signifies a path \\ntowards integrating YOLO with multimodal, context-aware, and Artificial General Intel -\\nligence (AGI) systems for the next YOLO decade, promising significant implications for \\nfuture developments in AI-driven applications.\\nKeywords You Only Look Once · YOLO · YOLOv1 to YOLOv12 · YOLO \\nconfigurations · CNN · Deep learning · Real-time object detection · Artificial \\nintelligence · Computer vision · Healthcare and medical imaging · Autonomous \\nvehicles · Traffic safety · Industrial manufacturing · Surveillance · Agriculture\\n1 Introduction\\nObject detection is a critical component of computer vision systems, which enables auto -\\nmated systems to identify and locate objects of interest within images or video frames (Liu \\net al. 2020; Badgujar et al. 2024; Ahmad and Rahimi 2022; Gheorghe et al. 2024; Arkin et al. \\n\\xa0et\\xa0al.\\xa0[full author details at the end of the article]\\n1 3\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='384d770b-317a-4317-9b79-ea536d87d78a', embedding=None, metadata={'page_label': '2', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\n2023). Real-time object detection has become integral to numerous applications requiring \\nreal- and near-real-time analysis, monitoring and interaction with dynamic environments \\nsuch as agriculture, transportation, education, and health-care (Fernandez et al. 2016; Wang \\net al. 2018; Ren et al. 2015; Tang et al. 2024; Chen and Guan 2022; Ragab et al. 2024). \\nFor instance, real-time object detection is the foundational technology for the success of \\nautonomous vehicles and robotic systems (Flippo et al. 2023; Malligere Shivanna and Guo \\n2024; Flores-Calero et al. 2024), allowing the system to quickly recognize and track differ-\\nent objects of interest such as vehicles, pedestrians, bicycles, and other obstacles, enhanc -\\ning navigational safety and efficiency (Guerrero-Ibáñez et al. 2018; Shoman et al. 2024a). \\nThe utility of object recognition extends beyond vehicular applications, and is also pivotal \\nin action recognition within video sequences, useful in digital surveillance, monitoring, \\nsports analysis, cityscapes (Hnewa and Radha 2023) and human–machine interaction (Hus-\\nsain and Zeadally 2018; Fernandez et al. 2016; Shoman et al. 2024b). These areas benefit \\nfrom the capability to analyze and respond to situational dynamics in real time, illustrating \\nits broad applicability, acceptance, and impact. However, the problem of object detection \\ninvolves several challenges:\\n ● Complexity of real-world environments: Real-world environments/scenes are highly \\nvariable and unpredictable. Objects can appear in various orientations, scales, distances \\nand lighting conditions, making it difficult for a detection algorithm to generalize and \\nmaintain accuracy in real time (Kaushal et al. 2018).\\n ● Illumination factors: Illumination plays a crucial role in object detection, as factors \\nlike lighting intensity, direction, shadows, and glare can significantly affect perfor -\\nmance (Xiang et al. 2014; Xiao et al. 2020). Non-uniform or low light, color tempera -\\nture changes, and dynamic lighting variations can obscure object features or cause false \\ndetections. Solutions include controlled lighting setups, preprocessing techniques like \\nnormalization and color correction, and training models with diverse, augmented data -\\nsets to enhance robustness (Seoni et al. 2024).\\n ● Occlusions and clutter: Objects may be partially or fully obscured by other objects, \\ncreating cluttered scenes that result in incomplete information, which requires careful \\ninterpretation for accurate analysis (Khan and Shah 2008; Mostafa et al. 2022).\\n ● Speed and efficiency: Many applications necessitate rapid processing of visual data to \\nenable timely decision-making. This requires detection algorithms to achieve a balance \\nbetween high accuracy and low latency, ensuring that the systems can deliver efficient \\nand reliable results in real- or near-real-time scenarios, such as autonomous vehicles and \\ntraffic safety, healthcare and medical imaging, industrial manufacturing, security and \\nsurveillance and agricultural automation (Gupta et al. 2021).\\nAddressing these challenges, as discussed below, required innovative techniques, which \\nconventionally relied on hand-crafted features and classical machine learning methods. \\nLater, the focus shifted towards automated feature learning and end-to-end deep learning \\nmethods.\\n1 3\\n  274  Page 2 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4e4ab656-ee2a-45e3-8a12-dcc303422cff', embedding=None, metadata={'page_label': '3', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\n1.1 Traditional object detection approaches\\nBefore the advent of deep learning, object detection relied on a combination of hand-crafted \\nfeatures and machine learning classifiers (Zou et al. 2023). Some of the notable traditional \\nmethods include:\\n ● Correlation filters: Used to detect objects by correlating a filter with the image, such as \\nmatching templates (Park et al. 2019). These approaches struggle with variations in the \\nappearance of objects and lighting conditions (Liu et al. 2021).\\n ● Sliding window approach: This method involves moving a fixed-size window across \\nthe image and applying a classifier to each window to determine whether it contains an \\nobject (Teutsch and Kruger 2015). However, it struggles with varying object sizes and \\naspect ratios, which can lead to inaccurate detections and a high computational cost due \\nto the exhaustive search involved.\\n ● Viola–Jones detector: The Viola-Jones detector, introduced in 2001, uses Haar-like fea-\\ntures (Lienhart and Maydt 2002) and a cascade of AdaBoost trained classifiers (Jun-\\nFeng and Yu-Pin 2009) to detect objects in images efficiently (Li et al. 2012).\\nSupporting these methods are various hand-crafted feature extraction techniques, including:\\n ● Gabor features: Extracted texture features using Gabor filters, which are effective for \\ntexture representation but can be computationally intensive (Hu et al. 2020).\\n ● Histogram of oriented gradients (HOG): Captures edge or gradient structures that char-\\nacterize the shape of objects, typically combined with Support Vector Machines (SVM) \\nfor classification (Surasak et al. 2018).\\n ● Local binary patterns (LBP): Utilizes pixel intensity comparisons to form a binary pat -\\ntern, used in texture classification and face recognition (Karis et al. 2016).\\n ● Haar-like features: These features consider adjacent rectangular regions in a detection \\nwindow, sum up the pixel intensities in each region, and calculate the difference be -\\ntween the sums. This difference is then used to categorize sub-regions in images (Mita \\net al. 2005).\\n ● DPM (Deformable part models): DPM (Yan et al. 2014) represents objects as a collec-\\ntion of deformable parts arranged in a spatial structure. It proved particularly effective \\nfor detecting objects under occlusions, pose variations, and cluttered backgrounds.\\n ● SIFT: Scale-Invariant Feature Transform (SIFT) is a robust method for detecting and \\ndescribing local features in images (Piccinini et al. 2012). Beyond feature extraction, \\nit has been effectively used for object detection by matching keypoints between input \\nimages and reference templates, leveraging its invariance to scale, rotation, and illumi-\\nnation changes.\\n ● SURF: speeded-up robust features A faster alternative to SIFT, SURF detects and de -\\nscribes features using an efficient Hessian matrix approximation, making it suitable for \\nreal-time object detection tasks (Li and Zhang 2013).\\n1 3\\nPage 3 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7a50d88d-aafa-4c89-ab11-5210bea8bffb', embedding=None, metadata={'page_label': '4', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\n1.1.1 Classification techniques used in traditional object detection methods\\nSome of the most commonly employed classification methods for these traditional object \\ndetectors include Support Vector Machine (SVM), statistical classifiers (e.g., Bayesian Clas-\\nsifier) and ensemble methods (e.g. Adaboost, Random Forest) and Multilayer Perceptrons \\n(MLP) Neural Networks (Chiu et al. 2020; Mienye and Sun 2022).\\nThese traditional methods in early computer vision systems, reliant on hand-crafted \\nfeatures and classical classifiers, offered moderate success under controlled conditions but \\nstruggled with robustness and generalization in diverse real-world scenarios, lacking the \\naccuracy achieved by modern deep learning techniques (Xiang et al. 2014). Figure 1 shows \\nFig. 1 Timeline of object detection paradigms and evolution of YOLO models. The figure shows the \\nprogression from traditional methods like VJ Detector and HOG to deep learning-based approaches, \\nincluding R-CNN, Fast R-CNN, Faster R-CNN, and YOLO series. Recent advancements also highlight \\ntransformer-based models, such as DETR (Detection Transformer) (Carion et al. 2020) and ViTDet (Vi-\\nsion Transformer for Detection) (Li et al. 2022), which have demonstrated significant progress in object \\ndetection tasks\\n \\n1 3\\n  274  Page 4 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='87e8b4c8-e3a2-44a1-ad86-1eda8f5ae491', embedding=None, metadata={'page_label': '5', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nthe historical development of computer vision systems emphasizing how object detection \\nalgorithms evolved.\\n1.2 Emergence of convolutional neural networks\\nAfter 2010, the performance of handcrafted features plateaued, leading to saturation in object \\ndetection research. However, in 2012, the world witnessed the emergence of convolutional \\nneural networks (CNNs), marking a significant turning point in the field (Krizhevsky et al. \\n2012; Xie et al. 2021; Tang and Yuan 2015; Zhiqiang and Jun 2017). As a deep convolutional\\nnetwork, CNNs are able to learn robust and high-level feature representations of an \\nimage, and are particularly effective because:\\n ● Hierarchical feature learning: CNNs learn to extract low-level features (e.g., edges, tex-\\ntures) in early layers and high-level features (e.g., object parts, shapes) in deeper layers, \\nfacilitating robust object representation (Li et al. 2020).\\n ● Spatial invariance: Convolutional layers enable CNNs to recognize objects regardless of \\ntheir position within the image, enhancing detection robustness (Crawford and Pineau \\n2019).\\n ● Scalability and generalizability: CNNs can be scaled to handle larger datasets and more \\ncomplex models, improving performance and robustness on a wide range of tasks and \\napplication environments  (Tan et al. 2020).\\nHowever, CNNs can not be directly applied to the object detection task, due to varying num-\\nbers of objects, varying sizes, aspect ratios, and orientation. CNNs were primarily designed \\nfor image classification, meaning they output a single label for the entire image (Arkin et al. \\n2023). Whereas object detection tasks require not only classifying the object but also local-\\nizing it in the image, i.e., identifying the position of the object through bounding boxes.\\n1.3 Timeline of object detection paradigms and the evolution of YOLO models\\nOne of the earliest deep learning-based object detectors was R-CNN, introduced in 2014 by \\nGirshick et al. (2014). It marked a pivotal milestone in the development of detection mod -\\nels, breaking the stagnation in object detection by introducing Regions with CNN features \\n(R-CNN). This groundbreaking approach revolutionized the field, sparking rapid advance -\\nments and accelerating the evolution of object detection at an unprecedented pace.\\nThe idea behind R-CNN is simple, it uses the selective search algorithm to generate \\nabout 2000 region proposals, which are then processed by a CNN to extract features (Gir-\\nshick et al. 2014). Finally, linear SVM classifiers are utilized to detect objects within each \\nregion and identify their respective categories. While R-CNN achieved significant progress, \\nit has notable drawbacks: the redundant feature computations across a large number of over-\\nlapping proposals (over 2,000 boxes per image) result in extremely slow detection speeds, \\ntaking 14 s per image even with GPU acceleration (Xie et al. 2021).\\nAfter that, Fast R-CNN, introduced in 2015, improved object detection by addressing the \\nredundant feature computations across numerous overlapping proposals (Bhat et al. 2023). \\nIt integrated region proposal feature extraction and classification into a single pass, sig -\\nnificantly enhancing efficiency and speed compared to previous methods like R-CNN (Gir-\\n1 3\\nPage 5 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d71296ae-f952-43bc-baf6-0a34805961c0', embedding=None, metadata={'page_label': '6', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nshick 2015). Building on this, Faster R-CNN advanced the approach further by introducing \\nRegion Proposal Networks (RPNs), enabling end-to-end training. This innovation elimi -\\nnated the reliance on selective search, reducing computational complexity and streamlining \\nthe pipeline, thus allowing for faster and more accurate object detection without the need for \\nexternal proposal generation (Ren et al. 2015; Mostafa et al. 2022).\\nLater, the Single Shot Multibox Detector (SSD) (Liu et al. 2016), introduced at the begin-\\nning of 2016, discretizes bounding boxes into predefined default boxes of various scales and \\naspect ratios. It predicts object scores and adjusts box shapes accordingly. By leveraging \\nmulti-scale feature maps, SSD handles objects of different sizes effectively. Unlike earlier \\nmethods, it eliminates the need for a separate proposal generation step, simplifying the \\ntraining process and improving performance, particularly in detecting small objects.\\nThese breakthroughs laid the foundation for numerous subsequent advancements in the \\nfield. Over the years, several other detectors have been developed, and by 2024, the YOLO \\nseries has become a dominant force in the domain, continuously evolving to improve speed, \\naccuracy, and efficiency in real-time object detection tasks.\\nFig. 1 presents a chronological overview of deep learning-based detectors, categorized \\ninto YOLO and others. Traditional detectors, shown on the right side, as well as Trans -\\nformer-based detectors, are included for visual comparison.\\n1.4 You Only Look Once approach\\nThe “You Only Look Once” (YOLO) object detection algorithm was first introduced by \\nRedmon et al. (2016) in 2015, revolutionized real-time object detection by combining region \\nproposal and classification into a single neural network, significantly reducing computation \\ntime. YOLO’s unified architecture divides the image into a grid, predicting bounding boxes \\nand class probabilities directly for each cell, enabling end-to-end learning (Redmon et al. \\n2016). YOLOv1 utilized a simplified CNN backbone, setting the stage with basic bound -\\ning box predictions. Subsequent versions like YOLOv2 incorporated the DarkNet-19 back-\\nbone and refined anchor boxes through K-means clustering. YOLOv3 expanded this further \\nwith a DarkNet-53 architecture, integrating multi-scale detection and residual connections. \\nThe series continued to innovate with YOLOv4 implementing CSPDarkNet-53 and PANet \\nalongside mosaic data augmentation. YOLOv5 and YOLOv6 introduced CSPNet with \\ndynamic anchor refinement and further enhancements in PANet, respectively. YOLOv7 fea-\\ntured an EfficientRep backbone with dynamic label assignment, while YOLOv8 introduced \\na Path Aggregation Network with Dynamic Kernel Attention. YOLOv9 developed multi-\\nlevel auxiliary feature extraction, and YOLOv10 optimized the system with a lightweight \\nclassification head and distinct spatial and channel transformations. YOLOv11 introduced \\nthe C3k2 block in its backbone and utilized C2PSA for improved spatial attention. The lat-\\nest, YOLOv12, marks a significant shift towards an attention-centric design, introducing the \\nArea Attention (A2) module for efficient large receptive field processing, Residual Efficient \\nLayer Aggregation Networks (R-ELAN) for enhanced feature aggregation, and architec -\\ntural optimizations including FlashAttention and adjusted MLP ratios. This attention-based \\napproach allows YOLOv12 to achieve state-of-the-art performance in both accuracy and \\nefficiency, surpassing previous CNN-based models while maintaining real-time detection \\ncapabilities (Tian et al. 2025). autonomous vehicles and traffic safety (Gheorghe et al. 2024; \\nWang et al. 2024; Shoman et al. 2024a, c), healthcare (Patel et al. 2023; Luo et al. 2021; \\n1 3\\n  274  Page 6 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='15d81d83-de61-4987-9469-55f349346747', embedding=None, metadata={'page_label': '7', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nSalinas-Medina and Neme 2023), industrial applications (Pham et al. 2023; Klarák et al. \\n2024; Wang et al. 2024), surveillance and security (Arroyo et al. 2019; Bordoloi et al. 2020) \\nand agriculture (Badgujar et al. 2024; Li et al. 2022; Fu et al. 2021; Zhong et al. 2018; Wang \\net al. 2022; Jiang et al. 2022; Chen et al. 2023; Yu et al. 2024; Jia et al. 2023; Umar et al. \\n2024; Sapkota et al. 2024a), where accuracy and speed are crucial.\\nIn autonomous vehicles and traffic safety, YOLO has been widely adopted to improve \\nsafety in both road transport and aviation, allowing real-time object detection for colli -\\nsion avoidance, traffic monitoring, pedestrian detection, traffic sign detection, and aviation \\nhazard analysis. This application aids in minimizing accidents and improving operational \\nefficiency (Gheorghe et al. 2024; Bakirci and Bayraktar 2024; Wang et al. 2024).\\nIn healthcare, YOLO has been instrumental in assisting and improving diagnostic pro -\\ncesses and treatment outcomes. The applications include, but are not limited to, cancer \\ndetection (Prinzi et al. 2024; Aly et al. 2021), skin segmentation (Ünver and Ayan 2019), \\nand pill identification (Tan et al. 2021; Suksawatchon et al. 2022) which showcase the mod-\\nel’s ability to adapt to different needs, and essential tasks.\\nIn industrial applications, YOLO aids in surface inspection processes to detect defects \\nand anomalies (Pham et al. 2023; Klarák et al. 2024), structural health monitoring (Pratibha \\net al. 2023), reliability and safety (Fahim and Hasan 2024) ensuring quality control in manu-\\nfacturing and production (Pham et al. 2023).\\nSurveillance and security systems also leverage YOLO for real-time monitoring and \\nrapid identification of suspicious activities (Arroyo et al. 2019; Bordoloi et al. 2020). By \\nintegrating these models into surveillance systems, security personnel can more effectively \\nmonitor and respond to potential threats, enhancing public safety (Gorave et al. 2020). Simi-\\nlarly, in the context of public health measures like social distancing and face mask detection \\nduring pandemics (Kolpe et al. 2022; Bashir et al. 2023), YOLO models provided essential \\nsupport in enforcing health regulations.\\nIn agriculture, YOLO models have been applied to detect and classify crops (Ajayi et al. \\n2023), pests, and diseases (Morbekar et al. 2020; Li et al. 2022; Cheeti et al. 2021), facilitat-\\ning precision agriculture techniques and automating farming operations to increase produc-\\ntivity and optimizing inputs. Additionally, in remote sensing, YOLO contributes to object \\nrecognition in satellite (Pham et al. 2020; Cheng et al. 2021) and aerial imagery (Chen et al. \\n2023; Luo et al. 2022), which supports urban planning, land use mapping, and environmen-\\ntal monitoring. These capabilities demonstrate YOLO’s contribution to critical global chal-\\nlenges such as urban development and environmental conservation.\\n1.5 Motivation and organization of the study\\nSince “You Only Look Once” has been widely adopted in the field of computer vision, a \\nsearch for this keyword in Google Scholar yields approximately 5,550,000 results as of \\nJune 9, 2024. The acronym “YOLO” further emphasizes its popularity, generating around \\n210,000 search results at the same time instant. Thousands of researchers have cited YOLO \\npapers, highlighting its significant influence. This study aims to review and critically sum -\\nmarize the YOLO’s decadal progress and its advancements over time, as visually summa -\\nrized in the mind-map, shown in Fig. 2.\\nThis comprehensive analysis starts with Sect. 2: YOLO trajectory, tracing the evolution \\nfrom YOLOv1 to YOLOv12. In Sect. 3: Context and distinctions of prior YOLO literature \\n1 3\\nPage 7 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='74bc64d7-33fa-4ecf-936b-ae79668a02e6', embedding=None, metadata={'page_label': '8', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\noffers insights into the background and unique aspects of earlier studies. Section 4: Review \\nof YOLO versions  details the key features and improvements of each version. In Sect. 5: \\nApplications various use cases across different domains are highlighted, demonstrating \\nthe versatility of YOLO models. Following this, Sect. 6 Challenges, limitations and future \\ndirections addresses current issues and potential advancements. Finally, the Conclusion sec-\\ntion summarizes the findings of this comprehensive review. Each section is further divided \\ninto various sub-subsections to present and discuss specific topic areas relevant to the cor -\\nresponding sections.\\n2 The evolution of YOLO: trajectory and variants\\nFig. 1 illustrates the development timeline of the YOLO models, beginning with the release \\nof YOLOv1 and progressing to the latest version, YOLO12. This timeline highlights the key \\nadvancements and iterations in the YOLO series.\\nYOLOv1 (Redmon et al. 2016) was introduced in 2016 as a novel approach to object \\ndetection, offering good accuracy and computational speed by processing images using a \\nsingle stage network architecture. The first YOLO version laid the foundation for real-time \\napplications of machine vision systems, setting a new standard for subsequent developments.\\nYOLOv2, or YOLO9000 (Li and Yang 2018; Nakahara et al. 2018), expanded on the \\nfoundation of YOLOv1 by improving the resolution at which the model operated and by \\nexpanding the capability to detect over 9000 object categories, thus enhancing its versatility \\nand accuracy. YOLOv2 introduced two primary variants: a smaller version optimized for \\nspeed and a larger version focused on higher accuracy.\\nFig. 2 A schematic of the manuscript. We discuss all versions of YOLO, with comparative analysis. Ap-\\nplications in key areas; such as, autonomous vehicles, and security and surveillance are also presented. \\nChallenges in each YOLO version, with performance enhancements, are also highlighted, we also provide \\nvisionary thoughts on the future impact of YOLO on industry and society\\n \\n1 3\\n  274  Page 8 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='89d98d62-8e34-42a4-8fa5-54aaef62e3bf', embedding=None, metadata={'page_label': '9', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nYOLOv3 further advanced these capabilities by implementing multi-scale predictions \\nand a deeper network architecture, which allowed better detection of smaller objects (Kim \\net al. 2018). YOLOv3 introduced three primary variants, each designed to balance model \\nsize and performance: YOLOv3-spp (Small), Standard, and YOLOv3-tiny(Tiny), catering \\nto different trade-offs between speed and accuracy.\\nThe series continued to evolve with YOLOv4 and YOLOv5, each introducing more \\nrefined techniques and optimizations to improve detection performance (i.e., accuracy \\nand speed) even further (Nepal and Eslamiat 2022; Sozzi et al. 2022; Mohod et al. 2022). \\nYOLOv4 introduced four main variants: the standard version, YOLOv4-CSP, which incor-\\nporates Cross-Stage Partial (CSP) networks to enhance performance and reduce com -\\nputational cost; YOLOv4x-mish, which utilizes the Mish activation function to improve \\naccuracy while maintaining efficiency; and YOLOv4-tiny, a lightweight version optimized \\nfor real-time applications and edge devices, sacrificing some accuracy for speed. YOLOv5, \\ndeveloped by Ultralytics ( 2020) 1, brought significant improvements in terms of ease of \\nuse and performance, establishing itself as a popular choice in the computer vision com -\\nmunity. YOLOv5 introduced five primary variants to meet various performance needs: \\nYOLOv5s (small), optimized for speed and efficiency in resource-constrained environ -\\nments; YOLOv5m (medium), offering a balanced trade-off between speed and accuracy; \\nYOLOv5l (large), designed for higher accuracy at the expense of resources; YOLOv5x \\n(extra-large), focused on top-tier accuracy for powerful hardware; and YOLOv5n (nano), a \\nlightweight version tailored for rapid inference and low computational demands, ideal for \\nreal-time applications and edge devices.\\nSubsequent versions, YOLOv6 through YOLO11, have continued to build on this suc -\\ncess, focusing on enhancing model scalability, reducing computational demands, and \\nimproving real-time performance metrics.\\nLi et al. ( 2022) introduced YOLOv6 in 2022. Developed by a team from Meituan, a \\nChinese e-commerce platform, YOLOv6 features a novel backbone and neck architecture. \\nIt also incorporates advanced training techniques such as Anchor-Aided Training (AAT) and \\nSelf-Distillation to enhance performance and efficiency. YOLOv6 introduces three main \\nvariants: the standard version, balancing accuracy and speed for general detection tasks; \\nYOLOv6-Nano, optimized for real-time applications with a focus on speed and perfor -\\nmance on edge devices; and YOLOv6-Tiny, designed for even faster inference on low-\\nresource hardware, trading off some accuracy.\\nYOLOv7 (Wang et al. 2022, 2023) introduces advanced techniques like trainable bag-\\nof-freebies (optimizations that improve accuracy without increasing inference cost) and \\ndynamic label assignment. It introduces three variants: the standard version, balancing \\nspeed and accuracy; YOLOv7-X, a more powerful variant optimized for performance but \\nrequiring more computational resources; and YOLOv7-Tiny, a lightweight version designed \\nfor real-time applications on edge devices, prioritizing speed over accuracy.\\nYOLOv8 was released in 2023 by Ultralytics (Jocher et al. 2023). It features a more \\nefficient architecture, enhanced training techniques, and support for larger datasets. Its user-\\nfriendly implementation in PyTorch makes it accessible for both research and production. \\nYOLOv8 introduces four variants: YOLOv8-S, optimized for fast inference on edge devices \\nwith some accuracy trade-offs; YOLOv8-M, balancing accuracy and speed for general tasks; \\n1 Ultralytics specializes in AI and deep learning, known for YOLOv5, YOLOv8, and YOLO11 models used \\nin object detection, segmentation, and classification for computer vision tasks.\\n1 3\\nPage 9 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='af75c364-7c74-4f53-8319-340191af6f80', embedding=None, metadata={'page_label': '10', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nYOLOv8-L, prioritizing accuracy at the cost of computational demand; and YOLOv8-Tiny, \\na lightweight version for real-time applications.\\nYOLOv9 (Wang et al. 2024) proposed the concept of programmable gradient informa -\\ntion (PGI) to cope with the various changes required by deep networks to achieve mul -\\ntiple objectives. PGI can provide complete input information for the target task to calculate \\nobjective function, so that reliable gradient information can be obtained to update network \\nweights. In addition, a new lightweight network architecture – Generalized Efficient Layer \\nAggregation Network (GELAN), based on gradient path planning is designed. GELAN’s \\narchitecture confirms that PGI has gained superior results on lightweight models. We veri -\\nfied the proposed GELAN and PGI on MS COCO dataset based object detection. Its vari -\\nants are YOLOv9t, YOLOv9s, YOLOv9m, YOLOv9c, YOLOv9e (Ultralytics 2023a).\\nYOLOv10 (Ultralytics 2023b), developed by researchers at Tsinghua University, intro -\\nduces a novel approach to real-time object detection, addressing the limitations of both \\npost-processing and model architecture in previous YOLO versions. By eliminating non-\\nmaximum suppression (NMS) and optimizing key components of the model, YOLOv10 \\noffers significant improvements in efficiency and performance. This version introduces six \\ndistinct variants as YOLOv10-N, YOLOv10-S, YOLOv10-M, YOLOv10-B, YOLOv10-L, \\nand YOLOv10-X (Wang et al. 2024).\\nNotably, YOLOv10-N and YOLOv10-S exhibit the lowest latencies at 1.84 ms and 2.49 \\nms, respectively, making them highly suitable for applications requiring low latency. These \\nmodels outperform their predecessors, with YOLOv10-X achieving the highest mAP of \\n54.4% and a latency of 10.70 ms, reflecting a well-balanced enhancement in both accuracy \\nand inference speed.\\nYOLOv11 represents a significant advancement in object detection, featuring a sophis -\\nticated backbone and neck architecture for enhanced feature extraction. It optimizes speed \\nand efficiency while maintaining high accuracy. YOLOv11 balances precision and compu-\\ntational efficiency, suitable for various applications from embedded systems to large-scale \\ndeployments. The model comes in five variants: YOLOv11n, YOLOv11s, YOLOv11m, \\nYOLOv11L, and YOLOv11x, based on network depth.\\nYOLOv12 (Tian et al. 2025) further revolutionizes the field with an attention-centric \\napproach. It introduces the Area Attention (A2) module and Residual Efficient Layer Aggre-\\ngation Networks (R-ELAN) for improved feature processing. Utilizing these architectural \\nchanges, YOLOv12 achieves state-of-the-art performance while maintaining real-time \\ndetection capabilities. For instance, YOLOv12-N was reported to achieve 40.6% mAP with \\na 1.64 ms inference latency on a T4 GPU, outperforming YOLOv10-N and YOLOv11-N \\nby 2.1 mAP with a comparable speed. YOLOv12 also demonstrated superior object contour \\ndefinition and foreground activation compared to its predecessors. It supports various tasks, \\nincluding object detection, segmentation, classification, pose estimation, and oriented object \\ndetection, making it a versatile tool for diverse computer vision applications. YOLOv12 \\nis available in five variants: YOLOv12-N, YOLOv12-S, YOLOv12-M, YOLOv12-L, and \\nYOLOv12-X, each offering different trade-offs between performance and computational \\nrequirements (Tian et al. 2025).\\nTo summarize, each iteration of the YOLO series has set new benchmarks for object \\ndetection capabilities and significantly impacted various application areas, from autono -\\nmous vehicles and traffic safety\\n1 3\\n  274  Page 10 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a14334d-a248-4eb8-ac7f-39ea4f3b19f5', embedding=None, metadata={'page_label': '11', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\n2.1 Significance of latency and mAP scores in YOLO\\nInference Time (Tinf ) and mean Average Precision (mAP) are critical metrics to assess the \\nperformance of object detection models such as YOLO (Tang and Yuan 2015; Mao et al. \\n2019). Inference Time specifically measures the duration required for the model to pro -\\ncess an image and generate predictions, focusing solely on the computational phase and is \\ntypically measured in milliseconds (ms) (Mao et al. 2019). This metric excludes any delays \\nfrom image pre-processing or post-processing, providing a clear measure of the model’s \\ncomputational efficiency. Lower inference times are crucial for real-time applications such \\nas autonomous driving, surveillance, and robotics, where rapid and accurate detections are \\nessential (Chen et al. 2020). High inference times can lead to delays that compromise safety \\nand effectiveness in these dynamic settings (Pestana et al. 2021).\\nFrames Per Second (FPS) is another essential metric that indicates how many images the \\nmodel can evaluate each second, complementing inference time by illustrating the model’s \\nability to handle streaming video or rapid image sequences. Both inference time and FPS \\nprovide a detailed view of the real-time operational performance of a model.\\nIt is also important to note that these performance metrics are highly dependent on the \\nhardware platform used for testing. Differences in computational power can significantly \\ninfluence results, which makes it essential to standardize hardware during benchmark tests \\nto ensure fair comparisons. Likewise, mAP is a comprehensive metric used to evaluate the \\naccuracy of object detection models (Zhou et al. 2018). It considers both precision and recall \\n(Table 1), and it is calculated by taking the average precision (AP) across all classes and \\nthen averaging these AP scores (Hall et al. 2020; Zhou et al. 2018). It provides a balanced \\nview of how well the model performs across different object categories and varying condi -\\ntions within the dataset. Other metrics used for comprehensive evaluation of YOLO models \\n(Goutte and Gaussier 2005; Liang et al. 2021) are detailed in Table 1.\\nTable 1 Summary of performance metrics used in model evaluation\\nNo. Perfor-\\nmance \\nmetric\\nSym-\\nbol\\nEquation Description\\n1 Preci-\\nsion\\nP P = TP\\nTP+FP\\nRatio of true positive detections to the total predicted \\npositives\\n2 Recall R R = TP\\nTP+FN\\nRatio of true positive detections to the total actual \\npositives\\n3 F1 \\nscore\\nF1 F1=2 · P·R\\nP+R\\nHarmonic mean of precision and recall, balancing both \\nmetrics to provide a single performance measure for \\nthe model\\n4 Inter-\\nsection \\nover \\nunion\\nIoU IoU = Area of overlap\\nArea of union\\nMeasures the overlap between the predicted and actual \\nbounding boxes\\n5 Frames \\nper \\nsecond\\nFPS FPS = 1\\nL\\nNumber of images the model processes per second, \\ninversely related to latency\\n6 Non-\\nmaxi-\\nmum \\nsup-\\npres-\\nsion\\nNMS – NMS is a post-processing step in YOLO to remove \\nredundant bounding-boxes\\n1 3\\nPage 11 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='aa7028ea-dd29-452c-a716-d9f6e442449f', embedding=None, metadata={'page_label': '12', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nHere, True Positives (TP), True Negatives (TN), False Positives (FP), and False Nega -\\ntives (FN) are the key performance evaluators. TP is instance where the model correctly \\nidentifies an object as present. TN occurs when the model correctly predicts the absence \\nof an object. FP arises when the model incorrectly identifies an object as present, and FN \\nhappens when the model fails to detect an object that is actually present. These metrics are \\ncrucial for assessing the accuracy and reliability of the YOLO object detection (Hall et al. \\n2020; Zhou et al. 2018; Liang et al. 2021).\\n2.2 Single-stage detection with YOLO\\nThe Single Shot MultiBox Detector (SSD) (Liu et al. 2016) (Fig. 3) introduced in 2016 \\nrevolutionized object detection by streamlining the process through a single-stage approach, \\nsignificantly inspiring subsequent developments in YOLO models (Liu et al. 2016; Fu et al. \\n2017; Zhang et al. 2018). Unlike two-stage models like R-CNN, which rely on a region \\nproposal step before actual object detection, SSD and by extension, YOLO variants, per -\\nform detection and classification in a single sweep across the image. This paradigm shift \\nenhances the detection process by eliminating intermediate steps, thus facilitating faster and \\nmore efficient object detection suitable for real-time applications. The architecture of SSD, \\nwhich YOLO models have adapted, utilizes multiple feature maps at different resolutions \\nto detect objects of various sizes, employing a diverse array of anchor boxes at each feature \\nmap location to improve localization accuracy (Cui et al. 2018; Lin et al. 2017).\\nFigure 3 illustrates a YOLO model that incorporates SSD’s architectural principles to \\nenhance real-time detection capabilities through improved feature extraction using Multi-\\nHeaded Attention layers. This adoption from SSD methodology significantly boosts the pro-\\ncessing speed and detection accuracy of models such as YOLOv8, YOLOv9, and YOLOv10, \\nFig. 3 Enhanced YOLO model architecture incorporating SSD’s single-stage detection approach with \\nMulti-Headed Attention (MA) layers for superior real-time object detection performance (Jiang et al. \\n2019)\\n \\n1 3\\n  274  Page 12 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='17804d61-9f23-48ee-85bb-cacc8f11c193', embedding=None, metadata={'page_label': '13', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nmaking them ideal for rapid and reliable object detection in resource-constrained environ -\\nments (Tang et al. 2018; Li et al. 2017). The efficient single-shot mechanism, which directly \\nclassifies and localizes objects, highlights the ongoing evolution of the YOLO series to meet \\nthe accuracy and speed requirements of diverse real-world scenarios (Zhang et al. 2018).\\n3 Prior YOLO literature: context and distinctions\\nWe collected the existing published literature on YOLO to document and critically analyze \\nthe past knowledge, including major highlights and limitations, which are briefly summa -\\nrized and discussed here:\\n ● A review of YOLO algorithm developments  by Jiang et al. ( 2022) provided an insight -\\nful overview of YOLO algorithm development and its evolution through its versions. \\nThe authors analyze the fundamental aspects of YOLO’s to object detection, comparing \\nits various iterations to traditional CNNs. They emphasize ongoing improvements in \\nYOLO, particularly in enhancing target recognition and feature extraction capabilities. \\nIt also discusses the application of YOLO in specific fields, such as finance, highlight -\\ning its practical implications in feature extraction for image-based news analysis (Jiang \\net al. 2022).\\n ● A comprehensive systematic review of YOLO for medical object detection (2018 to 2023) \\nby Ragab et al. ( 2024) presented a systematic review of YOLO’s application in the \\nmedical field, that analyzes how different variants, particularly YOLOv7 and YOLOv8, \\nhave been employed for various medical detection tasks. They highlight the algorithm’s \\nsignificant performance in lesion detection, skin lesion classification, and other criti -\\ncal areas, demonstrating YOLO’s superiority over traditional methods in accuracy and \\ncomputational efficiency. Despite its successes, the review identifies challenges, such as \\nthe need for well-annotated datasets and addresses the high computational demands of \\nYOLO implementations. The paper suggested directions for future research to optimize \\nYOLO’s application in medical object detection (Ragab et al. 2024).\\n ● A comprehensive review of YOLO architectures in computer vision: from YOLOv1 to \\nYOLOv8 and YOLO-NAS by Terven et al. (2023) provides an extensive analysis of the \\nevolutionary trajectory of the YOLO algorithm, detailing how each iteration has con -\\ntributed to advances in real-time object detection. Their review covers the significant ar-\\nchitectural and training enhancements from YOLOv1 through YOLOv8 and introduces \\nYOLO-NAS and YOLO with Transformers. This study serves as a valuable resource \\nfor understanding the progression in network architecture, which has progressively im-\\nproved YOLO’s efficacy in diverse applications such as robotics and autonomous driv-\\ning.\\n ● YOLOv1 to v8: unveiling each variant-a comprehensive review of YOLO  by Hussain \\n(2024), provided in-depth analyses of the internal components and architectural inno -\\nvations of each YOLO variant. It provided a deep dive into the structural details and \\nincremental improvements that have marked the evolution of YOLO, presenting a well-\\nstructured analysis complete with performance benchmarks. This methodological ap -\\nproach not only highlights the capabilities of each variant but also discusses their practi-\\ncal impact across different domains, suggesting the potential for future enhancements \\n1 3\\nPage 13 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9d9e1a9a-4c2c-40a6-b434-560638bf2ef6', embedding=None, metadata={'page_label': '14', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nlike federated learning to improve privacy and model generalization (Hussain 2024).\\n ● YOLO-v1 to YOLO-v8, the rise of YOLO and its complementary nature toward digital \\nmanufacturing and industrial defect detection by Hussain (2023) reviewed and showed \\nrapid progression of the YOLO variants, focusing on their critical role in industrial ap -\\nplications, specifically for defect detection in manufacturing. Starting with YOLOv1 \\nand extending through YOLOv8, the paper illustrates how each version has been opti -\\nmized to meet the demanding needs of real-time, high-accuracy defect detection on con-\\nstrained devices. Hussain’s work not only examines the technical advancements within \\neach YOLO iteration but also validates their practical efficacy through deployment \\nscenarios in the manufacturing sector, emphasizing YOLO’s alignment with industrial \\nneeds (Hussain 2023).\\n ● YOLOv1 to YOLOv10: The fastest and most accurate real-time object detection systems \\nby Wang et al. (2024) provide a comprehensive literature survey on the YOLO series, \\nfrom YOLOv1 to YOLOv10. This review uniquely revisits the characteristics of YOLO \\nthrough a contemporary technical lens, highlighting its ongoing influence in advancing \\nreal-time computer vision research and subsequent technological developments. The \\nauthors explore the evolution of YOLO’s methodologies over the past decade and its \\ndiverse applications in fields requiring real-time object analysis. By doing so, they un -\\nderscore YOLO’s role as a foundational technology in various computer vision tasks, \\nsuch as instance segmentation and 3D object detection.\\n ● Evaluating the evolution of YOLO models: a comprehensive benchmark study of \\nYOLO11 and its predecessors by Jegham et al. ( 2024) conducts a detailed benchmark \\nanalysis of YOLO models from YOLOv3 to YOLO11. It examines their performance \\nacross three diverse datasets: Traffic Signs, African Wildlife, and Ships and Vessels, fo-\\ncusing on different challenges like object size and aspect ratio. Employing metrics like \\nPrecision, Recall, mAP, Processing Time, GFLOPs, and Model Size, the study identifies \\nthe strengths and limitations of each version, with YOLO11m showing exceptional bal-\\nance in accuracy and efficiency across the datasets.\\nFor the comprehensive review articles, it is advantageous to pinpoint a specific gap that the \\nproposed review will address. For instance, a common oversight in the existing literature \\nis the omission of the latest YOLO iterations, particularly YOLOv9, YOLOv10, YOLOv11 \\nand YOLOv12 or neglecting to cover the application domains of interest. Given the YOLO \\nalgorithm’s ten-year milestone, there is a pressing need to systematically document and crit-\\nically evaluate these newer models. Our review aims to fill this void by providing updated, \\nin-depth insights and comparative analysis of YOLOv9 and YOLOv12, extending across \\nvarious applications to serve the wider research and technical community. This state-of-the-\\nart review intends to highlight the continued advancements and capabilities of these models \\nwithin the dynamic field of object detection technology.\\nIn this review paper, we adopt a unique reverse-chronological approach to analyze the \\nprogression of YOLO, beginning with the most recent versions and moving backwards. The \\nanalysis is divided into six distinct subsections. The first subsection covers the latest itera -\\ntions, YOLOv12 and YOLOv11, The second subsection examines YOLOv10, YOLOv9, \\nand YOLOv8, where we delve into the architecture and advancements that define the fore -\\nfront of object detection technology. This approach not only shows the most cutting-edge \\ndevelopments but also sets the stage for understanding the incremental improvements \\n1 3\\n  274  Page 14 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6ea52fa2-5f03-460c-9f6f-97e4734cb0e5', embedding=None, metadata={'page_label': '15', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nthat have been realized over time. The third subsection reviews YOLOv7, YOLOv6, and \\nYOLOv5, tracing further back in the series to highlight the evolutionary steps contributing \\nto the enhancements observed in the later versions. We analyze each model’s technical and \\nscientific aspects to provide a comprehensive view of the progress within these iterations. \\nThe fourth subsection addresses the earlier YOLO versions, offering a complete historical \\nperspective that enriches the reader’s understanding of the foundational technologies and \\nthe methodologies, refined through successive updates. The fifth subsection presents alter -\\nnative versions derived from YOLO.\\nTo close this section, we discuss the application of the YOLO models in reverse order \\nacross five critical real-world domains: autonomous vehicles and traffic safety, healthcare \\nand medical image analysis, surveillance and security, industrial manufacturing and agricul-\\nture. For each application, we present a detailed examination and corresponding tabular data \\nin reverse chronological order, showcasing how YOLO technologies have been adapted and \\nimplemented to meet specific industry needs and challenges. This reverse review strategy \\nnot only emphasizes the state-of-the-art but also provides a narrative of technological evo -\\nlution, illustrating how each iteration builds upon the last to push the boundaries of what’s \\npossible in object detection. By understanding where YOLO technology stands today and \\nhow it got there, readers gain a comprehensive view of its capabilities and potential future \\ndirections. This methodical unpacking of the YOLO series not only highlights technologi -\\ncal advancements but also offers insights into the broader implications and utility of these \\nmodels in practical scenarios, setting the groundwork for anticipating future innovations in \\nobject detection technology.\\n4 Review of YOLO versions\\nThis section reviews YOLO series models, starting from the advanced and latest version, \\nYOLOv12, and progressively tracing back to the foundational YOLOv1. By first highlight-\\ning the most recent technological advancements, this approach enables immediate insights \\ninto the state-of-the-art capabilities of object detection. Subsequently, the narrative explores \\nhow earlier models laid the groundwork for these innovations.\\n4.1 YOLOv12 and YOLO11\\nYOLOv12 (Tian et al. 2025) is the most recent YOLO version introduced in February 2025, \\nwhich marks a substantial advancement in real-time object detection by integrating attention \\nmechanisms into the YOLO framework while maintaining competitive inference speeds. \\nThis attention-centric framework not only surpasses popular real-time detectors (e.g., Faster \\nR-CNN, RetinaNet, Detectron 2) in accuracy but also achieves state-of-the-art performance \\nthrough a combination of innovative attention methods, Residual Efficient Layer Aggrega -\\ntion Networks (R-ELAN), and several architectural optimizations.\\nFig. 4a demonstrates latency comparisons on the MS COCO benchmark dataset, \\nhighlighting YOLOv12’s significantly lower inference latency compared to YOLOv11, \\nYOLOv10, YOLOv9, and YOLOv8. The curve reveals a substantial reduction in latency, \\nenabling faster processing speeds while maintaining high detection accuracy (Tian et al. \\n2025). Complementing this, Fig. 4b presents comparison in terms of GFLOPs, which shows \\n1 3\\nPage 15 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8cd8e132-484e-4cc2-bd47-840216deefd2', embedding=None, metadata={'page_label': '16', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nYOLOv12 achieves higher computational efficiency, reflecting its ability to handle complex \\ncomputations effectively. This balance between speed and computational power demon -\\nstrates YOLOv12’s robust performance.\\nOn the COCO dataset, YOLOv12 sets a new state-of-the-art standard, with the light -\\nweight YOLOv12-N achieving 40.6% mAP and the larger YOLOv12-X reaching 55.2% \\nmAP. These results, combined with the latency and GFLOPs improvements, establish \\nYOLOv12 as a new benchmark in real-time object detection. The model framework is avail-\\nable in five scales: YOLOv12-N, S, M, L, and X, each optimized for specific applications. \\nFor instance, YOLOv12-N achieves 40.6% mAP at 1.64 ms on a T4 GPU, outperforming \\nYOLOv10-N and YOLOv11-N by 2.1% and 1.2% mAP, respectively. Similarly, YOLOv12-\\nS attains 48.0% mAP at 2.61 ms/image, surpassing YOLOv8-S, YOLOv9-S, YOLOv10-S, \\nand YOLOv11-S by margins of 3.0%, 1.2%, 1.7%, and 1.1% mAP. The larger models in the \\nYOLOv12 family continue to show improvements in efficiency and performance. Notably, \\nYOLOv12-M achieves 52.5% mAP at 4.86 ms/image. In terms of computational effieciency, \\nYOLOv12-L demonstrates a significant reduction in FLOPs, decreasing by 31.4G compared \\nto its predecessor, YOLOv10-L. At the highest end of the scale, YOLOv12-X showcases \\nsuperior performance, outperforming both YOLOv10-X and YOLOv11-X in detection \\naccuracy (Tian et al. 2025).\\nYOLOv12 also surpasses end-to-end detectors like RT-DETR and RT-DETRv2. For \\nexample, YOLOv12-S runs 42% faster than RT-DETR-R18 and RT-DETRv2-R18, using \\nonly 36% of the computation and 45% of the parameters. Residual connections show mini-\\nmal impact on convergence in smaller models (YOLOv12-N) but are critical for stable train-\\ning in larger models (YOLOv12-L/X), with YOLOv12-X requiring a scaling factor of 0.01. \\nThe area attention module reduces inference time by 0.7 ms on an RTX 3080 with FP32 \\nprecision, while FlashAttention further accelerates inference by 0.3-−0.4 ms.\\nVisualization analyses confirm that YOLOv12 produces clearer object contours and \\nmore precise foreground activations than its predecessors. A convolution-based attention \\nimplementation proves to be faster than linear alternatives. Additionally, a hierarchical \\ndesign, extended training (approximately 600 epochs), an optimized convolution kernel size \\nFig. 4 a Latency comparison on the MS COCO benchmark reveals significantly faster inference achieved \\nwith YOLOv12 compared to the same achieved with previous YOLO versions. b GFLOPs analysis also \\nshows enhanced computational efficiency. (Tian et al. 2025)\\n \\n1 3\\n  274  Page 16 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='42e9d4ec-ac13-4dfb-baac-833f5193df0a', embedding=None, metadata={'page_label': '17', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\n(7 × 7), absence of positional embedding, and an MLP ratio of 1.2 collectively enhance the \\nframework’s performance and efficiency.\\nAs discussed before, the YOLOv12 architecture (Fig. 5a), demonstrates an advanced \\nintegration of A2 (Area Attention) modules, R-ELAN (Residual Efficient Layer Aggrega -\\ntion Networks) blocks, and a streamlined detection head. This design optimizes the model’s \\nvisual information processing while maintaining high accuracy. The major innovations on \\nthe YOLOv12 architecture are listed below.\\n4.1.1 YOLOv12 architectural innovation\\n ● Area attention (A 2) module: This module implements segmented feature processing \\nwith Flash Attention integration, reducing computational complexity by 50% through \\nspatial reshaping while maintaining large receptive fields. AA enables real-time detec -\\ntion at fixed n = 640 resolution through optimized memory access patterns, as illus -\\ntrated in Fig. 5a.\\n ● Residual ELAN (R-ELAN) hierarchy: R-ELAN combines residual shortcuts (0.01 scal-\\ning) with dual-branch processing to mitigate the gradient vanishing problem. The model \\nalso features a streamlined final aggregation stage that reduces parameters by 18% and \\nFLOPs by 24% compared to baseline architectures, as shown in Fig. 5b.\\nFig. 5 a Architecture of the YOLOv12 object detection model that integrated Area Attention ( A2) mod-\\nules, R-ELAN blocks, and a streamlined detection head; b Comparison of “Attention Module” archi -\\ntectures: CSPNet (Wang et al. 2020), ELAN (Wang et al. 2022), C3K2 (used in YOLOv9) (Wang et al. \\n2024), and the novel R-ELAN (Tian et al. 2025) introduced with YOLOv12, which improved residual \\nconnections and enhanced feature aggregation, demonstrating superior performance\\n \\n1 3\\nPage 17 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='df708f6a-a962-498e-865f-5a7d464a09d5', embedding=None, metadata={'page_label': '18', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\n ● Efficient architectural revisions: YOLOv12 replaces positional encoding with 7 ×7 \\ndepth-wise convolution for implicit spatial awareness. It also implements adaptive MLP \\nratio (1.2×) and shallow block stacking to balance the computational load, achieving \\n4.1 ms inference latency on V100 hardware.\\n ● Optimized training framework: The model was trained over 600 epochs using SGD with \\ncosine scheduling (initial lr = 0.01). The model also incorporates Mosaic-9 and Mixup \\naugmentations with 12.8% mAP gain on COCO dataset, maintaining real-time perfor -\\nmance through selective kernel convolution integration.\\nFigure 5b presents an architectural comparison of popular attention modules: CSPNet, \\nELAN, C3K2 (a case of GELAN), and the proposed R-ELAN. Brief summary of these \\nmodules is blow.\\n ● CSPNet (Cross stage partial network): CSPNet enhances gradient flow by splitting fea-\\nture maps into two paths, one for learning and one for propagation, reducing compu -\\ntational bottlenecks and improving inference speed. This model is visually depicted in \\nFig. 5b (leftmost module).\\n ● ELAN (Efficient layer aggregation network): ELAN improves feature integration by ag-\\ngregating multi-scale features efficiently, enhancing the model’s ability to detect objects \\nat various scales. However, as shown in Fig. 5b (second module), ELAN can introduce \\ninstability due to gradient blocking and lacks of residual connections, particularly in \\nlarge-scale models.\\n ● C3K2 (Compact GELAN): This module is a compact version of GELAN (Generalized \\nEfficient Layer Aggregation Network) that offers a balance between computational effi-\\nciency and feature expressiveness, suitable for resource-constrained environments. The \\nmodule is also illustrated in Fig. 5b (third module).\\n ● R-ELAN (Residual ELAN): R-ELAN introduces residual connections and redesigns \\nfeature aggregation to address optimization challenges in attention-based models, com-\\nbining the benefits of residual learning with efficient feature aggregation. As shown in \\nFig. 5b (rightmost module), R-ELAN applies a residual shortcut with a scaling factor \\n(default 0.01) and processes the input through a transition layer, followed by a bottle -\\nneck structure for improved stability and performance.\\nThe R-ELAN design, as depicted in Fig. 5b, addresses the limitations of ELAN by introduc-\\ning residual connections and a revised aggregation approach. Unlike ELAN, which splits \\nthe input into two parts and processes them separately, R-ELAN applies a transition layer to \\nadjust channel dimensions and processes the feature map through subsequent blocks before \\nconcatenation. This design mitigates gradient blocking and ensures stable convergence, \\nparticularly in large-scale models like YOLOv12-L and YOLOv12-X. The integration of \\nresidual connections and attention mechanisms in R-ELAN, as shown in Fig. 5b, highlights \\nYOLOv12’s architectural advancements in balancing efficiency and accuracy.\\nYOLO11: YOLO11 developed by Ultralytics represents the most recent version build -\\ning upon the foundations established by its predecessors in the YOLO family. This latest \\niteration introduces several architectural innovations that enhance its performance across \\na wide spectrum of tasks as depicted in Fig. 6. The model incorporates the C3k2 (Cross \\nStage Partial with kernel size 2) block, which replaces the C2f block used in previous ver -\\n1 3\\n  274  Page 18 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4546a378-b008-4255-8540-f64c512219f0', embedding=None, metadata={'page_label': '19', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nsions, offering improved computational efficiency (Sapkota et al. 2024b). Additionally, \\nYOLOv11 retains the SPPF (Spatial Pyramid Pooling-Fast) component and introduces the \\nC2PSA (Convolutional block with Parallel Spatial Attention) module, collectively enhanc-\\ning feature extraction capabilities (Sapkota et al. 2024a). These architectural enhancements \\nenable YOLOv11 to capture intricate image details with greater precision, particularly in \\nchallenging scenarios involving small or occluded objects. The model’s versatility is evi -\\ndent in its support for a broad range of computer vision tasks, including object detection, \\ninstance segmentation, pose estimation, image classification, and oriented bounding box \\n(OBB) detection (Ultralytics 2024).\\nEmpirical evaluations of YOLOv achieves a higher mean Average Precision (mAP) score \\non the COCO dataset while utilizing 22% fewer parameters compared to its YOLOv8m coun-\\nterpart (Jegham et al. 2024). This reduction in parameter count contributes to faster model \\nperformance without significantly impacting overall accuracy. Furthermore, YOLOv11 \\nexhibits inference times approximately 2% faster than YOLOv10, making it particularly \\nwell-suited for real-time applications. The model’s efficiency extends across various \\ndeployment environments, including edge devices, cloud platforms, and systems support -\\ning NVIDIA GPUs. YOLOv11 is available in multiple variants, ranging from nano to extra-\\nlarge, catering to diverse computational requirements and use cases. These advancements \\nposition YOLOv11 as a state-of-the-art solution for industries requiring rapid and accurate \\nimage analysis, such as autonomous driving, surveillance, and industrial automation.\\nYOLOv10, incorporates advanced techniques like automated architecture search and \\nmore refined loss functions to enhance detection accuracy and speed, tailored for both \\nedge and cloud computing environments. This version and its predecessors, YOLOv9 and \\nYOLOv8, introduce substantial improvements in network architecture, such as the integra-\\ntion of cross-stage partial networks (CSPNets) and the use of transformer-based backbones \\nfor better feature extraction across different scales. YOLOv7 and YOLOv6 continued to build \\non these improvements by optimizing computational efficiency and expanding model scal -\\nability. Meanwhile, YOLOv5 introduced PyTorch support, which significantly enhanced the \\nmodel’s accessibility and adaptability, thus broadening its application in industry and aca -\\ndemia. YOLOv4, on the other hand, marked a pivotal point in YOLO history by integrating \\nFig. 6 YOLOv11 architecture diagram: enhanced backbone with C3k2 blocks replacing C2f, SPPF for \\nmulti-scale feature extraction, C2PSA for attention mechanism, and optimized neck. Efficient feature \\nprocessing through multiple scales, culminating in a detect head for multi-class object detection and \\nlocalization\\n \\n1 3\\nPage 19 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='059620b9-d60f-4f96-95cb-c720e153a6f3', embedding=None, metadata={'page_label': '20', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nfeatures like Mish activation and Cross-Stage Partial connections, setting new standards for \\nspeed and accuracy in real-time applications. The mid-generations, starting with YOLOv3, \\nwere notable for introducing multi-scale predictions and bounding box predictions across \\ndifferent layers, which greatly improved the model’s ability to detect small objects-a long -\\nstanding challenge in earlier versions. YOLOv3 was also one of the first YOLO models \\nto leverage deeper feature extractors like Darknet-53, which significantly boosted its per -\\nformance over YOLOv2. YOLOv2 itself had introduced important features such as batch \\nnormalization and high-resolution classifiers, which enhanced the overall accuracy without \\ncompromising the speed. The original YOLO model, YOLOv1, was revolutionary, propos-\\ning a single-stage detection framework that unified the object detection process into a single \\nneural network model.\\nFig. 7a illustrates a sophisticated transformer-based model that simplifies the detection \\nprocess by integrating dual label assignments and eliminating the need for non-max sup -\\npression (NMS), achieving a streamlined, end-to-end object detection. YOLOv9, as shown \\nin Fig. 7b, introduces the Programmable Gradient Information (PGI) system to enhance \\nmodel interpretability and robustness, significantly improving generalization across various \\ntasks. Moving to YOLOv8 and YOLOv7, Figs. 7c and d respectively depict their archi -\\ntectures which incorporate elements like ELAN and CSPNet to boost performance and \\nflexibility across computing devices. YOLOv6, highlighted in Fig. 7e, focuses on industry \\napplications with enhancements in model quantization and real-time performance.\\nFig. 7  Simplified Architecture diagrams for: a YOLOv10; b YOLOv9; c YOLOv8; d YOLOv7; \\ne YOLOv6; f YOLOv5; g YOLOv4; h YOLOv3; i YOLOv2; j YOLOv1. The diagrams are detailed \\nin Wang et al. (2024)\\n \\n1 3\\n  274  Page 20 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e02f612e-4191-4a35-9bda-a24f94f01fea', embedding=None, metadata={'page_label': '21', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nYOLOv5, represented in Fig. 7f, marks a pivotal development with its adoption of \\nPyTorch and improvements in training methods that enhance model accessibility and \\nefficiency. In contrast, YOLOv4, shown in Fig. 7g, integrates technologies like CSPNet \\nand Path Aggregation Network (PAN) (Liu et al. 2018) to optimize real-time detection. \\nYOLOv3, visualized in Fig. 7h, introduces significant architectural changes with Dark -\\nnet-53 and multi-scale predictions, which substantially enhance the detection of small \\nobjects. YOLOv2, depicted in Fig. 7i, advances the architecture with dimension clusters and \\nfine-grained features, improving the model’s efficiency and adaptability. Finally, YOLOv1, \\nas outlined in Fig. 7j, revolutionizes object detection by integrating a single-stage detector \\nthat performs grid-based predictions in real-time, significantly reducing model complexity \\nand enhancing speed.\\n4.2 YOLOv10, YOLOv9 and YOLOv8\\nYOLOv10 (Wang et al. 2024), developed at Tsinghua University, China, represents a break-\\nthrough in the YOLO series for real-time object detection, achieving unprecedented per -\\nformance. This version eliminates the need for non-maximum suppression (NMS) (Rothe \\net al. 2015), a traditional bottleneck in earlier models, thereby drastically reducing latency. \\nYOLOv10 introduces a dual assignment strategy in its training protocol, which optimizes \\ndetection accuracy without sacrificing speed with the help of one-to-many and one-to-one \\nlabel assignments, ensuring robust detection with lower latency (Li et al. 2023; Tian et al. \\n2024). The architecture of YOLOv10 includes several innovative components that enhance \\nboth computational efficiency and detection performance. Among these are lightweight \\nclassification heads (Bhagat et al. 2021) that reduce computational demands, spatial-chan -\\nnel decoupled downsampling to minimize information loss during feature reduction (Hu \\net al. 2023), and rank-guided block design that optimizes parameter use (Yang et al. 2023). \\nThese architectural advancements ensure that YOLOv10 operates synergistically across \\nvarious scales-from YOLOv10-N (Nano) to YOLOv10-X (Extra Large), making it adapt -\\nable to diverse computational constraints and operational requirements (Wang et al. 2024). \\nAccording to wang et al. (Wang et al. 2024), performance evaluations on benchmark data-\\nsets like MS-COCO (Lin et al. 2014) demonstrate that YOLOv10 not only surpasses its \\npredecessors-YOLOv9 and YOLOv8-in both accuracy and efficiency but also sets new \\nindustry standards. For instance, YOLOv10-S substantially outperforms comparable mod -\\nels (e.g., YOLOv9 BASE, YOLOV9 Gelan, YOLOv8, YOLOv7) with an improved mAP \\nand lower latency. This version also incorporates holistic efficiency-accuracy driven design, \\nlarge-kernel convolutions, and partial self-attention modules, collectively improving the \\ntrade-off between computational cost and detection capability. The architecture diagrams \\nof YOLOv10, YOLOv9, and YOLOv8 are summarized in Figs. 8, 9, and 10, respectively.\\nThe YOLOv10 model offers various configurations, each tailored to specific performance \\nneeds within real-time object detection frameworks. Starting with YOLOv10-N (Nano), it \\ndemonstrates a rapid detection capability with a mAP of 38.5% at an exceptionally reduced \\nlatency to 1.84 ms, making it highly suitable for scenarios demanding quick responses. \\nProgressing through the series, YOLOv10-S (Small) and YOLOv10-M (Medium) offer pro-\\ngressively higher mAP values of 46.3% and 51.1% at latencies of 2.49 ms and 4.74 ms, \\nrespectively, providing a balanced performance for versatile applications. The larger vari -\\nants, YOLOv10-B (Balanced) and YOLOv10-L (Large), cater to environments requiring \\n1 3\\nPage 21 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5352afdd-9cc6-4fbb-86ca-f91d09d3a5de', embedding=None, metadata={'page_label': '22', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\ndetailed detections, with mAPs of 52.5% and 53.2% and latencies of 5.74 ms and 7.28 ms \\nrespectively. The largest model, YOLOv10-X (Extra Large), excels with the highest mAP \\nof 54.4% at a latency of 10.70 ms, designed for complex detection tasks where precision is \\nparamount. These configurations underscore YOLOv10’s adaptability across a spectrum of \\noperational requirements.\\nFig. 9 YOLOv9 architecture  (Wang et al. 2024) with CSPNet, ELAN, and GELAN modules. CSPNet \\nenhances gradient flow and reduces computational load through feature map partitioning. ELAN focuses \\non the linear aggregation of features for improved learning efficiency, while GELAN generalizes this ap-\\nproach to combine features from multiple depths and pathways, providing greater flexibility and accuracy \\nin feature extraction\\n \\nFig. 8 YOLOv10 architecture, which employs a dual label assignment strategy to improve detection ac -\\ncuracy. A backbone processes the input image, while PAN (Path Aggregation Network) enhances feature \\nrepresentation. Employed heads are (1) one-to-many head for regression and classification tasks, and (2) \\none-to-one head for precise localization (Wang et al. 2024)\\n \\n1 3\\n  274  Page 22 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3af4cb24-f42a-4f2a-a71e-eddae4f684d9', embedding=None, metadata={'page_label': '23', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nReflecting on YOLO’s evolution, starting from YOLOv1, which set the benchmark with \\nan mAP of 63.4% and a latency of 45 ms, to the latest YOLOv10, significant technological \\nstrides have been evident. YOLOv10’s predecessors, YOLOv9 and YOLOv8, display com-\\nparable mAP scores to YOLOv10 but with marginally higher latency, indicating the incre -\\nmental enhancements YOLOv10 brings to the table. Specifically, YOLOv9 and YOLOv8 \\nmodels, such as YOLOv9-N and YOLOv8-N, showcase mAPs of 39.5% and 37.3%, respec-\\ntively, at latency indicative of their generational improvements. Meanwhile, the higher end \\nof these series, YOLOv9-X, and YOLOv8-X, achieve mAPs of 54.4% and 53.9%, respec -\\ntively, with YOLOv10 outperforming them in efficiency. The YOLO series, from YOLOv1 \\nthrough YOLOv8, YOLOv9, and now YOLOv10, has continually advanced the frontier of \\nFig. 11 Architecture diagrams for a YOLOv7; b YOLOv6; and c YOLOv5\\n \\nFig. 10 YOLOv8 architecture (Jocher et al. 2022): showcasing the key components and their connections. \\nThe backbone network processes the input image through multiple convolutional layers (C1 to C5), ex -\\ntracting hierarchical features. These features are then passed through the Feature Pyramid Network (FPN) \\nto create a feature pyramid (P3, P4, P5), which enhances detection at different scales. The network heads \\nperform final predictions, incorporating convolutional blocks and upsample blocks to refine features\\n \\n1 3\\nPage 23 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5a8b7145-f862-40c8-8e38-2d235289074b', embedding=None, metadata={'page_label': '24', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nreal-time object detection, enhancing both the speed and accuracy of detections, and thus \\nbroadening the scope for practical applications in sectors like autonomous driving, surveil-\\nlance, and real-time video analytics.\\nYOLOv9 (Wang et al. 2024) marks a significant advancement in real-time object detec-\\ntion by addressing the efficiency and accuracy challenges associated with earlier versions, \\nparticularly by mitigating information loss in deep neural processing. It introduces the \\ninnovative Programmable Gradient Information (PGI) and the Generalized Efficient Layer \\nAggregation Network (GELAN) architecture. These enhancements focus on preserving \\ncrucial information across the network, ensuring robust and reliable gradients that prevent \\ndata degradation, which is common in deep neural networks (Tishby and Zaslavsky 2015). \\nCompared to its successor, YOLOv10, YOLOv9 sets a foundational stage by addressing \\nthe information bottleneck problem that typically hinders deep learning models. While \\nYOLOv9’s PGI strategically maintains data integrity throughout the processing layers, \\nYOLOv10 builds upon this foundation by eliminating the need for NMS and further opti -\\nmizing model architecture for reduced latency and enhanced computational efficiency. \\nYOLOv10 also introduces dual assignment strategies for NMS-free training, significantly \\nenhancing the system’s response time without compromising accuracy, which reflects a \\ndirect evolution from the groundwork laid by YOLOv9’s innovations (Zhang et al. 2023). \\nFurthermore, YOLOv9’s GELAN architecture represents a pivotal improvement in net -\\nwork design, offering a flexible and efficient structure that effectively integrates multi-scale \\nfeatures. While GELAN contributes significantly to YOLOv9’s performance, YOLOv10 \\nextends these architectural improvements to achieve even greater efficiency and adaptability \\n(Chien et al. 2024). It reduces computational overhead and increases the model’s applicabil-\\nity to various real-time scenarios, showcasing an advanced level of refinement that lever -\\nages and enhances the capabilities introduced by YOLOv9.\\nYOLOv8 was released in January 2023 by Ultralytics, marking a significant progres -\\nsion in the YOLO series with an introduction of multiple scaled versions designed to cater \\nto a wide range of applications (Ultralytics 2024a, b). These versions included YOLOv8n \\n(nano), YOLOv8s (small), YOLOv8m (medium), YOLOv8l (large), and YOLOv8x (extra-\\nlarge), each optimized for specific performance and computational needs. This flexibility \\nmade YOLOv8 highly versatile, supporting many vision tasks such as object detection, \\nsegmentation, pose estimation, tracking, and classification, significantly broadening its \\napplication scope in real-world scenarios (Ultralytics 2024b). The architecture of YOLOv8 \\nunderwent substantial refinements to enhance its detection capabilities. It retained a similar \\nbackbone to YOLOv5 but introduced modifications in the CSP Layer, now evolved into the \\nC2f module-a cross-stage partial bottleneck with dual convolutions that effectively com -\\nbine high-level features with contextual information to bolster detection accuracy. YOLOv8 \\ntransitioned to an anchor-free model with a decoupled head, allowing independent process-\\ning of object detection, classification, and regression tasks, which, in turn, improved overall \\nmodel accuracy (Ultralytics 2024). The output layer employed a sigmoid activation function \\nfor objectness scores and softmax for class probabilities, enhancing the precision of bound-\\ning box predictions. YOLOv8 also integrated advanced loss functions like CIoU (Du et al. \\n2021) and Distribution Focal Loss (DFL) (Xu et al. 2022) for bounding-box optimization \\nand binary cross-entropy for classification, which proved particularly effective in enhanc -\\ning detection performance for smaller objects. YOLOv8’s architecture, demonstrated in \\ndetailed diagrams, features the modified CSPDarknet53 backbone with the innovative C2f \\n1 3\\n  274  Page 24 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='af562f92-d015-40be-ad6e-441e2a3d86e5', embedding=None, metadata={'page_label': '25', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nmodule, augmented by a spatial pyramid pooling fast (SPPF) layer that accelerates compu-\\ntation by pooling features into a fixed-size map. This model also introduced a semantic seg-\\nmentation variant, YOLOv8-Seg, which utilized the backbone and C2f module, followed by \\ntwo segmentation heads designed to predict semantic segmentation masks efficiently. This \\nsegmentation model achieved state-of-the-art results on various benchmarks while main -\\ntaining high speed and accuracy, evident in its performance on the MS COCO dataset where \\nYOLOv8x reached an AP of 53.9% at 640 pixels image size-surpassing the 50.7% AP of \\nYOLOv5-with a remarkable speed of 280 FPS on an NVIDIA A100 using TensorRT. As \\nwe progress backwards through the YOLO series, from YOLOv10 to YOLOv8 and soon to \\nYOLOv7, these architectural and functional advancements highlight the series’ evolution -\\nary trajectory in optimizing real-time object detection networks.\\nYOLOv8 was released in January 2023 by Ultralytics, marking a significant progres -\\nsion in the YOLO series with an introduction of multiple scaled versions designed to cater \\nto a wide range of applications (Ultralytics 2024a, b). These versions included YOLOv8n \\n(nano), YOLOv8s (small), YOLOv8m (medium), YOLOv8l (large), and YOLOv8x (extra-\\nlarge), each optimized for specific performance and computational needs. This flexibility \\nmade YOLOv8 highly versatile, supporting many vision tasks such as object detection, seg-\\nmentation, pose estimation, tracking, and classification, significantly broadening its applica-\\ntion scope in real-world scenarios (Ultralytics 2024b). YOLOv8’s architecture underwent \\nsignificant upgrades to boost its detection performance, maintaining a backbone similar to \\nYOLOv5 but enhancing it with the evolved C2f module, a cross-stage partial bottleneck \\nwith dual convolutions. This module integrates high-level features with contextual informa-\\ntion, improving accuracy. The model transitioned to an anchor-free system with a decoupled \\nhead for independent objectness, classification, and regression tasks, enhancing accuracy \\n(Ultralytics 2024). The output layer now uses sigmoid for objectness and softmax for class \\nprobabilities, refining bounding box precision. Additionally, YOLOv8 employs CIoU (Du \\net al. 2021), Distribution Focal Loss (DFL) (Xu et al. 2022) for bounding-box optimization, \\nand binary cross-entropy for classification, significantly boosting performance, particularly \\nfor smaller objects.\\nThis model also introduced a semantic segmentation variant, YOLOv8-Seg (Yue et al. \\n2023), which utilized the backbone and C2f module, followed by two segmentation heads \\ndesigned to predict semantic segmentation masks efficiently. This segmentation model \\nachieved state-of-the-art results on various benchmarks while maintaining high speed and \\naccuracy, evident in its performance on the MS COCO dataset where YOLOv8x reached an \\nAP of 53.9% at 640 pixels image size-surpassing the 50.7% AP of YOLOv5-with a remark-\\nable speed of 280 FPS on an NVIDIA A100 using TensorRT. As we progress backwards \\nthrough the YOLO series, from YOLOv10 to YOLOv8 and soon to YOLOv7, these archi -\\ntectural and functional advancements highlight the series’ evolutionary trajectory in opti -\\nmizing real-time object detection networks.\\n4.3 YOLOv7, YOLOv6 and YOLOv5\\nThe YOLOv7 model introduces enhancements in object detection tailored for drone-cap -\\ntured scenarios, particularly through the Transformer Prediction Head (TPH-YOLOv5) \\nvariant (Zhu et al. 2021), which emphasizes improvements in handling scale variations \\nand densely packed objects (Wang et al. 2023). By incorporating TPH and the Convolu -\\n1 3\\nPage 25 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5eef2436-9c48-4689-8605-9ef869024d21', embedding=None, metadata={'page_label': '26', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\ntional Block Attention Module (CBAM) (Woo et al. 2018), YOLOv7 substantially boosts \\nits capacity to focus on relevant regions in cluttered environments. These features particu -\\nlarly enhance the model’s ability to detect objects across varied scales, an essential trait \\nfor drone applications where altitude changes affect object size perception drastically. The \\nmodel integrates sophisticated strategies like multi-scale testing (Hnewa and Radha 2023) \\nand a self-trained classifier, which refines its performance on challenging categories by \\nspecifically addressing common issues in drone imagery, such as motion blur and occlusion. \\nThese adaptations have shown notable improvements, with YOLOv7 achieving competitive \\nresults in drone-specific datasets and challenges (Bai et al. 2024). The model’s adaptabil -\\nity and robustness in such specialized conditions demonstrate its potential beyond conven -\\ntional settings, catering effectively to next-generation applications like urban surveillance \\nand wildlife monitoring.\\nYOLOv6 emerges as a robust solution in industrial applications by delivering a finely \\nbalanced trade-off between speed and accuracy, crucial for deployment across various \\nhardware platforms (Li et al. 2022). It iterates on previous versions by incorporating cut -\\nting-edge network designs, training strategies, and quantization techniques to enhance its \\nefficiency and performance significantly. This model has been optimized for diverse opera-\\ntional requirements with its scalable architecture, ranging from YOLOv6-N to YOLOv6-X, \\neach offering different performance levels to suit specific computational budgets (Sirisha \\net al. 2023). Significant innovations in YOLOv6 include advanced label assignment tech -\\nniques and loss functions that refine the model’s predictive accuracy and operational effi -\\nciency. By leveraging state-of-the-art advancements in machine learning, YOLOv6 not only \\nexcels in traditional object detection metrics but also sets new standards in throughput and \\nlatency, making it exceptionally suitable for real-time applications in industrial and com -\\nmercial domains.\\nYOLOv6 and YOLOv7 each introduced innovative features that build on the foundation \\nset by YOLOv5. YOLOv6, released in October 2021, introduced lightweight nano mod -\\nels optimized for mobile and CPU environments alongside a more effective backbone for \\nimproved small object detection. YOLOv7 further advanced this development by incor -\\nporating a new backbone network, PANet (Wang et al. 2019), enhancing feature aggrega -\\ntion and representation, and introducing the CIOU loss function for better object scaling \\nand aspect ratio handling. YOLO-v6 significantly shifts the architecture to an anchor-free \\ndesign, incorporating a self-attention mechanism to better capture long-range dependencies \\nand employing adaptive training techniques to optimize performance during training (Zhang \\net al. 2021). These versions collectively push the boundaries of object detection perfor -\\nmance, emphasizing speed, accuracy, and adaptability across various deployment scenarios.\\nYOLOv5 has significantly contributed to the YOLO series evolution, focusing on \\nuser-friendliness and performance enhancements (Ultralytics 2024a, b). Its introduction \\nby Ultralytics brought a streamlined, accessible framework that lowered the barriers to \\nimplementing high-speed object detection across various platforms. YOLOv5’s architec -\\nture incorporates a series of optimizations including improved backbone, neck, and head \\ndesigns which collectively enhance its detection capabilities. The model supports multiple \\nsize variants, facilitating a broad range of applications from mobile devices to cloud-based \\nsystems (Ultralytics 2024a). YOLOv5’s adaptability is further evidenced by its continuous \\nupdates and community-driven enhancements, which ensure it remains at the forefront of \\nobject detection technologies. This version stands out for its balance of speed, accuracy, and \\n1 3\\n  274  Page 26 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='edf64fdf-7c07-4a0c-a3b7-5bcf3ec2bee7', embedding=None, metadata={'page_label': '27', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nutility, making it a preferred choice for developers and researchers looking to deploy state-\\nof-the-art detection systems efficiently.\\nYOLOv5 marks a significant evolution in the YOLO series, focusing on production-ready \\ndeployments with streamlined architecture for real-world applications. This version empha-\\nsizes reducing the model’s complexity by refining its layers and components, enhancing its \\ninference speed without sacrificing detection accuracy. The backbone and feature extraction \\nlayers were optimized to accelerate processing, and the network’s architecture was simpli -\\nfied to facilitate faster data throughput. Importantly, YOLO v5 enhances its deployment \\nflexibility, catering to edge devices with limited computational resources through model \\nmodularity and efficient activations. These architectural refinements ensure YOLO v5 oper-\\nates effectively in diverse environments, from high-resource servers to mobile devices, \\nmaking it a versatile tool in the arsenal of object detection technologies.\\n4.4 YOLOv4, YOLOv3, YOLOv2 and YOLOv1\\nThe introduction of YOLOv4 (Bochkovskiy et al. 2020) in 2020 marked the latest develop-\\nments, employing CSPDarknet-53 (Mahasin and Dewi 2022) as its backbone. This modi -\\nfied version of Darknet-53 uses Cross-Stage Partial connections to reduce computational \\ndemands while enhancing learning capacity.\\nYOLOv4 incorporates innovative features such as Mish activation (Misra 2019), replac-\\ning traditional ReLU to maintain smooth gradients, and utilizes new data augmentation tech-\\nniques such as Mosaic and CutMix (Yun et al. 2019). Additionally, it introduces advanced \\nregularization methods, including DropBlock regularization (Ghiasi et al. 2018) and Class \\nLabel Smoothing to prevent overfitting (Müller et al. 2019), alongside optimization strate-\\ngies termed BoF (Bag of Freebies) (Zhang et al. 2019) and BoS (Bag of Specials) that \\nenhance training and inference efficiency. “YOLOv3, introduced in 2018 before the release \\nof YOLOv4, employed the Darknet-53 architecture, incorporating principles of residual \\nlearning. Initially trained on ImageNet, this version excelled in detecting objects of various \\nsizes due to its multi-scale detection capabilities within the architecture. The subsequent \\ndevelopment of YOLOv4 built upon the success of YOLOv3, further enhancing the frame-\\nwork’s robustness and accuracy.\\nYOLOv3 (Redmon and Farhadi 2018) improved detection accuracy, especially for small \\nobjects, by using three different scales for detection, thereby capturing essential features at \\nvarious resolutions. Earlier, YOLOv2 and the original YOLO (YOLOv1) laid the ground -\\nwork for these advancements (Redmon et al. 2016).\\nEarlier, YOLOv2 and the original YOLO (YOLOv1) laid the groundwork for these \\nadvancements. Released in 2016, YOLOv2 introduced a new 30-layer architecture with \\nanchor boxes from Faster R-CNN and batch normalization to speed up convergence and \\nenhance model performance. YOLOv1, debuting in 2015 by Joseph Redmon, revolution -\\nized object detection with its single-shot mechanism that predicted bounding boxes and \\nclass probabilities in one network pass, utilizing a simpler Darknet-19 architecture. This \\ninitial approach significantly accelerated the detection process, establishing the founda -\\ntional techniques that would be refined in later versions of the YOLO series. YOLOv4 and \\nYOLOv3, showcasing their advanced architectures and features, are illustrated in Fig. 12a \\nand b, respectively, while YOLOv2 and YOLOv1 are depicted in Fig. 13a and b, showcas-\\ning the foundational developments in the series.\\n1 3\\nPage 27 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4f5945d0-d653-4801-ae33-354928968a7b', embedding=None, metadata={'page_label': '28', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\n4.5 Alternative versions derived from YOLO\\nSeveral alternative YOLO models have been developed from different versions, with the five \\nprimary ones being YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and Gold-YOLO.\\n4.5.1 YOLO-NAS\\nYOLO-NAS, developed by Deci AI, represents a significant advancement in object detec -\\ntion technology (Terven et al. 2023). This model leverages Neural Architecture Search \\n(NAS) (Ren et al. 2021) to address limitations of previous YOLO iterations such as \\nYOLOv4, YOLOv5, YOLOv6 and YOLOv7 (Mithun and Jawhar 2024). YOLO-NAS intro-\\nduces a quantization-friendly basic block, enhancing performance with minimal precision \\nloss post-quantization. The architecture employs quantization-aware blocks and selective \\nquantization, resulting in superior object detection capabilities. Notably, when converted \\nto INT8, the model experiences only a slight precision drop, outperforming its predeces -\\nsors. YOLO-NAS utilizes sophisticated training schemes and post-training quantization \\ntechniques, further improving its efficiency (Terven et al. 2023). The model is pre-trained \\non datasets such as COCO, Objects365, and Roboflow 100, making it suitable for various \\ndownstream object detection tasks. YOLO-NAS is available in three variants: Small (s), \\nMedium (m), and Large (l), each optimized for different computational requirements. These \\nvariants offer a balance between Mean Average Precision (mAP) and latency, with the INT-8 \\nversions demonstrating impressive performance metrics. The architecture of YOLO-NAS \\nFig. 12 Comparison of YOLOv4 (Bochkovskiy et al. 2020) and YOLOv3 (Redmon and Farhadi 2018) \\narchitectures. a YOLOv4 architecture shows a two-stage detector with a backbone, neck, dense predic -\\ntion, and sparse prediction modules. b YOLOv3 architecture features convolutional and upsampling lay-\\ners that lead to multi-scale predictions. This highlights the structural advancements in object detection \\nbetween the two versions\\n \\n1 3\\n  274  Page 28 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2bd59051-035c-460c-be1f-5715fb463765', embedding=None, metadata={'page_label': '29', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\n(Fig. 14a) supports inference, validation, and export modes, though it does not support train-\\ning. YOLO-NAS’s innovative design and superior performance position it as a critical tool \\nfor developers and researchers in the field of computer vision.\\n4.5.2 YOLO-X\\nYOLOX, developed by Megvii Technology, represents a significant advancement in the \\nYOLO series of object detectors. This model introduces several key improvements to \\nenhance performance and efficiency. YOLOX adopts an anchor-free approach, depart -\\ning from the anchor-based methods of its predecessors such as YOLOv6, YOLOv7 and \\nYOLOv8 (Ge 2021). It incorporates a decoupled head, separating classification and regres-\\nFig. 13 a YOLOv2 architecture  (Redmon and Farhadi 2017), illustrating improvements such as the use of \\nbatch normalization, higher resolution input, and anchor boxes; b YOLOv1 architecture  (Redmon et al. \\n2016), showing the sequence of convolutional layers, max-pooling layers, and fully connected layers used \\nfor object detection. This model performs feature extraction and prediction in a single unified step, aiming \\nfor real-time performance\\n \\n1 3\\nPage 29 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3471501b-e148-4ae2-bf45-56b1c60d42cb', embedding=None, metadata={'page_label': '30', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nsion tasks to address the known conflict between these objectives in object detection (Zhang \\net al. 2022). The model also implements SimOTA, an advanced label assignment strategy, \\nfurther improving its detection capabilities (Liu and Sun 2022). Architecturally (Fig. 14b), \\nYOLOX-DarkNet53 builds upon the YOLOv3-SPP baseline, incorporating enhancements \\nsuch as EMA weights updating, cosine learning rate scheduling, IoU loss, and an IoU-aware \\nFig. 14 Architecture diagram of a YOLONAS; b YOLOX; c YOLOR; d DAMO YOLO; e GOLD YOLO\\n \\n1 3\\n  274  Page 30 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cadec75c-b285-4cc8-a3bc-cfc92246afbb', embedding=None, metadata={'page_label': '31', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nbranch. The decoupled head consists of a 1x1 convolution layer for channel dimension \\nreduction, followed by two parallel branches with two 3x3 convolution layers each (Ashraf \\net al. 2024). This design significantly improves convergence speed and is crucial for end-\\nto-end detection performance. YOLOX demonstrates superior performance across various \\nmodel sizes. The YOLOX-L variant achieves 50.0% AP on COCO at 68.9 FPS on Tesla \\nV100, surpassing YOLOv5-L by 1.8% AP. Even the lightweight YOLOX-Nano, with only \\n0.91M parameters and 1.08 GFLOPs, attains 25.3% AP on COCO, outperforming NanoDet \\nby 1.8% AP. These advancements position YOLOX as a state-of-the-art object detector, \\nbalancing accuracy and efficiency across a wide range of model scales  (Ge 2021).\\n4.5.3 YOLO-R\\nYOLOR (You Only Learn One Representation) is a novel object detection algorithm devel-\\noped by  Chang et al. (2023). Unlike other YOLO versions, YOLOR introduces a novel \\napproach to multi-task learning by unifying implicit and explicit knowledge representation \\n(Andrei-Alexandru et al. 2022). The algorithm’s core concept is inspired by human cogni -\\ntion, aiming to process multiple tasks simultaneously given a single input. YOLOR’s archi-\\ntecture (Fig. 14c) incorporates three key components: kernel space alignment, prediction \\nrefinement, and a CNN with multi-task learning capabilities. This unified network encodes \\nboth implicit knowledge (learned subconsciously from deep layers) and explicit knowledge \\n(obtained from shallow layers and clear metadata), resulting in a more refined and general-\\nized representation  (Sun et al. 2024; Chang et al. 2023). Compared to other YOLO algo -\\nrithms such as YOLOv9, YOLOv5 or YOLOv3, YOLOR significantly improves both speed \\nand accuracy. It achieves comparable object detection accuracy to Scaled YOLOv4 while \\nincreasing inference speed by 88%, making it one of the fastest object detection algorithms \\nin modern computer vision. On the MS COCO dataset, YOLOR outperforms PP-YOLOv2 \\nby 3.8% in mean average precision at the same inference speed  (Chang et al. 2023).\\n4.5.4 DAMO-YOLO\\nDAMO-YOLO is developed by Alibaba’s DAMO Academy, which significantly enhances \\nperformance by integrating novel technologies like Neural Architecture Search (NAS), \\na reparameterized Generalized-FPN (RepGFPN), and lightweight head architectures \\n(Fig. 14d) with AlignedOTA label assignment and distillation enhancement  (Xu et al. 2022). \\nLeveraging MAE-NAS, the model employs a heuristic, training-free approach to architect \\ndetection backbones under strict latency and performance constraints, generating efficient \\nstructures akin to ResNet and CSPNet  (Terven et al. 2023). The neck design emphasizes a \\nrobust “large neck, small head” architecture, optimizing the fusion of high-level semantic \\nand low-level spatial features through an enhanced FPN. This approach effectively bal -\\nances computational efficiency and detection accuracy, particularly notable in its deploy -\\nment across various model scales, from lightweight versions for edge devices to more robust \\nconfigurations for general industry applications. DAMO-YOLO’s architectural prowess is \\nshowcased through impressive performance metrics, achieving mAP scores ranging from \\n43.6 to 51.9 on COCO datasets with relatively low latency on T4 GPUs. Moreover, the mod-\\nel’s lightweight variants demonstrate substantial efficiency on edge devices, underscoring \\nits adaptability and broad application potential. Such capabilities are further augmented by \\n1 3\\nPage 31 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4dcf1a7d-cfd9-418c-852a-b25e0f357fde', embedding=None, metadata={'page_label': '32', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nstrategic enhancements in label assignment and knowledge distillation, addressing common \\nchallenges in object detection like label misalignment and model generalization.\\n4.5.5 Gold-YOLO\\nGold-YOLO was developed by the team at Huawei Noah’s Ark Lab to significantly enhance \\nmulti-scale feature fusion through an innovative Gather-and-Distribute (GD) mecha -\\nnism  (Wang et al. 2024). This mechanism, which utilizes convolution and self-attention \\noperations, was implemented to optimize the exchange and integration of information \\nacross different levels of the feature pyramid. This approach facilitated a more effective \\nbalance between latency and detection accuracy  (Wang et al. 2024). Furthermore, an MAE-\\nstyle unsupervised pretraining was incorporated into the YOLO-series for the first time, \\nwhich was reported to enhance learning efficiency and overall model performance. Gold-\\nYOLO’s architecture (Fig. 14e) aimed to address the limitations inherent in traditional Fea-\\nture Pyramid Networks (FPNs) by preventing recursive information loss and enabling more \\ndirect and efficient feature fusion. This was achieved by a method where features from all \\nlevels were gathered to a central processing node, enhanced, and then redistributed, ensur -\\ning enriched feature maps that retained critical information across scales. The effectiveness \\nof this novel design was demonstrated through good performance metrics; Gold-YOLO \\nachieved a 39.9% AP on the COCO dataset with high throughput speeds on a T4 GPU, \\nsurpassing previous state-of-the-art models like YOLOv6 −3.0-N. The contributions made \\nby this paper were significant, as they not only enhanced the YOLO model’s capabilities to \\nhandle various object sizes and complexities but also established a new benchmark for the \\nintegration of advanced neural network techniques with traditional convolutional architec -\\ntures for real-time applications.\\n5 Applications\\nYOLO has many real-time practical applications (Vijayakumar and Vairavasundaram 2024; \\nChen et al. 2023), such as autonomous vehicles and traffic safety where the technology is \\nused for obstacle detection, pedestrian pose estimation for intention prediction, and traffic \\nsign recognition, enhancing safety and navigation (Gheorghe et al. 2024). Similarly, YOLO \\nis employed in healthcare for detecting anomalies in medical images, aiding in accurate and \\nefficient diagnostics (Vijayakumar and Vairavasundaram 2024; Ragab et al. 2024). Addi-\\ntionally, industrial manufacturing benefits from YOLO’s capabilities, with applications \\nin quality control and defect detection (Li et al. 2022; Hussain 2023), in surveillance for \\nintrusion detection and anomaly identification (Mohod et al. 2022), while in agriculture, \\nit supports crop stress detection, monitoring, and robotic fruit harvesting, among other use \\ncases (Alibabaei et al. 2022; Badgujar et al. 2024; Wang et al. 2021).\\nThe remainder of this section is categorically divided into five key application areas \\nwhere YOLO models have demonstrated significant impact: Autonomous Vehicles and \\nTraffic Safety, Healthcare and Medical Imaging, Security and Surveillance, Industrial Man-\\nufacturing, and Agriculture.\\n1 3\\n  274  Page 32 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3bb41dd6-6b6c-422b-aff7-ae884329dcc7', embedding=None, metadata={'page_label': '33', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\n5.1 Autonomous vehicles and traffic safety\\nEach YOLO version has been pivotal in advancing the capabilities of autonomous vehicles \\nand traffic safety by providing highly efficient and accurate real-time detection systems. \\nEach iteration of YOLO has brought improvements that enhance the vehicle’s ability to \\nperceive its environment quickly and accurately, which is critical for safe navigation and \\ndecision-making (Benjumea et al. 2021; Malligere Shivanna and Guo 2024). Starting with \\nYOLOv1 (Redmon et al. 2016), the YOLO algorithm revolutionized the approach by per -\\nforming detection tasks directly from full images in a single network pass, allowing for the \\ndetection of objects at a remarkable speed (Sarda et al. 2021). This initial model was pivotal, \\nsetting a high standard for real-time object detection and establishing a framework that \\nfuture versions would build upon. Subsequent iterations, including YOLOv2 and YOLOv3, \\ncontinued to refine this approach by introducing concepts such as real-time multi-scale pro-\\ncessing and improved anchor box adjustments, which enhanced the accuracy and robustness \\nof the detections. These versions were particularly adept at handling the variable scales of \\nobjects seen in driving environments-from nearby pedestrians to distant road signs-making \\nthem invaluable for autonomous driving applications. YOLOv4 and later versions further \\npushed the boundaries by integrating advanced neural network techniques and optimiza -\\ntions that improved detection accuracy while maintaining the high-speed processing neces-\\nsary for real-time applications (Cai et al. 2021; Zhao et al. 2022). These advancements in \\nYOLO technology have not only bolstered the capabilities of autonomous vehicles in terms \\nof environmental perception and decision-making but have also significantly contributed to \\nadvancements in automotive safety and operational reliability (Woo et al. 2022).\\nYe et al. ( 2022) developed an end-to-end adaptive neural network control for autono -\\nmous vehicles that predicts steering angles using YOLOv5, enhancing vehicle navigation \\nprecision (Ye et al. 2022). Mostafa et al. ( 2022) compared the effectiveness of YOLOv5, \\nYOLOX, and Faster R-CNN in detecting occluded objects for autonomous vehicles, \\nimproving detection reliability (Mostafa et al. 2022). Jia et al. (2023) proposed an enhanced \\nYOLOv5 detector for autonomous driving, which offers increased speed and accuracy (Jia \\net al. 2023). Chen et al. (2023) utilized an improved YOLOv5-OBB algorithm for autono -\\nmous parking space detection in electric vehicles, enhancing operational efficiency (Chen \\net al. 2023). Liu and Yan (2022) customized YOLOv7 for vehicle-related distance estima -\\ntion, providing essential metrics for safe navigation (Liu and Yan 2022). Mehla et al. (2023) \\nevaluated YOLOv8 against EfficientDet in autonomous maritime vehicles, highlighting the \\nsuperior detection capabilities of YOLOv8 (Mehla et al. 2023).\\nFurther advancements with YOLOv8 have led to significant improvements in object \\ndetection in adverse weather conditions, an area of particular concern for autonomous driv-\\ning. Applying transfer learning techniques using datasets from diverse weather conditions \\nhas markedly increased the detection performance of YOLOv8, ensuring reliable recog -\\nnition of crucial road elements like pedestrians and obstacles under challenging weather \\nscenarios (Kumar and Muhammad 2023). Additionally, the development of YOLOv8 for \\nspecific tasks such as brake light status detection illustrates the algorithm’s flexibility and \\nits potential in enhancing interpretability and safety for autonomous vehicles (Oh and Lim \\n2023). These innovations underscore the critical role of YOLOv8 and YOLOv9 in pushing \\nthe boundaries of what is possible in the autonomous vehicle industry, highlighting their \\nimpact in meeting the rigorous demands for safety and reliability in self-driving technolo -\\n1 3\\nPage 33 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ba7a719d-879f-4eac-81e6-4416eb5a6469', embedding=None, metadata={'page_label': '34', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\ngies (Afdhal et al. 2023). YOLOv8 and YOLOv9 are at the forefront of transforming the \\nlandscape of autonomous vehicle technologies, playing a pivotal role in enhancing the oper-\\national safety and efficiency of self-driving cars. These models have excelled in real-time \\nobject detection, a crucial aspect of autonomous driving, especially under the challenging \\nand variable conditions typical in real-world traffic environments. An enhanced version, \\naddresses the need for detecting smaller objects such as traffic signs and signals, demon -\\nstrating its utility with a notable accuracy rate and efficiency in processing, making it ideal \\nfor high-speed driving scenarios (Wang et al. 2024). Table 2 illustrates different applications \\nof YOLO in the autonomous vehicle industry, presented in reverse chronological order from \\nthe most recent versions to the older ones.\\n5.1.1 Pedestrian pose estimation\\nAli et al. ( 2023) presented a Bayesian Generalized Extreme Value Model to evaluate real-\\ntime pedestrian crash risks at signalized intersections, leveraging advanced AI-based video \\nanalytics. This framework employs deep learning algorithms like YOLO for precise object \\ndetection and DeepSORT for effective tracking. The model concentrates on crucial safety \\nindicators such as Post Encroachment Time (PET). Through this approach, the study under-\\nscores the significant role of AI-driven video analysis in boosting intersection safety by \\ndelivering real-time risk assessments. This development signifies a substantial advancement \\nin the proactive management of traffic safety. Hussain et al. (2024) explored the enhance -\\nment of pedestrian crash estimation using machine learning techniques focused on anomaly \\ndetection. Their study addresses the limitations of traditional Extreme Value Theory (EVT) \\nmodels by applying unconventional sampling methods, thereby increasing the accuracy and \\nreducing uncertainty in crash risk estimations. The use of YOLO for object detection and \\nDeepSORT for tracking is pivotal in this methodology, enhancing detection accuracy and \\ntracking reliability in real-time scenarios. Ghaziamin et al. (2024) developed a privacy-pre-\\nserving real-time passenger counting system for bus stops using overhead fisheye cameras. \\nThis innovative system employs YOLOv4, Detecnet-V2 and Faster-RCNN for detection \\npurposes and DeepSORT for tracking. The system processes data in real-time at 30 frames \\nper second (FPS) when utilizing YOLOv4 as the detection model. This technology signifi -\\ncantly enhances transit planning by providing accurate passenger counts, while also main -\\ntaining passenger privacy and energy efficiency.\\nAdditionally, Pedestrian-vehicle conflict prediction was explored by Zhang et al. (2020) \\nproposed a model employing a Long Short-Term Memory (LSTM) neural network to fore-\\ncast pedestrian-vehicle conflicts at signalized intersections by analyzing video data. The \\nmodel uses YOLOv3 for object detection and Deep SORT for tracking, achieving impres -\\nsive accuracy rates and demonstrating the transformative potential of LSTM networks in \\ncollision warning systems. This approach suggests a proactive enhancement of pedestrian \\nsafety in connected vehicle environments.\\nCrossing intention prediction and behavioral analysis was explored by Zhang et al. (2020) \\nutilized an LSTM neural network to predict pedestrian red-light crossing intentions at inter-\\nsections by analyzing video data of real traffic scenarios. The model uses YOLOv3 for \\ndetection and DeepSORT for tracking, recognizing patterns that indicate potential red-light \\ncrossings with a high accuracy rate. This capability aims to improve traffic safety through \\nvehicle-to-infrastructure communication systems that alert drivers to potential pedestrian \\n1 3\\n  274  Page 34 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4b3ae303-8cd2-4b88-9fd1-1b3a640179a6', embedding=None, metadata={'page_label': '35', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nTable 2 Studies on YOLO applications focus on object detection and real-time performance improvements to enhance autonomous vehicles and traffic safety\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. and \\nYear\\n“Multi-Class Vehicle Detection and \\nClassification with YOLO11 on UA V-\\nCaptured Aerial Imagery”\\nUtilizes YOLO11 for real-time vehicle detec-\\ntion and classification in UA V-captured traffic \\nimages, with a comparative analysis against \\nYOLOv10\\nFocuses on robust vehicle detection in \\ndynamic UA V-captured images, enhancing \\nreal-time processing and accuracy in aerial \\ntraffic monitoring\\nYOLO11 Ba-\\nkirci et al. \\n(2024), \\n2024\\n“YOLOv10-Based Real-Time Pedestrian \\nDetection for Autonomous Vehicles”\\nPresents a real-time pedestrian detection \\nmethod enhancing YOLOv10 with Efficient-\\nNet backbone, C2F-DM, BiFormer, and \\nmulti-scale feature fusion detection head for \\ncomplex environments\\nAims to enhance autonomous vehicle safety \\nby providing efficient multi-scale pedestrian \\ndetection\\nYOLOv10 Li et al. \\n(2024), \\n2024\\n“Improved YOLOv10 for Visually \\nImpaired: Balancing Model Accuracy \\nand Efficiency in the Case of Public \\nTransportation”\\nIntroduces Improved-YOLOv10 with Coordi-\\nnate Attention and Adaptive Kernel Convolu-\\ntion for enhanced bus detection and POV \\nclassification for the visually impaired\\nEnhances accessibility in public transporta-\\ntion for visually impaired individuals through \\nimproved detection and efficiency\\nYOLOv10 Ari-\\nfando et al. \\n(2025), \\n2025\\n“Transforming Aircraft Detection \\nThrough LEO Satellite Imagery and \\nYOLOv9 for Improved Aviation Safety”\\nUtilizes YOLOv9 with LEO satellite imagery \\nfor enhanced aircraft detection in wide-area \\nairport environments\\nAims to improve airport security and aviation \\nsafety by integrating advanced YOLO-based \\nobject detection with satellite imagery\\nYOLOv9 Bakirci and \\nBayraktar \\n(2024), \\n2024\\n“YOLOv8-QSD: An Improved Small \\nObject Detection Algorithm for Autono-\\nmous Vehicles Based on YOLOv8”\\nDeveloped an anchor-free, BiFPN-enhanced \\nYOLOv8 model for better small object detec-\\ntion in driving scenarios\\nEnhances detection of small objects for \\nautonomous vehicles with reduced computa-\\ntional demands, tested on SODA-A dataset\\nYOLOv8-QSD Wang et al. \\n(2024), \\n2024\\n“Object Detection in Dense and Mixed \\nTraffic for Autonomous Vehicles With \\nModified YOLO”\\nAdapted YOLOv7 with deformable layers \\nand softNMS for object detection in heavy \\nIndonesian traffic\\nEnhances detection and classification of \\nobjects around autonomous vehicles using a \\nmodified YOLOv7, tested on a novel Indone-\\nsian traffic dataset\\nYOLOv7-MOD Wi-\\nbowo et al. \\n(2023), \\n2023\\n“Object Tracking for Autonomous Ve-\\nhicle Using YOLOV3”\\nEvaluated YOLOv3 for object tracking in \\nautonomous vehicles\\nTwo models were provided, one trained using \\nonly the online COCO dataset and the other \\nwith additional images from various locations \\nat Universiti Malaysia Pahang (UMP).\\nYOLOv3 Hung et al. \\n(2022), \\n2022\\n“The improvement in obstacle detec-\\ntion in autonomous vehicles using \\nYOLO non-maximum suppression fuzzy \\nalgorithm”\\nEmployed a hybrid of fuzzy logic and NMS \\nin YOLO for better obstacle detection in \\nautonomous driving\\nEnhances obstacle detection accuracy and \\nspeed using a modified YOLO algorithm\\nYOLOv3 Zaghari \\net al. \\n(2021), \\n2021\\n1 3\\nPage 35 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7503085d-a5cc-4833-9659-9ede77324866', embedding=None, metadata={'page_label': '36', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nviolations, thereby preventing accidents. Yang et al. (2022) introduced the VENUS smart \\nnode, a cooperative traffic signal assistance system for non-motorized users and individu -\\nals with disabilities. This novel infrastructure leverages computer vision and edge AI to \\nintegrate real-time data on pedestrian movement and intent. The system employs YOLOv4 \\nfor detection and OpenPose for pose estimation, achieving high accuracy in detecting cross-\\ning intentions and mobility status across various test sites. This innovation has significant \\npotential for widespread use in smart city infrastructures, greatly enhancing safety and \\naccessibility.\\nJiao and Fei ( 2023) conducted a study on monitoring pedestrian walking speeds at the \\nstreet level using drones. The research utilized UA V-based video footage to measure the \\nwalking speeds of pedestrians on a commercial street. Deep learning algorithms, particularly \\nYOLOv5 for object detection and DeepSORT for tracking, were employed in this study. \\nSpeed calculations were adjusted for geometric distortions using the SIFT and RANSAC \\nalgorithms, achieving high accuracy. The study found that 90.5% of the corrected speeds \\nhad an absolute error of less than 0.1 m/s, providing a precise and non-intrusive method \\nfor analyzing pedestrian walking speeds. Wang et al. ( 2024) used drone-captured video \\nfootage to examine “safe spaces” for pedestrians and e-bicyclists at urban crosswalks. The \\nstudy discovered that e-bicyclists maintain larger semi-elliptical safe zones that are sensi -\\ntive to speed changes compared to the semi-circular zones maintained by pedestrians. By \\nquantifying these safe spaces and examining variations due to speed and traffic presence, \\nthe study offers valuable insights for enhancing crosswalk safety and managing urban traf -\\nfic more effectively. The use of YOLOv3 for object detection and DeepSORT for tracking \\nplays a critical role in this analysis. Zhou et al. ( 2023) developed an innovative model \\nthat integrates a pedestrian-centric environment graph with Graph Convolutional Networks \\n(GCNs) and a pedestrian-state encoder. This model effectively captures dynamic interac -\\ntions between pedestrians and their environments, providing advanced safety warnings by \\npredicting crossing intentions up to three seconds in advance. This model holds significant \\npotential for applications in intelligent transportation systems. The integration of YOLOv5 \\nfor detection, DeepSORT for tracking, and HRNet for pose estimation enhances the model’s \\npredictive accuracy and real-time application. Table 3 illustrates different applications of \\nYOLO usage in pedestrian pose estimation, for intention prediction and behavioral analysis.\\n5.1.2 Traffic sign detection\\nTraffic sign detection and recognition systems play a pivotal role in enhancing road safety \\nand are essential for the advancement of autonomous driving. These systems enable drivers, \\nor autonomous vehicles, to effectively respond to road conditions, ensuring the safety of all \\nroad users (Flores-Calero et al. 2024). However, the complexity and variability of traffic \\nenvironments, such as adverse weather conditions, combined with the small size of traffic \\nsigns present significant challenges for accurately detecting small traffic signs in real-world \\nscenarios (Li et al. 2023; Mahaur and Mishra 2023; Zhang et al. 2020)\\nFor instance, Li et al. ( 2022) presented a classical YOLO-based architecture for traffic \\nsign recognition. First, traffic signs are categorized and preprocessed according to their spe-\\ncific characteristics. The processed images are then input into an optimized convolutional \\nneural network for finer category classification. The proposed recognition algorithm was \\ntested using a dataset based on the German traffic sign recognition standard, and its per -\\n1 3\\n  274  Page 36 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e2df29b1-6cd5-41cf-9258-b615530850f5', embedding=None, metadata={'page_label': '37', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. \\nand \\nYear\\n“Multi-Object Pedestrian Track-\\ning Using Improved YOLOv8 and \\nOC-SORT”\\nProposes a comprehensive approach for pedestrian tracking by \\ncombining the improved YOLOv8 object detection algorithm with \\nthe OC-SORT tracking algorithm, integrating advanced techniques \\nsuch as SoftNMS, GhostConv, and C3Ghost Modules\\nAimed to enhance multi-object pedestrian \\ntracking for autonomous driving systems by \\nimproving detection accuracy and model ef-\\nficiency with YOLOv8, and integrating it with \\nthe OC-SORT tracking algorithm for robust \\ntracking in challenging scenarios\\nYOLOv8 Xiao \\nand \\nFeng \\n(2023), \\n2023\\n“Revisiting The Hybrid Approach \\nof Anomaly Detection and \\nExtreme Value Theory for Esti-\\nmating Pedestrian Crashes Using \\nTraffic Conflicts Obtained from \\nAI-Based Video Analytics”\\nFocuses on improving pedestrian crash predictions by utilizing \\nmachine learning for anomaly detection and integrating it with \\nExtreme Value Theory (EVT). The approach leverages YOLOv7 \\nfor extracting traffic conflicts and relevant data, providing a robust \\nframework for predicting pedestrian crashes from traffic incidents\\nYOLOv7 is used for the extraction of traffic \\nconflicts and relevant data needed for the EVT \\nmodel\\nYOLOv7 Hus-\\nsain \\net al. \\n(2024), \\n2024\\n“Pedestrian Walking Speed \\nMonitoring at Street Scale by An \\nIn-Flight Drone”\\nThis study introduces a method for measuring pedestrian walking \\nspeeds on a commercial street using drone video. Pedestrians \\nare detected and tracked using YOLOv5 and the DeepSORT \\nalgorithm, with distance calculations performed using Scale-\\nInvariant Feature Transform (SIFT) and random sample consensus \\n(RANSAC) algorithms, followed by geometric correction\\nProvide an accurate and cost-effective method \\nfor monitoring pedestrian walking speeds over \\nlarge areas. YOLOv5 is utilized for pedestrian \\ndetection in drone footage, which is crucial for \\ntracking and speed calculation\\nYOLOv5 Jiao \\nand \\nFei \\n(2023), \\n2023\\n“Pedestrian Crossing Intention \\nPrediction From Surveillance \\nVideos For Over-The-Horizon \\nSafety Warning”\\nPrediction of pedestrian crossing intentions using surveillance \\ncamera footage. The framework constructs a pedestrian-centric \\nenvironment graph, uses a Graph Convolutional Network (GCN) \\nfor environment encoding, and employs a pedestrian-state encoder \\nfor extracting behaviour features. An intention prediction decoder \\nis then used to determine crossing probabilities\\nYOLOv5 is used to identify and track pedestri-\\nans within the surveillance footage, facilitating \\nthe extraction of visual and behavioural fea-\\ntures necessary for the prediction framework\\nYOLOv5 Zhou \\net al. \\n(2023), \\n2024\\n“A Bayesian Generalised Extreme \\nValue Model to Estimate Real-\\nTime Pedestrian Crash Risks at \\nSignalised Intersections Using \\nArtificial Intelligence-Based \\nVideo Analytics”\\nDevelop a Bayesian Generalised Extreme Value (GEV) model for \\nestimating real-time pedestrian crash risks at signalized intersec-\\ntions. The model uses AI-based video analytics to identify and \\ntrack vehicles and pedestrians, extracting traffic conflicts and \\nrelevant covariates for risk assessment\\nIdentify crash-prone conditions and implement \\ntimely risk mitigation strategies to enhance \\npedestrian safety at intersections. YOLOv4 is \\nused to detect and track pedestrians and ve-\\nhicles, facilitating the extraction of necessary \\ndata for the Bayesian EVT model\\nYOLOv4 Ali \\net al. \\n(2023), \\n2022\\nTable 3 Studies on YOLO usage in pedestrian pose estimation, for intention prediction and behavioral analysis\\n1 3\\nPage 37 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e117bd80-c2fe-4651-8d5e-279863597c10', embedding=None, metadata={'page_label': '38', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. \\nand \\nYear\\n“A Privacy-Preserving Edge \\nComputing Solution For Real-\\nTime Passenger Counting At Bus \\nStops Using Overhead Fisheye \\nCamera”\\nDevelopment of a privacy-preserving edge computing solution for \\nreal-time passenger counting at bus stops using fisheye camera \\nfootage. The system leverages YOLOv4 for passenger detection \\nand is designed to operate efficiently on edge devices powered by \\nsolar panels\\nYOLOv4 creates an automated, efficient, and \\nprivacy-respecting passenger counting system \\nsuitable for smart city bus stops. Model is \\nidentified as the superior object detection \\nmodel in this study, outperforming DetectNet \\nand Faster-RCNN\\nYOLOv4 Gha-\\nziamin \\net al. \\n(2024), \\n2024\\n“Cooperative Traffic Signal Assis-\\ntance System for Non-Motorized \\nUsers and Disabilities Empow-\\nered by Computer Vision and \\nEdge Artificial Intelligence”\\nCreate the VENUS smart node, a system designed to aid non-mo-\\ntorized and disabled users at intersections. The system integrates \\ncomputer vision and edge AI technologies for object recognition, \\nuser localization, and pose direction estimation, ensuring accurate \\ndetection and tracking of various users\\nEnhancing traffic signal assistance for non-\\nmotorized and disabled users by providing \\nreal-time data and interaction capabilities. \\nYOLOv4 is used for object recognition, user \\nlocalization, and pose direction estimation of \\nnon-motorized users\\nYOLOv4 Yang \\net al. \\n(2022), \\n2022\\n“Examining Safe Spaces for \\nPedestrians and E-Bicyclists at \\nUrban Crosswalks: An Analysis \\nBased on Drone-Captured Video”\\nAssess the safety zones for pedestrians and e-bicyclists at urban \\ncrosswalks using drone footage. YOLOv3 is employed for ac-\\ncurate identification and tracking of individuals, allowing for \\ndetailed analysis of their movements and interactions\\nEnhancing the safety of pedestrians and e-\\nbicyclists at crosswalks by analyzing their safe \\nspaces. YOLOv3 is used for the study of user’s \\ndynamic interactions\\nYOLOv3 Wang \\net al. \\n(2024), \\n2024\\n“Forecast Pedestrian-Vehicle Col-\\nlisions at Traffic Lights”\\nImplements YOLOv3 for detecting pedestrian-vehicle interactions \\nand classifying them into safe interactions, slight conflicts, and \\nsevere conflicts\\nEnhance pedestrian safety at intersections by \\nmodelling and predicting potential pedestrian-\\nvehicle conflicts\\nYOLOv3 Zhang \\net al. \\n(2020), \\n2020\\n“Prediction of Pedestrian Cross-\\ning Intentions at Intersections \\nBased On Long Short-Term \\nMemory Recurrent Neural \\nNetwork”\\nPrediction pedestrian red-light crossing behaviour at intersections \\nusing LSTM networks. YOLOv3 is employed to detect pedestrians \\nand extract relevant characteristics from video data, which are \\nthen used for behavioural prediction\\nYOLOv3 is used to identify pedestrians and \\nextract relevant characteristics from the video \\ndata, which are then passed into the LSTM \\nneural network for prediction\\nYOLOv3 Zhang \\net al. \\n(2020), \\n2020\\nTable 3 (continued)\\n \\n1 3\\n  274  Page 38 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1f5fef85-4a40-43b0-8d98-63dd2d34efc6', embedding=None, metadata={'page_label': '39', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nformance was compared with other baseline algorithms. Results show that the algorithm \\nsignificantly improves processing speed while maintaining high classification accuracy, \\nmaking it better suited for traffic sign recognition systems.\\nZhang et al. (2017) presented a Chinese traffic sign detection algorithm based on a deep \\nconvolutional network. To enable real-time detection, they proposed an end-to-end con -\\nvolutional network inspired by YOLOv2. Considering the characteristics of traffic signs, \\nthey incorporated multiple 1 ×1 convolutional layers in the intermediate network layers \\nwhile reducing the number of convolutional layers in the top layers to decrease computa -\\ntional complexity. For effective small traffic sign detection, the input images are divided into \\ndense grids to capture finer feature maps. Additionally, they expanded the Chinese Traffic \\nSign Dataset (CTSD) and enhanced the marker information available online. Experimental \\nresults using both the expanded CTSD and the German Traffic Sign Detection Benchmark \\n(GTSDB) demonstrate that the proposed method is faster and more robust.\\nOn the other hand, Zhang et al. (2020) proposed a new detection scheme, MSA_YOLOv3, \\nfor accurate real-time localization and classification of small traffic signs. The approach \\nbegins with data augmentation using image mixup technology. A multi-scale spatial pyra -\\nmid pooling block is incorporated into the Darknet53 network, enabling more comprehen -\\nsive learning of object features. Additionally, a bottom-up augmented path is designed to \\nenhance the feature pyramid in YOLOv3, allowing effective utilization of fine-grained fea-\\ntures in the lower layers for precise object localization. Tests on the TT100K dataset show \\nthat MSA_YOLOv3 outperforms YOLOv3 in detecting small traffic signs.\\nRecently, later versions of YOLO are being applied to detect traffic signs. For exam -\\nple, Mahaur and Mishra ( 2023) presented a new version called iS-YOLOv5 model, which \\nincreases the mean Average Precision (mAP) by 3.35% on the BDD100K dataset to detect \\ntraffic sign and traffic lights. While, Bai et al. (2023) introduced two innovative traffic sign \\ndetection models, called YOLOv5-DH and YOLOv5-TDHSA, based on the YOLOv5s \\nmodel with the following modifications (YOLOv5-DH uses only the second modification): \\n(1) replacing the last layer of the ‘Conv + Batch Normalization + SiLU’ (CBS) structure \\nin the YOLOv5s backbone with a transformer self-attention module (T in the YOLOv5-\\nTDHSA’s name), and also adding a similar module to the last layer of its neck, so that the \\nimage information can be used more comprehensively, (2) replacing the YOLOv5s coupled \\nhead with a decoupled head (DH in both models’ names) to increase the detection accu -\\nracy and speed up the convergence, and (3) adding a small-object detection layer (S in the \\nYOLOv5-TDHSA’s name) and an adaptive anchor (A in the YOLOv5-TDHSA’s name) to \\nthe YOLOv5s neck to improve the detection of small objects. Their experiments were con-\\nducted using the TT100K dataset.\\nSimilarly, Li et al. ( 2023) proposed a small object detection algorithm for traffic signs \\nbased on the improved YOLOv7 called SANO-YOLOv7. First, the small target detection \\nlayer in the neck region was added to augment the detection capability for small traffic sign \\ntargets. Simultaneously, the integration of self-attention and convolutional mix modules \\n(ACmix) was applied to the newly added small target detection layer, enabling the cap -\\nture of additional feature information through the convolutional and self-attention channels \\nwithin ACmix. Furthermore, the feature extraction capability of the convolution modules \\nwas enhanced by replacing the regular convolution modules in the neck layer with omni-\\ndimensional dynamic convolution (ODConv). To further enhance the accuracy of small tar-\\nget detection, the normalized Gaussian Wasserstein distance (NWD) metric was introduced \\n1 3\\nPage 39 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3d19131c-552b-4209-8285-7af2be018056', embedding=None, metadata={'page_label': '40', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nto mitigate the sensitivity to minor positional deviations of small objects. SANO-YOLOv7 \\nwas trained on the highly challenging TT100K public dataset.\\nMore interesting traffic sign detection studies were presented with YOLOv8 and later \\nYOLO versions. In the Robotaxi-Full Scale Autonomous Vehicle Competition, YOLOv8 \\nwas specifically adapted to recognize and interpret traffic signs, providing real-time alerts \\nthat are essential for safe driving (Soylu and Soylu 2024). Next, Zhang ( 2024) introduced \\nan enhanced traffic sign detection algorithm based on YOLOv9. AKConv replaces the Conv \\nmodule in RepNCSPELAN4, maintaining detection accuracy while reducing weight. Focal-\\nEIoU Loss replaces the original regression loss function, Clou Loss, accelerating conver -\\ngence and improving accuracy by dividing the aspect ratio’s loss term into the difference \\nbetween the minimum outer frame’s width and height and the predicted width and height. \\nAdditionally, the network’s feature extraction capability and detection accuracy are fur -\\nther strengthened by incorporating the Convolutional Block Attention Module (CBAM) \\nattention mechanism. The public TT100K traffic sign dataset was used for training and \\nevaluation.\\nUp until now, YOLO versions 10, 11 and 12 have not been applied in detecting traffic \\nsigns. This presents an exciting opportunity for future research and application, as these \\nversions may offer improvements in accuracy, speed, and adaptability for detecting traffic \\nsigns in complex environments (4).\\n5.2 Healthcare and medical imaging\\nYOLO has marked a significant technological advancement in healthcare applications as \\nwell, especially with the introduction of newer versions such as YOLOv7 and YOLOv8 \\n(Pandey et al. 2023; Ju and Cai 2023; Inui et al. 2023). The recent iterations of YOLO, \\nparticularly YOLOv7, YOLOv8, and YOLOv9, could significantly enhance medical \\ndiagnostics by offering advanced computational efficiency and improved feature extrac -\\ntion capabilities, making them suitable for real-time medical imaging applications. Such \\ncapabilities are crucial in urgent care scenarios, where swift diagnosis can be pivotal. For \\ninstance, YOLOv8’s sophisticated algorithms excel in accurately delineating complex bio -\\nlogical structures, vital for identifying pathologies in conditions like vascular diseases or \\ntumors. Similarly, YOLOv9’s rapid processing power enables immediate analysis of medi-\\ncal images, essential in emergency medical responses where timely intervention is critical. \\nThese versions have the potential to revolutionize healthcare by facilitating early detec -\\ntion of diseases and supporting continuous patient monitoring, transforming the traditional \\napproach of healthcare diagnostics into one that integrates accurate, swift diagnostics seam-\\nlessly with routine medical examinations. Unlike the traditional methods which depend \\nheavily on manual annotation and are prone to errors and subjectivity, YOLO algorithms \\nautomate the detection and localization of medical anomalies such as tumors, lesions, and \\nother pathological markers across various imaging modalities. This automation is driven \\nby YOLO’s unique architecture that efficiently predicts multiple bounding boxes and class \\nprobabilities in a single analysis, enhancing diagnostic accuracy and reducing the potential \\nfor human error.\\nIn the field of medical imaging and diagnostics, the adoption of the YOLO object detec-\\ntion algorithm has showcased promising improvements in accuracy and efficiency, particu-\\nlarly with its latest versions like YOLOv5, YOLOv6, YOLOv7, and YOLOv8. For instance, \\n1 3\\n  274  Page 40 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='591f5aeb-be00-491c-816a-eeaaa132afc3', embedding=None, metadata={'page_label': '41', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. \\nand \\nYear\\n“FINE-GRAINED \\nCLASSIFICATION OF \\nMILITARY AIRCRAFT \\nUSING PRE-TRAINED \\nDEEP LEARN-\\nING MODELS AND \\nYOLO11”\\nExamines the use of pre-trained CNN models and \\nYOLO11 for fine-grained classification of military \\naircraft, using a dataset of 24,164 images for training and \\n6,042 for testing\\nAims to enhance military aviation applications by achieving \\naccurate aircraft identification and tail number extraction \\nusing advanced AI-powered image recognition\\nYOLO11x-cls Karaca \\nand \\nAtasoy \\n(2025), \\n2025\\n“Research on traffic \\nsign detection based on \\nimproved YOLOv9”\\nAn enhanced traffic sign detection algorithm based on \\nYOLOv9 was introduced to address issues of low accu-\\nracy, missed detection, and the omission of small targets \\nin complex environments. The TT100K traffic sign \\ndataset was used for training and evaluation\\nYOLOv9 was modified by replacing the Conv module with \\nAKConv, reducing weight while maintaining accuracy. \\nFocal-EIoU Loss improves convergence and accuracy, and \\nthe CBAM attention mechanism enhances feature extraction \\nand detection performance\\nYOLOv9 Zhang \\n(2024), \\n2024\\n“A performance compar-\\nison of YOLOv8 models \\nfor traffic sign detection \\nin the Robotaxi-full scale \\nautonomous vehicle \\ncompetition”\\nTo develop a traffic sign recognition system using \\nYOLOv8 for real-time detection and classification of \\nobjects, and to compare the performance of YOLOv8 \\nmodels for traffic sign detection within the Robotaxi-full \\nframework\\nYOLOv8 was trained on a dataset of traffic sign images to \\ndevelop a model capable of accurately recognizing and clas-\\nsifying various types of traffic signs\\nYOLOv8 Soylu \\nand \\nSoylu \\n(2024), \\n2024\\n“A Small Object Detec-\\ntion Algorithm for \\nTraffic Signs Based on \\nImproved YOLOv7”\\nA traffic sign detection algorithm for small-sized signs \\nwas proposed, built upon the enhanced YOLOv7 model, \\nimproving its ability to accurately identify and classify \\nsmall traffic signs in challenging environments. Their \\neffectiveness was validated on a public dataset\\nYOLOv7 was enhanced with a small target detection layer \\nincorporating ACmix modules for improved feature extrac-\\ntion via self-attention and convolution. ODConv replaced \\nstandard convolution in the neck layer, boosting feature \\nextraction. Additionally, the NWD metric was introduced to \\nreduce sensitivity to positional deviations, improving small \\ntraffic sign detection accuracy\\nYOLOv7 Li \\net al. \\n(2023), \\n2023\\n“Local Regression \\nBased Real-Time Traffic \\nSign Detection using \\nYOLOv6”\\nThe capabilities of YOLOv6 are combined with an \\noptimized Logistic Regression (LR)-based classifier, \\ndelivering improved accuracy and performance tailored \\nto resource-constrained environments\\nYOLOv6 is well-suited for applications on embedded sys-\\ntems and smartphones, where power efficiency is a critical \\nconstraint\\nYOLOv6 Kaur \\nand \\nSingh \\n(2022), \\n2022\\nTable 4 Studies on YOLO usage in traffic sign detection and recognition\\n1 3\\nPage 41 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6f89899b-6dee-489c-9a47-572425360198', embedding=None, metadata={'page_label': '42', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. \\nand \\nYear\\n“Two Novel Models for \\nTraffic Sign Detection \\nBased on YOLOv5s”\\nBased on the YOLOv5s architecture, two novel traffic \\nsign detection models, YOLOv5-DH and YOLOv5-\\nTDHSA, have been proposed. Their effectiveness was \\nvalidated on a public dataset\\nProposed YOLOv5-DH and YOLOv5-TDHSA, enhancing \\nYOLOv5s with transformer self-attention modules, decou-\\npled heads, small-object detection layers, and adaptive an-\\nchors. YOLOv5-DH focuses on accuracy and convergence, \\nwhile YOLOv5-TDHSA improves small object detection\\nYOLOv5 Bai \\net al. \\n(2023), \\n2023\\n“Small-object detec-\\ntion based on YOLOv5 \\nin autonomous driving \\nsystems”\\nInvestigated and refined YOLOv5 for improved detection \\nof small objects such as traffic signs and traffic lights, \\ntested on BDD100K, TT100K, and DTLD datasets\\nIntroduced architectural changes to the popular YOLOv5 \\nmodel to improve its performance in the detection of small \\nobjects without sacrificing the detection accuracy of large \\nobjects\\nYOLOv5 Ma-\\nhaur \\nand \\nMishra \\n(2023), \\n2023\\n“Deep convolutional \\nneural network for \\nenhancing traffic sign \\nrecognition developed on \\nYOLO V4”\\nAnalyzed YOLO V4 and YOLO V4-tiny with SPP for \\nbetter feature extraction in traffic sign recognition\\nCompared improving traffic sign recognition performance by \\nintegrating SPP into YOLO V4 backbones\\nYOLOv4 Dewi \\net al. \\n(2022), \\n2022\\n“Real-Time Detec-\\ntion Method for Small \\nTraffic Signs Based on \\nYOLOv3”\\nYOLOv3 was enhanced for real-time localization and \\nclassification of small traffic signs, effectively utilizing \\nfine-grained features in the lower layers for accurate \\nobject detection. Experiments were conducted using the \\nTT100K dataset\\nMSA_YOLOv3, was proposed for real-time localization and \\nclassification of small traffic signs. It incorporates image \\nmixup for data augmentation, integrates a multi-scale spatial \\npyramid pooling block into Darknet53 for comprehensive \\nfeature learning, and enhances YOLOv3’s feature pyra-\\nmid with a bottom-up augmented path for accurate object \\nlocalization\\nYOLOv3 Zhang \\net al. \\n(2020), \\n2020\\n“A real-time Chinese \\ntraffic sign detection \\nalgorithm based on \\nmodified YOLOv2”\\nA Chinese traffic sign detection algorithm based on \\nYOLOv2 was presented. To effectively detect small \\ntraffic signs, input images are divided into dense grids to \\ncapture finer feature maps. Experiments were conducted \\nusing the Chinese Traffic Sign Dataset (CTSD)\\nYOLOv2 was modified by adding multiple 1×1 convo-\\nlutional layers in the intermediate layers and reducing the \\nconvolutional layers in the top layers to reduce computa-\\ntional complexity. To enhance small traffic sign detection, \\ninput images are divided into dense grids, capturing finer \\nfeature maps\\nYOLOv2 Zhang \\net al. \\n(2017), \\n2017\\nTable 4 (continued)\\n \\n1 3\\n  274  Page 42 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f5330bb0-54d3-4bda-bb09-6042d3c5cfa0', embedding=None, metadata={'page_label': '43', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. \\nand \\nYear\\n“YOLO-Based Traf-\\nfic Sign Recognition \\nAlgorithm”\\nA modified YOLO architecture for traffic sign recogni-\\ntion, which categorizes and preprocesses traffic signs \\nbefore inputting them into an optimized CNN for finer \\nclassification. Tested on a German traffic sign dataset, the \\nalgorithm improves speed and accuracy, outperforming \\nbaseline models for traffic sign recognition systems\\nA classical YOLO architecture was modified by enhancing \\nthe pooling layer, which reduces the spatial resolution of \\nthe input and eliminates redundant information, improving \\nefficiency and focus on relevant features\\nYOLO Li \\net al. \\n(2022), \\n2022\\nTable 4 (continued)\\n \\n1 3\\nPage 43 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4b171806-91f0-48fc-b92f-cab1eb5f7544', embedding=None, metadata={'page_label': '44', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nLuo et al. (2021) leveraged YOLOv5 in conjunction with ResNet50 to enhance chest abnor-\\nmality detection, demonstrating the algorithm’s proficiency in identifying subtle medical \\nconditions (Luo et al. 2021). Similarly, Wu et al. (2022) developed Me-YOLO, an adapted \\nversion of YOLOv5, to improve the detection of medical personal protective equipment, \\nhighlighting the model’s adaptability to varied medical use cases (Wu et al. 2022). More-\\nover, advancements like the CSFF-YOLOv5 by Zhao et al. ( 2024) introduced modifica -\\ntions for better feature fusion, significantly boosting the detection accuracy in femoral neck \\nfracture cases (Zhao et al. 2024). This specificity is further explored by Goel and Patel \\n(2024), who enhanced YOLOv6 for lung cancer detection using an advanced PSO opti -\\nmizer, underscoring the potential of YOLO algorithms in facilitating early disease diagnosis \\nand treatment (Goel and Patel 2024). Additionally, the extension of YOLOv6 by Norko -\\nbil Saydirasulovich et al. ( 2023) for improved fire detection in smart city environments \\nexemplifies the algorithm’s versatility beyond traditional medical applications, proving its \\nefficacy in diverse environmental conditions (Norkobil Saydirasulovich et al. 2023). Each \\nof these developments not only enhances specific medical diagnostic processes but also \\npaves the way for integrating these advanced object detection systems into broader health -\\ncare applications, as illustrated by the innovative uses of YOLOv7 and YOLOv8 in detect-\\ning whole body bone fractures and enhancing hospital efficiency (Zou and Arshad 2024; \\nSalinas-Medina and Neme 2023). These studies collectively demonstrate the significant \\nadvancements brought by YOLO in the healthcare sector, ensuring more precise, efficient, \\nand versatile diagnostic solutions.\\nRecent versions such as YOLOv7, YOLOv8 and YOLOv9 have been effectively demon-\\nstrated across a variety of healthcare applications. Razaghi et al. ( 2024) utilized YOLOv8 \\nfor the innovative diagnosis of dental diseases, highlighting its precision in identifying den-\\ntal pathologies (Razaghi et al. 2024). Similarly, Pham and Le ( 2024) leveraged YOLOv8 \\nfor the detection and classification of ovarian tumors from ultrasound images, showcas -\\ning the model’s adaptability to different medical imaging modalities (Pham and Le 2024). \\nKrishnamurthy et al. ( 2023) applied custom YOLO architectures to enhance object detec -\\ntion capabilities during endoscopic surgeries, illustrating the potential of YOLO in surgical \\nsettings (Krishnamurthy et al. 2023). Furthermore, Palanivel et al. ( 2023) discussed the \\napplication of YOLOv8 in cancer diagnosis through medical imaging, further cementing \\nYOLO’s role in critical healthcare applications (Palanivel et al. 2023).\\nContinuing with advancements, Karaköse et al. ( 2024) introduced CSFF-YOLOv5, an \\nimproved YOLO model for femoral neck fracture detection, utilizing advanced feature \\nfusion techniques (Karaköse et al. 2024). Inui et al. (2023) demonstrated YOLOv8’s effec-\\ntiveness in detecting elbow osteochondritis dissecans in ultrasound images, which supports \\nits use in orthopedic diagnostics (Inui et al. 2023). Bhojane et al. (2023) employed YOLOv8 \\nfor detecting liver lesions from MRI and CT images, underscoring the algorithm’s capabil-\\nity across various imaging technologies (Bhojane et al. 2023). Additionally, Zhang et al. \\n(2023) developed an improved detection model for microaneurysms using YOLOv8, which \\nillustrates continuous enhancements in YOLO’s application to highly specific medical tasks \\n(Zhang et al. 2023).\\nTable 5 illustrates the different uses of YOLO versions in security and survelliance:\\n1 3\\n  274  Page 44 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='91a8d919-2ec4-4f5d-8e00-f9f0d1fb762e', embedding=None, metadata={'page_label': '45', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\n5.3 Security and surveillance\\nIn the ever-evolving field of security systems, YOLO’s application extends to detecting \\nunauthorized entries and identifying potential threats swiftly, thereby bolstering security \\nmeasures (Majeed et al. 2022; Aboah et al. 2021). Recent YOLO models such as YOLOv6 \\nenhanced security and surveillance applications by improving detection accuracy through \\ndeeper network layers that process images with greater precision (AFFES et al. 2023). \\nMeanwhile, YOLOv7 offers advanced customization options that allow security systems to \\nbe finely tuned to specific surveillance needs, enhancing the adaptability and effectiveness \\nof threat detection (AFFES et al. 2023; Cao and Ma 2023). These YOLO versions sup -\\nport high-resolution video feeds, ensuring that security personnel can engage with real-time \\ndata to make informed decisions quickly. Further advancements in surveillance systems \\nare embodied by YOLOv8 and YOLOv9, which introduce significant innovations in deep \\nlearning for security applications (Chatterjee et al. 2024; Sandhya and Kashyap 2024; Tran \\net al. 2024). YOLOv8’s architecture is designed to handle complex environments where \\ntraditional surveillance systems may fail, such as varying lighting and weather conditions. \\nThis version’s robust performance in diverse scenarios enhances its utility in comprehen -\\nsive security strategies. On the other hand, YOLOv9 pushes the boundaries of speed and \\naccuracy, providing unparalleled real-time analysis and detection capabilities. Its deploy -\\nment in surveillance systems ensures that even the subtlest anomalies are detected, reducing \\nthe likelihood of security breaches. The integration of recent versions of YOLO such as \\nYOLOv8 and YOLOv9 into security frameworks not only streamlines operations but also \\nensures a proactive approach to threat management, keeping public and private spaces safer \\nacross the globe (Bakirci and Bayraktar 2024a, b; Shoman et al. 2024c).\\nThe application of YOLO models in surveillance and security systems highlights their \\npivotal role in enhancing real-time response and precision. Majeed et al. ( 2022) investi-\\ngated the effectiveness of a YOLOv5-based security system within a real-time environ -\\nment, underscoring its capability to significantly improve operational efficiency in dynamic \\nsettings. Similarly, AFFES et al. ( 2023) conducted a comparative study across YOLOv5, \\nYOLOv6, YOLOv7, and YOLOv8, focusing on their performance in intelligent video sur -\\nveillance systems. Their analysis demonstrated the incremental improvements in detection \\naccuracy and processing speed, crucial for real-time security applications. Further advanc -\\ning the field, Cao and Ma ( 2023) utilized a refined YOLOv7 model to enhance campus \\nsecurity through improved target detection capabilities, highlighting the model’s precision \\nin identifying potential threats in densely populated environments. Chatterjee et al. (2024) \\nintroduced a YOLOv8-based intrusion detection system specifically tailored for physical \\nsecurity and surveillance, which significantly contributes to safeguarding assets and indi -\\nviduals by detecting unauthorized entries or activities effectively. Additionally, Sandhya and \\nKashyap (2024) employed YOLOv8 for real-time object-removal tampering localization in \\nsurveillance videos, a crucial technology for maintaining the integrity of video evidence and \\nensuring the reliability of surveillance feeds. Together, these studies showcase the robust -\\nness of YOLO architectures in addressing diverse and complex security challenges, provid-\\ning substantial improvements in both the efficacy and efficiency of surveillance operations.\\nRecent studies have significantly leveraged advanced YOLO models to enhance surveil-\\nlance and security across various domains. Bakirci and Bayraktar ( 2024a) discussed opti -\\nmizing ground surveillance for aircraft monitoring using YOLOv9, highlighting its efficacy \\n1 3\\nPage 45 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='644727bf-b0d0-4eaf-96eb-feb878b13b83', embedding=None, metadata={'page_label': '46', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. and \\nYear\\n“Development of an AI-Supported \\nClinical Tool for Assessing Mandibu-\\nlar Third Molar Tooth Extraction Dif-\\nficulty Using Panoramic Radiographs \\nand YOLO11 Sub-Models”\\nDevelops an AI tool using YOLO11 sub-\\nmodels to evaluate mandibular third molar \\nextraction difficulty from panoramic radio-\\ngraphs, incorporating different YOLO11 sizes \\nfor scenario-based evaluations\\nEnhances dental practice by providing an AI tool for \\naccurate and reliable assessment of tooth extraction \\ndifficulty, facilitating improved decision-making \\nand patient management\\nYOLO11 Akdoğan \\net al. (2025), \\n2025\\n“A Machine Learning-Based Model \\nfor the Detection of Skin Cancer \\nUsing YOLOv10”\\nPresents a machine learning model for skin \\ncancer detection using YOLOv10, involving \\npreprocessing, augmentation, and training on \\ntwo datasets for different skin conditions\\nAims to improve early detection and survival rates \\nfor skin cancer through advanced YOLO-based \\nimage detection\\nYOLOv10 Ali et al. \\n(2024), 2024\\n“Efficient Skin Lesion Detection \\nusing YOLOv9 Network”\\nUtilized YOLOv9 for advanced skin lesion \\ndetection, leveraging deep learning to en-\\nhance diagnostic accuracy and speed\\nDeveloped improved skin lesion identification using \\nYOLOv9, showcasing significant advances in detec-\\ntion performance\\nYOLOv9 Ju and Cai \\n(2023), 2023\\n“Fracture detection in pediatric wrist \\ntrauma X-ray images using YOLOv8 \\nalgorithm”\\nEmployed YOLOv8 with data augmenta-\\ntion on the GRAZPEDWRI-DX dataset for \\ndetecting fractures in pediatric wrist X-ray \\nimages\\nEnhanced fracture detection in pediatric wrist \\ntrauma using YOLOv8, achieving superior mAP \\ncompared to previous versions. Designed an app for \\nsurgical use\\nYOLOv8 Ju and Cai \\n(2023), 2023\\n“Chapter 4 - Medical image analysis \\nof masses in mammography using \\ndeep learning model for early diagno-\\nsis of cancer tissues”\\nUtilizes YOLOv7 to detect and diagnose \\ncancerous tissues in mammogram images, \\nleveraging advancements in deep learning for \\nearly cancer detection\\nAims to enhance early detection of breast cancer \\nusing YOLOv7, improving diagnostic accuracy with \\ndeep learning integration. Performance measured by \\nPrecision, Recall, and F1-score\\nYOLOv7 Julia et al. \\n(2024), 2024\\n“Improving YOLOv6 using advanced \\nPSO optimizer for weight selec-\\ntion in lung cancer detection and \\nclassification”\\nEnhanced YOLOv6 with Particle Swarm \\nOptimization for weight optimization in lung \\ncancer detection from CT scans\\nUtilized advanced PSO to optimize YOLOv6 for \\nhigher accuracy in detecting lung cancer, sig-\\nnificantly outperforming previous methods on the \\nLUNA 16 Dataset\\nYOLOv6 Goel and \\nPatel (2024), \\n2024\\n“One-Stage methods of computer \\ncision object detection to classify \\ncarious lesions from smartphone \\nimaging”\\nUtilized YOLO v5, YOLO v5X, and YOLO \\nv5M to detect and classify carious lesions \\nfrom smartphone images\\nAimed to automate caries detection with enhanced \\naccuracy using YOLO. mAP, P, and R metrics \\nvalidated performance\\nYOLOv5, \\nYOLOv5X, \\nYOLOv5M\\nSalahin et al. \\n(2023), 2023\\n“An Improved Method of Polyp De-\\ntection Using Custom YOLOv4-Tiny”\\nCustomized YOLOv4-tiny with Inception-\\nResNet-A block for enhanced detection of \\npolyps in wireless endoscopic images\\nDeveloped to improve the detection performance \\nof polyp detection using a modified YOLOv4-tiny. \\nDemonstrated significant performance improvement\\nYOLOv4-Tiny Doniyorjon \\net al. (2022), \\n2022\\nTable 5 Studies on YOLO applications in healthcare and medicine, emphasizing object detection for diagnostic imaging and real-time medical analysis\\n1 3\\n  274  Page 46 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='be602fdb-ca5e-4ed8-a39d-73c6af856d49', embedding=None, metadata={'page_label': '47', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. and \\nYear\\n“Detection of dental caries in oral \\nphotographs taken by mobile phones \\nbased on the YOLOv3 algorithm”\\nUtilized YOLOv3 for detecting dental caries \\nfrom mobile phone images, employing image \\naugmentation and enhancement for improved \\naccuracy\\nEnhanced detection and diagnosis of dental caries \\nusing YOLOv3, with evaluation of diagnostic preci-\\nsion, recall, and F1-score across different datasets\\nYOLOv3 Ding et al. \\n(2021), 2021\\n“Automatic thyroid nodule recogni-\\ntion and diagnosis in ultrasound \\nimaging with the YOLOv2 neural \\nnetwork”\\nEmployed YOLOv2 for automatic detec-\\ntion and diagnosis of thyroid nodules in \\nultrasound images, enhancing diagnostic \\nprecision\\nCompared AI performance with radiologists using \\nYOLOv2, showing improved accuracy and specific-\\nity in thyroid nodule diagnosis. ROC curve analysis \\nconfirms effectiveness\\nYOLOv2 Wang et al. \\n(2019), 2019\\n“Real-Time Facial Features Detection \\nfrom Low Resolution Thermal Images \\nwith Deep Classification Models”\\nDeveloped a method to localize facial \\nfeatures from low-resolution thermal images \\nby modifying existing deep classification \\nnetworks for real-time detection\\nDemonstrates how spatial information can be \\nrestored and utilized from classification models \\nfor facial feature detection, significantly reducing \\ndataset preparation time while maintaining high \\nprecision\\nCustom Deep \\nClassification \\nModel and \\nYOLO\\nKwaśniewska \\net al. (2018), \\n2018\\nTable 5 (continued)\\n \\n1 3\\nPage 47 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='dab0ff9c-a1a2-4a75-b011-67c22e2edbb7', embedding=None, metadata={'page_label': '48', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. and \\nYear\\n“Assessment of YOLO11 for \\nShip Detection in SAR Imagery \\nUnder Open Ocean and Coastal \\nChallenges”\\nEvaluated YOLO11’s performance for ship detection in \\nchallenging SAR imagery, distinguishing between open \\nocean and coastal scenarios\\nAims to enhance maritime surveillance and safety \\nby leveraging the improved detection capabilities of \\nYOLO11 in both open ocean and coastal environments\\nYOLO11 Bakirci and \\nBayraktar \\n(2024), \\n2024\\n“Deploying YOLOv10 for \\nAffordable Real-Time Handgun \\nDetection”\\nInvestigated the deployment of YOLOv10 on cost-effec-\\ntive hardware like Raspberry Pi for real-time handgun \\ndetection in various scenarios, analyzing detection \\nparameters and model performance\\nEnhances public safety by enabling affordable, \\nreal-time firearm detection technology in everyday \\nenvironments, using YOLOv10 to adapt AI capabili-\\nties for low-cost devices\\nYOLOv10 Žigulić \\net al. \\n(2024), \\n2024\\n“YOLOv9-Enabled Vehicle \\nDetection for Urban Security \\nand Forensics Applications”\\nImplements YOLOv9 for aerial vehicle detection \\nvia UA Vs, enhancing urban security and forensic \\ncapabilities\\nFocus on utilizing YOLOv9 for real-time vehicle \\nmonitoring, facilitating efficient law enforcement and \\nforensic analysis in urban settings\\nYOLOv9 Bakirci and \\nBayraktar \\n(2024b), \\n2024\\n“SC-YOLOv8: A Security \\nCheck Model for the Inspection \\nof Prohibited Items in X-ray \\nImages”\\nDeveloped a custom YOLOv8 model for X-ray image \\nanalysis to detect prohibited items. Enhanced model \\naccuracy using a novel backbone structure and data \\naugmentation\\nAimed to improve security screening effectiveness \\nand reduce error rates in detecting prohibited items. \\nShowcases an innovative use of YOLOv8 in security \\napplications\\nYOLOv8 Han et al. \\n(2023), \\n2023\\n“Detection of Prohibited Items \\nBased upon X-ray Images and \\nImproved YOLOv7”\\nImproved YOLOv7 with spatial attention for contraband \\ndetection in X-ray images. Implemented large kernel \\nattention mechanisms to improve texture and feature \\nextraction to boost accuracy\\nAims to automate security inspections and enhance \\npublic safety by improving prohibited item detection \\nwith modified YOLOv7. Demonstrates YOLOv7’s \\nadaptability in security systems\\nYOLOv7 Yuan et al. \\n(2022), \\n2022\\n“Suspicious Activity Trigger \\nSystem using YOLOv6 Convo-\\nlutional Neural Network”\\nImplements YOLOv6 to detect and classify suspicious \\nactivities in CCTV footage, enhancing home surveil-\\nlance systems. Utilizes deep learning to automatically \\ntrigger alerts, improving response times and security \\neffectiveness\\nAims to reduce property theft by integrating YOLOv6 \\ninto home security systems to auto-detect suspicious \\nbehavior and alert users. Demonstrates YOLOv6’s \\neffectiveness in real-world security applications\\nYOLOv6 Awang \\net al. \\n(2023), \\n2023\\n“Real-time Object Detection \\nfor Substation Security Early-\\nwarning with Deep Neural \\nNetwork based on YOLO-V5”\\nUtilizes YOLO-v5 to enhance substation security by \\ndetecting multiple threats like fire, unauthorized entry, \\nand vehicle misplacement in real-time. Combines deep \\nlearning with video surveillance to reduce the need for \\nextra hardware\\nDesigned to improve substation security management \\nwithout costly additional equipment by detecting vari-\\nous security threats simultaneously using YOLO-v5. \\nDemonstrates the application of YOLO-v5 in critical \\ninfrastructure protection\\nYOLOv5 Xiao et al. \\n(2022), \\n2022\\nTable 6 Studies on YOLO usage in security and surveillance, for real-time threat detection and enhanced monitoring to improved safety measures\\n1 3\\n  274  Page 48 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a48ed00-468f-4dc1-b54e-8053719de86f', embedding=None, metadata={'page_label': '49', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. and \\nYear\\n“Fighting against terrorism: A \\nreal-time CCTV autonomous \\nweapons detection based on \\nimproved YOLO v4”\\nImproved YOLOv4 with SCSP-ResNet backbone and \\nF-PaNet module for detecting weapons in CCTV foot-\\nage, integrating synthetic and real-world data to enhance \\ndetection\\nAims to bolster security and counter-terrorism efforts \\nby accurately identifying weapons in CCTV using \\nan advanced YOLOv4 architecture, demonstrating \\nsignificant performance improvements\\nYOLOv4 Wang et al. \\n(2023), \\n2023\\n“Automatic tracking of objects \\nusing improvised YOLOv3 \\nalgorithm and alarm human \\nactivities in case of anomalies”\\nUtilizes an enhanced YOLOv3 model to automatically \\ntrack objects and alert for anomalies in live video feeds, \\ncomparing performance with CNNs and decision trees\\nDesigned to enhance surveillance systems by detect-\\ning and alerting on anomalies like bag stealing and \\nlock-breaking, demonstrating rapid processing and \\nhigh detection accuracy\\nYOLOv3 Kashika \\nand \\nVenkatapur \\n(2022), \\n2022\\n“Multi-Object Detection using \\nEnhanced YOLOv2 and LuNet \\nAlgorithms in Surveillance \\nVideos”\\nEmploys a novel YOLOv2-LuNet combination for \\nefficient multi-object tracking in video surveillance, en-\\nhancing feature extraction and object detection accuracy\\nDesigned to improve real-time surveillance by \\nenabling robust multi-object tracking in challenging \\nconditions. Highlights the effectiveness of combined \\nYOLOv2 and LuNet approach\\nYOLOv2 Mohandoss \\nand Ranga-\\nraj (2024), \\n2024\\n“From Silence to Propagation: \\nUnderstanding the Relationship \\nbetween ’Stop Snitchin’ and \\n’YOLO”’\\nExamines the cultural shift from ’Stop Snitchin” to \\n’YOLO’ in urban hip-hop culture, highlighting the \\nrole of social media in promoting individualism and \\nexceptionalism\\nAims to explore how social media influences criminal \\nbehavior and public perception, applying cultural \\ncriminology to assess changes in social interactions \\nand deviance\\nN/A Smiley \\n(2015), \\n2015\\nTable 6 (continued)\\n \\n1 3\\nPage 49 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ecd8e288-142c-4421-a877-a2ae691a9ad8', embedding=None, metadata={'page_label': '50', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nin real-time security applications. Similarly, Chakraborty et al. (2024) explored a multi-\\nmodel approach for violence detection, incorporating YOLOv8 to improve public safety \\nthrough automated surveillance. These advancements indicate a shift towards reliable and \\nefficient security systems for complex scenarios.\\nChen et al. (2024) delve into the application of an enhanced YOLOv8 model for large-\\nscale security and low-altitude drone-based law enforcement, demonstrating its potential \\nin managing security risks effectively. Further, Pashayev et al. (2023) utilize YOLO8 for \\nintelligent face recognition in smart cameras, contributing to the development of smarter, \\nmore responsive surveillance technologies. Additionally, Kaç et al. ( 2024) investigate \\nimage-based security techniques for critical water infrastructure surveillance, employing \\nYOLO models to ensure robust monitoring. Lastly, Gao et al. (2024) introduce an improved \\nYOLOv8s network model for contraband detection in X-ray images, underscoring the ver -\\nsatility and precision of YOLO models in enhancing contraband security measures.\\nRecent advancements in surveillance technologies have leveraged the YOLO’s capabili-\\nties, particularly in managing crowd dynamics and detecting critical events. Antony et al. \\n(2024) explored the use of YOLOv8 alongside ByteTrack for crowd management, empha -\\nsizing the system’s efficiency in improving surveillance and public safety. This integration \\nmarks a significant step towards enhancing real-time monitoring capabilities during large \\npublic gatherings. Concurrently, Zhang ( 2024) utilized a YOLO model to detect fire and \\nsmoke in IoT surveillance systems, showcasing the model’s ability to respond swiftly to \\nemergency situations, thus bolstering safety protocols within environments.\\nIn security, Khin and Htaik ( 2024) conducted a comparative study of YOLOv8 with \\nother models like RetinaNet and EfficientDet for gun detection, emphasizing YOLOv8’s \\nsuperior accuracy in detecting firearms within a custom dataset. It underlines the critical \\nrole of precise object detection to prevent potential threats. Additionally, Nkuzo et al. (2023) \\nprovided a comprehensive analysis of the YOLOv7 in detecting car safety belts in real-time, \\nillustrating its importance in enforcing road safety measures. Moreover, Chang et al. (2023) \\ndeveloped an improved YOLOv7, equipped with feature fusion and attention mechanisms, \\ntailored for detecting safety gear violations in high-risk environments like construction, to \\nenhance workplace safety standards. Table 6 presents the various YOLO usage in security \\nand surveillance.\\n5.4 Manufacturing\\nIn the landscape of industrial manufacturing, the deployment of YOLO algorithms can sig-\\nnificantly enhance various processes and quality assessment tasks such as the development \\nof automated optical inspection (AOI) systems. Each iteration of the YOLO family, from \\nYOLOv2 to YOLOv5, and beyond into the latest versions like YOLOv6 and YOLOv7, \\nbrings forward substantial improvements in detecting defects across various manufactur -\\ning domains (Hussain 2023; Ahmad and Rahimi 2022; Pendse et al. 2023; Yi et al. 2024). \\nThe high accuracy and real-time processing capabilities of YOLOv6 and YOLOv7, for \\ninstance, allow for immediate identification of production flaws, crucial for maintaining \\nworkflow efficiency on fast-paced production lines (Wang et al. 2023; Ludwika and Rifai \\n2024). Advancing into the domain of smart manufacturing, YOLO algorithms are pivotal in \\nrevolutionizing quality control mechanisms (Beak et al. 2023; Zhao et al. 2024). The con-\\ntinuous evolution from YOLOv5 to YOLOV6, YOLOv7, YOLOv8, and upto 10th version \\n1 3\\n  274  Page 50 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='789f74e4-011a-4622-be3f-198400825b58', embedding=None, metadata={'page_label': '51', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nof YOLO exemplifies the adaptation of deep learning to meet the stringent quality demands \\nof modern manufacturing processes. These algorithms reduce the need for labor-intensive \\nmanual inspections, thereby minimizing the margin for human error and enhancing the \\noverall speed of quality assessments (Hussain 2023; Ahmad and Rahimi 2022; Pendse et al. \\n2023; Yi et al. 2024; Beak et al. 2023; Zhao et al. 2024).\\nFor instance, (Liu and Ye 2023) pioneered YOLO-IMF, an enhanced version of \\nYOLOv8 tailored for precise surface defect detection in industrial settings, exemplifying \\nthe algorithm’s efficacy in real-time environments. This refinement aims to cater to the \\nhigh demands for accuracy in manufacturing sectors where defects can significantly impact \\nquality and safety. Continuing this trend, (Wen and Wang 2024) introduced YOLO-SD, \\nwhich utilizes simulated feature fusion for few-shot learning, enhancing YOLOv8’s capa -\\nbility in detecting industrial defects under varied conditions. Similarly, (Karna et al. 2023) \\nextended YOLOv8’s utility in monitoring 3D printing processes by optimizing hyperpa -\\nrameters to detect faults more accurately, reflecting a targeted approach to maintaining pro-\\nduction integrity. Li et al. ( 2024) adapted YOLOv8 to inspect cylindrical parts, a critical \\naspect of quality control in specialized manufacturing. Lastly, Hu et al. (2024) leveraged \\na conditioned version of YOLOv8, named Cond-YOLOv8-seg, to assess the uniformity of \\nindustrially produced materials, showcasing the model’s versatility across different manu -\\nfacturing scenarios. These innovations underscore the pivotal role of YOLO algorithms in \\ndriving forward the capabilities of industrial inspection systems, highlighting their impact \\non enhancing operational efficiency and product quality.\\nAdditionally (Yang et al. 2024) introduced DCS-YOLOv8, a variant optimized for detect-\\ning steel surface defects, demonstrating its effectiveness in addressing the complexities of \\nsteel manufacturing. This adaptation ensures that even minor imperfections are identified, \\ncrucial for maintaining the structural integrity of steel products.Likewise, Wang et al. (2023) \\nfurther refined YOLOv8 to develop BL-YOLOv8, focusing on road defect detection. This \\nmodel enhances the safety and maintenance of transportation infrastructure by enabling \\nmore accurate and real-time detection of road surface anomalies. Similarly, Luo et al. (2023) \\npresented a “Hardware-Friendly” YOLOv8 model designed for foreign object identification \\non belt conveyors, crucial for preventing equipment damage in materials handling. This ver-\\nsion of YOLOv8 is tailored to perform well on the limited computational resources typical \\nof industrial hardware systems. Finally, Wang et al. (2024) employed an improved YOLOv8 \\nalgorithm for the detection of defects in automotive adhesives, a critical quality control \\nmeasure for ensuring vehicle safety and durability. These applications of YOLOv8 exem -\\nplify its adaptability and precision in industrial settings, where high accuracy and efficiency \\nare paramount for operational success and safety compliance.\\nThe recent advancements in YOLOv7 have paved the way for significant improvements \\nin industrial inspection and monitoring systems. Wu et al. ( 2023) developed an enhanced \\nYOLOv7 model specifically tailored for detecting objects in complex industrial equipment \\nscenarios, highlighting its application in real-world settings (Wu et al. 2023). Similarly, \\nKim et al. ( 2022) implemented YOLOv7 in a real-time inspection system that leverages \\nMoire patterns to detect defects in highly reflective injection molding products, demonstrat-\\ning the algorithm’s capability in manufacturing quality control (Kim et al. 2022). Further, \\nChen et al. ( 2023) explored the defect detection capabilities of YOLOv7 for automotive \\nrunning lights, contributing to safer automotive systems through precise quality assurance \\ntechniques (Chen et al. 2023).\\n1 3\\nPage 51 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4aec074c-9b95-4e5e-a61f-394ee287ff1f', embedding=None, metadata={'page_label': '52', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nHussain et al. (2022) applied domain feature mapping with YOLOv7 to automate inspec-\\ntions of pallet racking in storage facilities, enhancing safety and efficiency in logistics \\noperations (Hussain et al. 2022). Zhu et al. (2023) extended YOLOv7’s utility to the identi-\\nfication and classification of surface defects in belt grinding processes, aiding in maintain -\\ning the integrity of manufacturing workflows (Zhu et al. 2023). Lastly, Zhang et al. (2024) \\ninnovated with YOLO-RDP, a lightweight version of YOLOv7, optimized for detecting \\nsteel defects in real-time, showcasing the adaptability of YOLOv7 to resource-constrained \\nenvironments and promoting sustainable manufacturing practices (Zhang 2024). Table 7 \\nillustrates the different use of YOLO versions in the field of industrial manufacturing:\\n5.5 Agriculture\\nIn agricultural environments, advanced object detection techniques such as YOLOv5 (Wang \\net al. 2022; Badgujar et al. 2023; Shoman et al. 2022), YOLOv6 (Bhat et al. 2023; Bist et al. \\n2023), YOLOv7 (Jiang et al. 2022; Kumar and Kumar 2023), YOLOv8 (Chen et al. 2023; \\nZhang et al. 2023) and YOLOv8 to YOLO11(Sharma et al. 2024) have proven to be instru-\\nmental in transforming traditional farming into smart, precision agriculture(Badgujar et al. \\n2024). YOLOv5, for example, has been adept at weed detection (Chen et al. 2023; Junior \\nand Ulson 2021), enabling farmers to apply herbicides more effectively and economically \\nby precisely identifying and localizing weed species amidst crops. This level of precision \\nnot only conserves resources but also mitigates the adverse environmental impact of exces-\\nsive chemical usage. Furthermore, YOLOv6, YOLOv7 and YOLOv8 have enhanced capa-\\nbilities in broader agricultural applications such as monitoring and analyzing crop health \\nand growth patterns, significantly improving yield predictions and crop management strate-\\ngies (Yu et al. 2024; Khalid et al. 2023; Gallo et al. 2023).\\nThe recent introduction of YOLOv7 and YOLOv8 has further pushed the boundaries \\nof agricultural innovation. YOLOv7 (Jia et al. 2023; Vaidya et al. 2023; Umar et al. 2024) \\nand YOLOv8 (Zhang et al. 2023; Yue et al. 2023; Zayani et al. 2024) have been specifi -\\ncally refined to detect small pests and subtle disease symptoms on crops, which are often \\noverlooked by human inspectors. Its enhanced deep learning framework allows for inte -\\ngrating complex image recognition tasks that facilitate early detection, thereby prevent -\\ning widespread crop damage. On the other hand, YOLOv8 has made significant strides in \\nfruit and other crop canopy detection tasks. Its application in orchards for detecting fruits \\nsuch as apples and apple tree branches (Ma et al. 2024) supports optimal harvesting by \\ndetermining the right stage of fruit maturity. This technique helps maximize the harvest \\nquality and ensures that the fruits are picked at their nutritional peak, thereby enhancing \\ntheir market value. The application of these advanced YOLO models including YOLOv5, \\nYOLOv6, YOLOv7, and YOLOv8 represents a leap toward a more sustainable and efficient \\nagricultural sector.\\nRecent studies have demonstrated the efficacy of YOLO-based models in enhancing \\nvarious aspects of smart farming and agricultural automation solutions. Junos et al. ( 2021) \\noptimized a YOLO-based object detection model to improve crop harvesting systems, \\nshowcasing the potential to boost yield and reduce labor costs (Junos et al. 2021). Zhao \\net al. (2024) extended this application to real-time object detection combined with robotic \\nmanipulation, further aligning agricultural practices with advanced automation technolo -\\ngies (Zhao et al. 2024). Chen et al. ( 2021) developed an apple detection method using a \\n1 3\\n  274  Page 52 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ad30d79b-7870-44f2-889f-53f98726789c', embedding=None, metadata={'page_label': '53', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. and \\nYear\\n“Automated Dual-Side Leather \\nDefect Detection and Classifica-\\ntion Using YOLOv11: A Case \\nStudy in the Finished Leather \\nIndustry”\\nExplores leather defect detection on both the grain and \\nflesh sides using YOLOv11, with a custom-designed light \\nchamber to enhance quality control and reduce waste\\nAims to improve defect detection accuracy and \\nleather utilization in the leather industry by utilizing \\nadvanced AI capabilities of YOLOv11 for dual-side \\nanalysis\\nYOLOv11 Banduka \\net al. \\n(2024), \\n2024\\n“A Novel YOLOv10-Based \\nAlgorithm for Accurate Steel \\nSurface Defect Detection”\\nPresents YOLOv10n-SFDC, a new system improving \\nsteel surface defect detection by integrating DualConv, \\nSlimFusionCSP modules, and Shape-IoU loss function for \\nenhanced accuracy\\nEnhances steel manufacturing processes by offering \\na more accurate and efficient defect detection sys-\\ntem, demonstrating significant improvements over \\ntraditional methods\\nYOLOv10 Liao \\net al. \\n(2025), \\n2025\\n“An Improved YOLOv9 and Its \\nApplications for Detecting Flex-\\nible Circuit Boards Connectors”\\nEnhances YOLOv9 with Multi-scale Dilated Attention \\nand Deformable Large Kernel Attention for detecting \\ndefects in FPC connectors, improving feature capture and \\nboundary adaptation\\nAims to address challenges in automatic defect \\ndetection of flexible circuit board connectors by \\nsignificantly enhancing detection precision and com-\\nputational efficiency with modified YOLOv9\\nYOLOv9 Huang \\net al. \\n(2024), \\n2024\\n“YOLO-IMF: An Improved \\nYOLOv8 Algorithm for Surface \\nDefect Detection in Industrial \\nManufacturing Field”\\nProposes an enhanced YOLOv8, YOLO-IMF, for surface \\ndefect detection on aluminum plates. Replaces CIOU with \\nEIOU loss function to better handle small and irregularly \\nshaped targets, achieving significant improvements in \\nprecision\\nDemonstrates YOLOv8’s extended applicability in \\nindustrial settings by enhancing accuracy and defect \\ndetection capabilities\\nYOLOv8 Liu \\nand Ye \\n(2023), \\n2023\\n“YOLOv7-SiamFF: Industrial \\nDefect Detection Algorithm \\nBased on Improved YOLOv7”\\nIntroduces YOLOv7-SiamFF, an advanced defect \\ndetection framework employing YOLOv7 with Siamese \\nnetwork enhancements for superior defect identification \\nand background noise suppression\\nEnhances industrial defect detection by integrating \\nattention mechanisms and feature fusion modules, \\nachieving higher accuracy in pinpointing defect \\nlocations\\nYOLOv7 Yi et al. \\n(2024), \\n2024\\n“A Novel Finetuned YOLOv6 \\nTransfer Learning Model for \\nReal-Time Object Detection”\\nEnhances real-time object detection by integrating a \\ntransfer learning approach with a pruned and finetuned \\nYOLOv6 model, significantly boosting detection accuracy \\nand speed\\nFocuses on improving YOLOv6 for efficient object \\ndetection in embedded systems, using advanced \\npruning techniques for reduced model size without \\nsacrificing performance\\nYOLOv6 Gupta \\net al. \\n(2023), \\n2023\\n“Real-time Tool Detection in \\nSmart Manufacturing Using \\nYOLOv5”\\nUtilizes YOLOv5 for advanced real-time tool detection in \\nmanufacturing environments, optimizing object detection \\ncapabilities for precise tool localization\\nAims to enhance smart manufacturing by leverag-\\ning YOLOv5 for accurate and real-time detection of \\nvarious tools, contributing significantly to Industry \\n4.0 initiatives\\nYOLOv5 Zende-\\nhdel et al. \\n(2023), \\n2023\\nTable 7 Studies on YOLO applications in the manufacturing industry, focusing on real-time defect detection and process optimization for improved efficiency\\n1 3\\nPage 53 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a02664d-6ec9-4759-bd37-cd01185c3511', embedding=None, metadata={'page_label': '54', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. and \\nYear\\n“Efficient Automobile Assembly \\nState Monitoring System Based \\non Channel-Pruned YOLOv4”\\nImplements a channel-pruned YOLOv4 algorithm to \\noptimize monitoring in automobile assembly, enhancing \\ndetection speed without compromising accuracy\\nDesigned to streamline assembly monitoring in in-\\ndustrial environments, showcasing YOLOv4’s utility \\nin enhancing operational efficiency and deployment \\nreadiness\\nYOLOv4 Jiang \\net al. \\n(2024), \\n2024\\n“YOLO V3 + VGG16-\\nbased Automatic Operations \\nMonitoring in Manufacturing \\nWorkshop”\\nUtilizes a combined YOLO V3 and VGG16 framework to \\nrecognize and monitor industrial operations accurately for \\nIndustry 4.0 manufacturing workshops\\nAims to enhance production efficiency and qual-\\nity by automating action analysis and process \\nmonitoring using advanced YOLO V3 and VGG16 \\ntechnologies\\nYOLO V3, \\nVGG16\\nYan and \\nWang \\n(2022), \\n2022\\n“Improvements of Detection Ac-\\ncuracy by YOLOv2 with Data \\nSet Augmentation”\\nEmploys YOLOv2 with an innovative data set augmenta-\\ntion method to enhance the detection accuracy and confi-\\ndence in identifying defective areas in industrial products\\nSeeks to optimize defect detection and visualization \\non production lines, demonstrating YOLOv2’s effec-\\ntiveness with limited data augmentation options\\nYOLOv2 Arima \\net al. \\n(2023), \\n2023\\nTable 7 (continued)\\n \\n1 3\\n  274  Page 54 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a042e5ad-0a80-4535-b79f-70478bdf3620', embedding=None, metadata={'page_label': '55', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\ntailored YOLOv4 algorithm, specifically designed to support harvesting robots operating \\nin complex environments, which significantly enhances the precision and efficiency of fruit \\npicking (Chen et al. 2021).\\nFurther contributions include work by Nergiz (2023), who utilized YOLOv7 to enhance \\nstrawberry harvesting efficiency, providing practical solutions for small to medium-sized \\nenterprises in the agricultural (NERGİZ 2023). Wang et al. ( 2024) focused on planning \\nharvesting operations in large strawberry fields using a deep learning-based image process-\\ning method, demonstrating the scalability of YOLO for larger agricultural operations (Wang \\net al. 2024). Lastly, Zhang et al. (2023) introduced DCF-YOLOv8, an improved algorithm \\nfor agricultural pests and diseases detection by aggregating low-level features, which helps \\nin early detection and management of crop health (Zhang et al. 2023). These studies col -\\nlectively illustrate the transformative impact of YOLO-based models in modernizing agri -\\ncultural practices, ensuring higher productivity and sustainability.\\nIn orchard automation, the YOLO object detection models have been specifically pivotal \\nin enhancing the accuracy and efficiency of fruit detection (Chen et al. 2022; Mirhaji et al. \\n2021; Sapkota et al. 2024c), flower identification (Wu et al. 2020; Wang et al. 2021; Khanal \\net al. 2023), and automated harvesting processes (Xiao et al. 2023; Junos et al. 2021; Yijing \\net al. 2021). These models adeptly identify and classify fruits at various stages of ripeness, \\ndetect flowers and other canopy objects such as branches with high precision, and facilitate \\nefficient harvesting operations. The development of YOLO models has introduced signifi -\\ncant improvements that cater specifically to the challenges of agricultural environments. For \\ninstance, YOLOv5’s introduction of multi-scale predictions improved the detection of small \\nand clustered objects like flowers and young fruits, which are critical during the early stages \\nof crop yield management (Zhang et al. 2024). As the models advanced, YOLOv7 and \\nYOLOv8 incorporated better segmentation techniques, which enhanced the differentiation \\nbetween fruit types and maturity stages, critical for targeted harvesting (Zhou et al. 2023; \\nXiuyan and ZHANG 2023).\\nIn addition to their application in fruit detection and harvesting, YOLO object detection \\nmodels are increasingly vital across other agricultural practices such as pruning (Sapkota \\net al. 2024a), thinning (Sapkota et al. 2024c), pollination and harvest (He et al. 2021) man-\\nagement. In pruning, YOLO models facilitate the accurate identification of non-fruiting \\nbranches, aiding in the automation of pruning tasks to optimize plant health and productiv-\\nity. For thinning processes, these models can distinguish between fruit clusters, allowing for \\nprecise thinning that improves fruit size and quality at harvest. Furthermore, YOLO models \\nare being adapted to recognize pollination patterns, helping to monitor and enhance pollina-\\ntion efficiency, which is crucial for maximizing crop yield. Each advancement in YOLO \\ntechnology contributes to more sustainable and effective agricultural operations, underscor-\\ning the critical role of AI-driven technologies in modern farming practices.\\nMoreover, recent iteration, YOLOv9 have leveraged advanced algorithms with spatial \\npyramid pooling and attention mechanisms, which have refined the detection capabilities \\nin plant disease detection (Boudaa et al. 2024). Boudaa et al. (2024) performed a compara-\\ntive study on different important versions of YOLO (v5, v8 and v9) on a real-world dataset \\nfor tomato plant disease detection and suggested that YOLOv9 outperforms YOLOv5 and \\nYOLOv8. A study on weed species control was presented by Sharma et al. (2024), where a \\ncomparative analysis of different YOLO versions (v8, v9, v10, and 11) and Faster R-CNN \\nwas conducted. The study utilized an annotated image database containing five weed spe -\\n1 3\\nPage 55 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='35c6ada1-ca80-4343-9794-74bd28165ea2', embedding=None, metadata={'page_label': '56', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\ncies: cocklebur (Xanthium strumarium L.), dandelion (Taraxacum officinale), common \\nwaterhemp (Amaranthus tuberculatus), Palmer amaranth (Amaranthus palmeri), and com -\\nmon lambsquarters (Chenopodium album L.).\\nIt is also noted that autonomous driving has been an active area of research and develop-\\nment in agriculture as well (Andreyanov et al. 2022; Jung et al. 2020), similar to self-driving \\ncars. YOLO models have played crucial roles in advancing this technology for farming for \\nexample in peach crop (Xu and Rai 2024). Object detection based on YOLO has enhanced \\nthe navigation systems of autonomous agricultural vehicles, enabling them to operate effi -\\nciently in diverse field conditions. By accurately detecting and classifying field boundaries, \\nobstacles such as rocks and trees, and other critical elements like rows of crops and water \\nsources, YOLO-powered systems significantly improve the precision and safety of opera -\\ntions. This capability is particularly vital in precision agriculture, where exactness in plant-\\ning, fertilizing, and treatment applications can substantially impact crop health and yield. \\nThe integration of YOLO models into agricultural drones and robots also supports tasks \\nsuch as crop monitoring, disease detection, and targeted pesticide application, further auto-\\nmating and optimizing farm management practices (Alibabaei et al. 2022).\\nTable 8 illustrates the different use of YOLO versions in the field of Agriculture:\\n6 Challenges and limitations\\n6.1 Challenges and limitations of each version\\nYOLOv12:\\n ● Computational efficiency and performance trade-offs: YOLOv12 introduces the Area \\nAttention (A2) module and FlashAttention mechanism, which, while theoretically ef -\\nficient, present practical challenges. Empirical studies demonstrate a reduction in infer-\\nence speed (30 FPS compared to YOLO11’s 40 FPS) and an increase in training time by \\napproximately 20% (Tian et al. 2025). The effectiveness of these attention mechanisms \\nvaries with hardware architecture, potentially limiting performance gains on certain sys-\\ntems. Moreover, YOLOv12 requires lower confidence thresholds (0.3−0.4) for effective \\ndetection, which may increase false positive rates. These factors collectively contribute \\nto the ongoing challenge of optimizing the trade-off between detection accuracy and \\nreal-time performance, particularly in resource-constrained environments and complex \\ndetection scenarios.\\n ● Detection limitations and environmental sensitivity: Despite architectural advance -\\nments, YOLOv12 continues to face challenges in accurately detecting small objects \\nand objects in crowded scenes, a limitation persistent from previous iterations. This is \\nprimarily due to three factors: 1) The downsampling process in YOLO architectures can \\nlead to loss of fine-grained details, particularly affecting small object detection (Diwan \\net al. 2023), 2) Limited contextual information in a single forward pass makes it difficult \\nfor the model to differentiate between closely packed or overlapping objects in crowded \\nscenes (Ji et al. 2023; Fang et al. 2019), and 3) The use of fixed anchor boxes with pre-\\ndefined scales and aspect ratios may not optimally capture objects with highly variable \\nsizes or unusual shapes (Sirisha et al. 2023).\\n1 3\\n  274  Page 56 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cb910839-c321-4ea0-b087-b0854ed1edb2', embedding=None, metadata={'page_label': '57', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. \\nand \\nYear\\n“Improved YOLOv12 with LLM-\\nGenerated Synthetic Data for \\nEnhanced Apple Detection and \\nBenchmarking Against YOLOv11 \\nand YOLOv10”\\nEvaluates YOLOv12 for apple detection in com-\\nmercial orchards using LLM-generated synthetic \\ndatasets and benchmarks it against YOLOv11 and \\nYOLOv10\\nDemonstrates that YOLOv12 achieves the high-\\nest precision (0.916), recall (0.969), and mAP@50 \\n(0.978), surpassing YOLOv11 and YOLOv10 in ac-\\ncuracy while YOLO11 remains the fastest. Highlights \\nefficiency in processing speeds and cost-effectiveness \\nby reducing the need for extensive manual data \\ncollection\\nYOLOv12 Sap-\\nkota and \\nKarkee \\n(2025), \\n2025\\n“Synthetic meets authentic: \\nLeveraging llm generated datasets \\nfor YOLO11 and YOLOv10-based \\napple detection through machine \\nvision sensors”\\nUtilizes LLM-generated datasets to train \\nYOLOv10 and YOLO11 for detecting apples, \\nshowcasing significant improvements in detection \\nmetrics and processing times\\nAims to streamline data collection and enhance ob-\\nject detection in orchards with minimal fieldwork by \\nemploying YOLO11 and YOLOv10, demonstrating \\nsuperior performance in real-world tests\\nYOLO11 Sapkota \\net al. \\n(2024b), \\n2024\\n“Comprehensive Performance \\nEvaluation of YOLO11, YOLOv10, \\nYOLOv9 and YOLOv8 on Detect-\\ning and Counting Fruitlet in Com-\\nplex Orchard Environments”\\nConducts a thorough performance evaluation of \\nYOLOv8, YOLOv9, YOLOv10, and YOLO11 on \\ngreen fruit detection and counting across multiple \\nconfigurations, highlighting in-field validation \\nwith an iPhone and machine vision sensors\\nProvides comparative insights on various YOLO \\nconfigurations for optimizing fruitlet detection in \\ncommercial orchards, recommending YOLO11 for its \\nspeed and accuracy\\nYOLO11 Sapkota \\net al. \\n(2024d), \\n2024\\n“YOLOv10-pose and YOLOv9-\\npose: Real-time Strawberry Stalk \\nPose Detection Models”\\nIntroduces YOLOv10-pose and YOLOv9-pose \\nfor high-precision strawberry stalk pose detection, \\ncomparing them with previous YOLO versions to \\noptimize agricultural automation tasks\\nEnhances the efficiency of robotic harvesting and \\nother agricultural applications by providing accurate \\npose detection, crucial for automated operations in \\nthe agricultural industry\\nYOLOv10 Meng \\net al. \\n(2025), \\n2025\\n“Comparing YOLOv11 and \\nYOLOv8 for Instance Segmenta-\\ntion of Occluded and Non-occluded \\nImmature Green Fruits in Complex \\nOrchard Environment”\\nEvaluates YOLOv11 and YOLOv8 for instance \\nsegmentation capabilities of immature green \\nfruits, focusing on occluded and non-occluded \\nscenarios in orchards\\nEnhances understanding of YOLO11 and YOLOv8’s \\nsegmentation performance, particularly their efficacy \\nin detecting and segmenting immature green fruits \\namidst complex environmental conditions\\nYOLO11 Sap-\\nkota and \\nKarkee \\n(2024), \\n2024\\n“Automating Tomato Ripeness \\nClassification and Counting with \\nYOLOv9”\\nImplements YOLOv9 to automate and enhance \\nthe accuracy of classifying and counting ripe \\ntomatoes, replacing labor-intensive visual \\ninspections\\nAims to streamline tomato ripeness monitoring \\nand counting, to enhance agricultural productivity \\nand quality. Utilizes YOLOv9 for high accuracy in \\ndetection\\nYOLOv9 V o et al. \\n(2024), \\n2024\\nTable 8 Studies on YOLO usage in agriculture, emphasizing automated crop monitoring, pest detection, and yield estimation for enhanced productivity\\n1 3\\nPage 57 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a457a290-abc0-443c-9894-38404af5dc68', embedding=None, metadata={'page_label': '58', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. \\nand \\nYear\\n“A Lightweight YOLOv8 \\nTomato Detection Algorithm Com-\\nbining Feature Enhancement and \\nAttention”\\nEnhances YOLOv8 for tomato detection in \\nagriculture using depthwise separable convolution \\nand dual-path attention gate modules. Optimizes \\nreal-time detection for robotic tomato picking\\nAims to advance agricultural automation by boosting \\nYOLOv8’s efficiency and accuracy in tomato harvest-\\ning. Demonstrates improved performance over earlier \\nYOLO versions\\nYOLOv8 Yang \\net al. \\n(2023), \\n2023\\n“An Attention Mechanism-Im-\\nproved YOLOv7 Object Detection \\nAlgorithm for Hemp Duck Count \\nEstimation”\\nImplements CBAM-YOLOv7 to enhance feature \\nextraction capabilities within YOLOv7 for precise \\nhemp duck counting in agriculture, outperform-\\ning SE-YOLOv7 and ECA-YOLOv7 in precision \\nand mAP\\nEnhances livestock management by automating duck \\ncount with advanced object detection, reducing labor \\nand improving accuracy\\nYOLOv7 Jiang \\net al. \\n(2022), \\n2022\\n“Detecting Crops and Weeds in \\nFields Using YOLOv6 and Faster \\nR-CNN Object Detection Models”\\nUtilizes YOLOv6 and Faster R-CNN to detect \\ncrops and weeds for precise management\\nAims to boost agricultural productivity and environ-\\nmental sustainability by improving accuracy in weed \\ndetection using YOLOv6\\nYOLOv6 Bhat \\net al. \\n(2023), \\n2023\\n“An improved YOLOv5-based veg-\\netable disease detection method”\\nEnhances YOLOv5 for precise detection of \\nvegetable diseases by upgrading CSP, FPN, and \\nNMS modules to handle complex environmental \\ninterference\\nAims to improve food security by boosting the ac-\\ncuracy and speed of disease detection in vegetables \\nusing an improved YOLOv5 algorithm\\nYOLOv5 Li et al. \\n(2022), \\n2022\\n“Using channel pruning-based \\nYOLO v4 deep learning algorithm \\nfor the real-time and accurate \\ndetection of apple flowers in natural \\nenvironments”\\nImplements a channel pruned YOLOv4 model \\nto enhance efficiency and accuracy in detecting \\napple flowers, supporting the development of \\nflower thinning robots\\nAims to optimize apple flower detection in orchards \\nby applying channel pruning to YOLOv4, signifi-\\ncantly reducing model size and improving processing \\nspeed while maintaining high accuracy\\nYOLOv4 Wu \\net al. \\n(2020), \\n2020\\n“Fast and accurate detection of \\nkiwifruit in orchard using improved \\nYOLOv3-tiny model”\\nEnhances YOLOv3-tiny with additional convo-\\nlutional kernels for improved kiwifruits detection \\nin orchards, in occlusions and varying lighting \\nconditions\\nFocus on increasing the efficiency of kiwifruit detec-\\ntion in dynamic orchard environments with a modi-\\nfied YOLOv3-tiny, demonstrating high performance\\nYOLOv3-tiny Fu et al. \\n(2021), \\n2021\\n“A Detection Method for Tomato \\nFruit Common Physiological Dis-\\neases Based on YOLOv2”\\nImplements YOLOv2 to detect and identify \\nhealthy and diseased tomato, using advanced \\nimage processing and data augmentation to \\nenhance detection accuracy\\nAims to boost tomato yield and quality control \\nthrough efficient detection of physiological diseases, \\ndemonstrating the effectiveness of YOLOv2 in \\nagriculture\\nYOLOv2 Zhao \\nand Qu \\n(2019), \\n2019\\nTable 8 (continued)\\n \\n1 3\\n  274  Page 58 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1ebaff57-aad8-46c3-9add-9fd381177a5c', embedding=None, metadata={'page_label': '59', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nTitle of paper Description of work Purpose and YOLO usage Version Refs. \\nand \\nYear\\n“A Vision-Based Counting and Rec-\\nognition System for Flying Insects \\nin Intelligent Agriculture”\\nUtilizes YOLO for initial detection and counting, \\nand SVM for fine classification of flying insects, \\nfor efficient insect pest control\\nDemonstrates a robust, efficient system for insect \\nmonitoring, greatly enhancing accuracy and speed in \\npest management\\nYOLO, SVM Zhong \\net al. \\n(2018), \\n2018\\n“Comparative performance of \\nYOLOv8, YOLOv9, YOLOv10, \\nYOLO11 and Faster R-CNN models \\nfor detection of multiple weed \\nspecies”\\nCompare the performance of YOLOv8, YOLOv9, \\nYOLOv10, YOLO11, and Faster R-CNN algo-\\nrithms in terms of speed and accuracy\\nDemonstrates the robust efficiency of several YOLO \\nversions in detecting weed species using a database \\ncomprising cocklebur (Xanthium strumarium L.), \\ndandelion (Taraxacum officinale), common water-\\nhemp (Amaranthus tuberculatus), Palmer amaranth \\n(Amaranthus palmeri), and common lambsquarters \\n(Chenopodium album L.)\\nYOLOv8, \\nYOLOv9, \\nYOLOv10, \\nYOLOv11 and \\nFaster R-CNN\\nSharma \\net al. \\n(2024), \\n2024\\nTable 8 (continued)\\n \\n1 3\\nPage 59 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d665df3e-4ae0-4555-a0ce-fa002de14ee6', embedding=None, metadata={'page_label': '60', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\n These issues contribute to the model’s potential struggle with objects at vastly different \\nscales within the same image, indicating a need for improved scale-invariant feature \\nextraction.\\nYOLOv11:\\nAs the most recent addition to the YOLO series, this version must address and overcome\\n ● Challenges in Detecting Small and Rotated Objects: Despite advancements, YOLOv11s-\\ntill struggles with small, low-resolution objects and those with varied orientations. This \\nlimitation is due to its architectural constraints, which may not fully capture the com -\\nplexities of such objects, leading to potential inaccuracies in detection.\\n ● Susceptibility to Overfitting: YOLOv11 is prone to overfitting, particularly when trained \\non limited or homogeneous datasets. This overfitting can adversely affect the model’s \\nperformance on new or varied datasets, indicating a need for improved generalization \\ncapabilities and robust training approaches.\\n ● Computational Efficiency vs. Accuracy Trade-off: While YOLOv11 has improved com-\\nputational efficiency, there remains a trade-off with accuracy, particularly in complex \\ndetection environments. This trade-off highlights the ongoing challenge of balancing \\nspeed and accuracy to support real-time applications.\\nYOLOv10:\\n ● YOLOv10 has not yet seen widespread adoption in published research. Its release \\npromises cutting-edge improvements in object detection capabilities, but the lack of \\nextensive testing and real-world application data makes it difficult to ascertain its full \\npotential and limitations.\\n ● Preliminary evaluations suggest that while YOLOv10 might offer advancements in \\nspeed and accuracy, integrating it into existing systems could present challenges due \\nto compatibility and computational demands. Potential users may hesitate to adopt this \\nversion until more comprehensive studies and benchmarks are available, which articu -\\nlate its advantages over previous models.\\n ● The expectation with YOLOv10, much like its predecessors, is that it will drive fur -\\nther research in object detection technologies. Its eventual widespread implementation \\ncould pave the way for addressing complex detection scenarios with higher accuracy, \\nparticularly in dynamic environments. However, as with any new technology, the ad -\\naptation phase will be crucial in understanding its practical limitations and operational \\nchallenges.\\nYOLOv9:\\n ● Despite YOLOv9’s enhancements in detection capabilities, it has only been featured in \\na handful of studies, which limits a comprehensive understanding of its performance \\nacross diverse applications. This lack of extensive validation may deter organizations \\nfrom adopting it until more empirical evidence and comparative analyses establish its \\nefficacy and efficiency over earlier versions.\\n ● YOLOv9 significantly rectified the computational efficiency challenges faced by \\n1 3\\n  274  Page 60 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4f7ffcff-1c64-4f8c-b846-12437fa6a2f5', embedding=None, metadata={'page_label': '61', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nYOLOv8. YOLOv9 introduces several improvements that enhance its efficiency while \\nmaintaining or improving accuracy. The model achieves a 49% reduction in parameters \\nand a 43% reduction in computational load compared to YOLOv8, while improving \\naccuracy by 0.6% on benchmark dataset. The introduction of the Generalized Efficient \\nLayer Aggregation Network and Programmable Gradient Information enhances feature \\nextraction and gradient flow, leading to improved efficiency. YOLOv9-E demonstrates \\na 16% reduction in parameters and a 27% reduction in FLOPs compared to YOLOv8-X, \\nwhile also gaining a 1.7% improvement in mAP value.\\n ● While YOLOv9 improves upon the speed and accuracy of its predecessors, it may still \\nstruggle with detecting small or overlapping objects in cluttered scenes. This is a re -\\ncurring challenge in high-density environments like crowded urban areas or complex \\nnatural scenes in transportation and agriculture, where precise detection is critical for \\napplications such as autonomous driving, wildlife monitoring and robotic fruit picking.\\n ● Future developments for YOLOv9 could focus on enhancing its robustness in adverse \\nconditions, such as varying weather, lighting, or occlusions. Integrating more adaptive \\nand context-aware mechanisms could help in mitigating false positives and improving \\nthe reliability of the system under different operational conditions. The implementation \\nof advanced training techniques such as federated learning could also be explored to \\nenhance its adaptability and learning efficiency from decentralized data sources.\\nYOLOv8:\\n ● YOLOv8 has shown significant improvements in object detection tasks, particularly in \\nreal-time applications. However, it continues to face challenges in terms of computa -\\ntional efficiency and resource consumption when deployed on lower-end hardware (Ye \\net al. 2024). This can limit its applicability in resource-constrained environments where \\ndeploying advanced hardware solutions is not feasible (Soylu and Soylu 2024).\\n ● The future direction for YOLOv8 could involve optimizing its architectural design to re-\\nduce computational load without compromising detection accuracy. Enhancing its scal-\\nability to efficiently process images of varying resolutions and conditions can broaden \\nits application scope. Moreover, incorporating adaptive scaling and context-aware train-\\ning methods could potentially address the detection challenges in complex scenes, mak-\\ning it more robust against diverse operational challenges.\\nYOLOv7:\\n ● Although YOLOv7 introduces significant improvements in detection accuracy and \\nspeed, its adoption across varied real-world applications reveals a persistent challenge \\nin handling highly dynamic scenes. For instance, in environments with rapid motion or \\nin scenarios involving occlusions, YOLOv7 can still experience drops in performance. \\nThe algorithm’s ability to generalize across different types of blur and motion artifacts \\nremains an area for further research and enhancement.\\n ● The complexity of YOLOv7’s architecture, while beneficial for accuracy, imposes a \\nsubstantial computational burden. This makes it less ideal for deployment on edge de -\\nvices or platforms with limited processing capabilities, where maintaining a balance \\nbetween speed and power efficiency is crucial (Olorunshola et al. 2023; AFFES et al. \\n1 3\\nPage 61 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='998d6c26-92d5-4fd1-b0b6-a213ca9319dd', embedding=None, metadata={'page_label': '62', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\n2023). Efforts to streamline the model for such applications without significant loss of \\nperformance are necessary.\\n ● Looking forward, there is significant potential in expanding YOLOv7’s capabilities \\nthrough the integration of semi-supervised or unsupervised learning paradigms. This \\nwould enable the model to leverage unlabeled data effectively, a common challenge in \\nthe real-world where annotated datasets are often scarce or expensive to produce. Ad -\\nditionally, enhancing the model’s resilience to adversarial attacks and variability in data \\nquality could further solidify its utility in security-sensitive applications like surveil -\\nlance and fraud detection.\\nYOLOv6:\\n ● One of the notable challenges with YOLOv6 is its handling of scale variability within \\nimages, which can affect its efficacy in environments where objects appear at diverse \\ndistances from the camera. While YOLOv6 shows improved accuracy and speed over \\nits predecessors, it sometimes struggles with small or partially occluded objects, which \\nare common in crowded scenes or complex industrial environments (Norkobil Saydira-\\nsulovich et al. 2023; Li et al. 2023). This limitation can be critical in applications such \\nas automated surveillance or advanced manufacturing monitoring.\\n ● YOLOv6, while efficient, still requires considerable computational resources when \\ncompared to other models optimized for edge devices. Its deployment in resource-con-\\nstrained environments such as mobile or embedded systems often requires a trade-off  \\nbetween detection performance and operational efficiency. Further optimizations and \\nmodel pruning are necessary to achieve the best of both worlds-real-time performance \\nwith reduced computational demands.\\n ● Future enhancements for YOLOv6 could focus on incorporating more advanced feature \\nextraction techniques that improve its robustness to variations in object appearance and \\nenvironmental conditions. Additionally, integrating more adaptive and context-aware \\nlearning mechanisms could help overcome some of the challenges related to background \\nclutter and similar adversities. Enhancing the model’s capacity to learn from a limited \\nnumber of training samples, through techniques such as few-shot learning or transfer \\nlearning, could address the scarcity of labeled training data in specialized applications.\\nYOLOv5:\\n ● YOLOv5 has made significant strides in improving detection speed and accuracy, but it \\nfaces challenges in consistently detecting small objects due to its spatial resolution con-\\nstraints. This is particularly evident in fields such as medical imaging or satellite image \\nanalysis, where precision is crucial for identifying fine details. Techniques such as spa-\\ntial pyramid pooling or enhanced up-sampling may be needed to increase the receptive \\nfield and improve the detection of smaller objects without compromising the model’s \\nefficiency (Benjumea et al. 2021; Jung and Choi 2022; Wang et al. 2022).\\n ● While YOLOv5 offers faster training and inference times compared to previous ver -\\nsions, its deployment on edge devices is limited by high memory and processing re -\\nquirements (Wu et al. 2021; Jia et al. 2023). Although optimized models like YOLOv5s \\nprovide a solution, they sometimes do so at the cost of detection accuracy. Optimizing \\n1 3\\n  274  Page 62 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c61e5cb2-3906-4931-bfff-898a5217d592', embedding=None, metadata={'page_label': '63', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nnetwork architecture through neural architecture search (NAS) could potentially offer a \\nmore balanced solution, enhancing both performance and efficiency for real-time object \\ndetection applications.\\n ● The adaptability of YOLOv5 to varied environmental conditions and different types \\nof data distribution remains an area for development. Future research could focus on \\nenhancing the robustness of YOLOv5 through advanced data augmentation techniques \\nand domain adaptation strategies. This would enable the model to maintain high ac -\\ncuracy levels across diverse application settings, from urban surveillance to complex \\nnatural environments, effectively handling variations in lighting, weather, and seasonal \\nchanges.\\nYOLOv4, YOLOv3, YOLOv2 and YOLOv1:\\n ● While YOLOv4 introduced notable enhancements in speed and accuracy, it still exhibits \\nperformance inconsistencies across different datasets, particularly with class imbalance \\nand the detection of rare objects. The model’s high computational demand also restricts \\nits deployment on low-power devices. Continued efforts to improve model compression \\nand increase adaptability to varying environmental conditions are essential to extend its \\npractical utility in diverse real-world applications.\\n ● YOLOv3 improved upon the balance of speed and accuracy, yet it struggles with small \\nobject detection due to its grid limitation. Its computational efficiency poses challenges \\nfor deployment in resource-constrained environments, prompting research towards op -\\ntimization techniques to improve efficiency without sacrificing performance. Addition-\\nally, enhancing the model’s robustness to environmental variations could improve its \\nreliability for applications like autonomous driving and urban surveillance.\\n ● Despite the incremental improvements introduced in YOLOv2, it faces challenges in \\ndetecting small objects, balancing speed with accuracy, and maintaining relevance with \\nthe advent of more capable successors. This version’s reliance on a fixed grid system \\nhampers its ability to perform in high-precision detection tasks. Future developments \\nmay shift towards adapting YOLOv2’s core strengths in new architectures that enhance \\nits spatial resolution and dynamic scaling capabilities.\\nFor the versions of YOLO under YOLOv5, their use may decrease and discontinue in the \\nfuture as newer versions are replacing the older YOLO versions in overall performance and \\nefficiency.\\n ● The potential for YOLOv4, YOLOv3, and YOLOv2 in future research involves explor-\\ning adaptive mechanisms that can tailor learning rates and augment data to better handle \\ndiverse operational scenarios. Integrating these models with newer technologies like \\nmodel pruning and feature fusion may address existing inefficiencies and extend their \\napplicability to a wider range of applications.\\n ● YOLOv1 was revolutionary for its time, introducing real-time object detection by pro -\\ncessing the entire image at once as a single regression problem. However, it faces sig -\\nnificant challenges in dealing with small objects due to each grid cell predicting only \\ntwo boxes and the probabilities for the classes. This structure often leads to poor perfor-\\nmance on groups of small objects that are close together, such as flocks of birds or traffic \\n1 3\\nPage 63 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d4c75cf-5b6b-4f6c-93da-57be4346abf8', embedding=None, metadata={'page_label': '64', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nscenes with multiple vehicles at a distance. Improvements in subsequent models focus \\non increasing the number of predictions per grid and incorporating finer-grained feature \\nmaps to enhance small object detection.\\n ● Another limitation of YOLOv1 is the spatial constraints of its bounding boxes. Since \\neach cell in the grid can only predict two boxes and has limited context about its neigh-\\nboring cells, the precision in localizing objects, especially those with complex or ir -\\nregular shapes, is often compromised. This challenge is particularly evident in medical \\nimaging and satellite image analysis, where the exact contours of the objects are crucial. \\nAdvances in convolutional neural network designs and cross-layer feature integration in \\nlater versions seek to address these drawbacks.\\n ● Although YOLOv1 laid the groundwork for real-time object detection, its direct usage \\nhas significantly diminished, with advancements in the field largely driven by more \\nrecent iterations such as YOLOv4 and beyond. These newer models have not only re -\\ntained the core principles of YOLOv1 but have also introduced improved mechanisms \\nfor handling diverse object sizes and aspect ratios. Current and future research is less \\nlikely to concentrate on YOLOv1 and earlier versions like YOLOv3, but rather on ad -\\nvancing these later iterations or developing hybrid models that might incorporate el -\\nements of YOLOv1’s architecture to benefit applications where high speed and low \\nlatency are paramount, despite potential trade-offs in detection precision and detail.\\n ● Future iterations could focus on dynamic grid systems, lighter network architectures, \\nand advanced scaling features to tackle the challenges of small object detection and \\ncomputational limitations. These improvements could enhance their deployment in \\nemerging areas such as edge computing, where real-time processing and low power \\nconsumption are crucial.\\n ● As YOLO continues to evolve with newer iterations like YOLOv8 and YOLOv9, the \\ncore principles of earlier versions such as YOLOv4, YOLOv3, and YOLOv2 still hold \\nsignificant value for developing hybrid models and specialized applications. The re -\\nsearch community is increasingly focused on harnessing the rapid detection capabilities \\nof these older versions while addressing their limitations in detection accuracy through \\ncomposite and hybrid modeling strategies. This trend is evidenced by innovations that \\nintegrate YOLO with other architectures to enhance overall performance. For exam -\\nple, a hybrid model that combines CNNs, YOLO, and Vision Transformers (ViTs) has \\ndemonstrated enhanced detection accuracy and reduced inference times by utilizing \\nCNNs for robust feature extraction, YOLO for quick object detection, and ViTs for \\ncapturing global context (Ali et al. 2024). Similarly, the DA-ActNN-YOLOv5 model \\nmerges YOLOv5 with advanced data augmentation and ActNN’s model compression \\ntechniques to optimize both accuracy and efficiency across diverse operational environ-\\nments (Zhu et al. 2021; Bashir et al. 2023)\\n6.2 Challenges in statistical metrics for evaluation\\nThreat: Evaluating YOLO detection systems requires a unique approach, as each version, \\nfrom the original YOLO to the latest YOLOv12, targets different aspects of detection capa-\\nbility, such as speed, accuracy, or computational efficiency. For a comprehensive evaluation, \\nit is essential to employ a diverse array of metrics, including precision, recall, GFLOPs, \\nand model size. This approach allows for a more complete comparison and understanding \\n1 3\\n  274  Page 64 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a02f26b1-4266-4e1a-ae80-d836f5103b17', embedding=None, metadata={'page_label': '65', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nof each model’s strengths and weaknesses in various real-world applications. This multi-\\nmetric evaluation is crucial to assessing the practical utility and technological advancement \\nof the YOLO series. Future YOLO versions are expected to introduce novel evaluation met-\\nrics that capture emerging capabilities in edge computing, multi-modal fusion, and adaptive \\narchitecture optimization, necessitating an even more sophisticated evaluation framework \\nto accurately assess their performance across diverse deployment scenarios.\\nMitigation: Despite this limitation, our main premise is that the selected metrics enable \\nus to compare various YOLO systems and adequately assess their overall effectiveness. \\nRecognizing the inherent limitations of statistical summaries is crucial when conducting \\na comprehensive evaluation of detection systems across different applications. Therefore, \\nwe aim to improve the clarity and reliability of our review by openly acknowledging these \\npotential threats to construct validity. This approach provides a more nuanced understand -\\ning of the limitations associated with various aspects of YOLO techniques for object detec-\\ntion in diverse domains.\\nSpectral versus RGB images: Beyond traditional RGB imaging, spectral features encom-\\npass a broader spectrum, including infrared, ultraviolet, and even multispectral and hyper -\\nspectral imaging. These advanced spectral techniques can significantly enhance YOLO’s \\nobject detection capabilities by providing additional information not visible in the RGB \\nspectrum. For example, hyperspectral imaging can detect subtle variations in plant health \\nfor agricultural applications or distinguish between materials based on their spectral sig -\\nnatures in industrial settings. This expansion into wider spectral data not only improves \\ndetection accuracy but also opens up new avenues for application-specific optimizations, \\nreinforcing YOLO’s versatility and potential across various fields.\\nOver the past decade, the series of YOLO models have significantly impacted various \\nsectors, demonstrating the powerful capabilities of deep learning in real-world applica -\\ntions. As a pioneering object detection algorithm, YOLO has facilitated rapid advance -\\nments across diverse fields by offering high-speed, real-time detection with commendable \\naccuracy. One of the most notable applications has been in public safety and surveillance, \\nwhere YOLO models have improved the efficacy of monitoring systems, enhancing the \\ndetection of suspicious activities and ensuring public safety more efficiently. In the realm of \\nautomotive technology, YOLO has been integral in developing advanced driver-assistance \\nsystems (ADAS)  (Malligere Shivanna and Guo 2024), contributing to object detection that \\nsupports collision avoidance systems and pedestrian safety. Furthermore, YOLO has trans-\\nformed the healthcare sector by accelerating medical image analysis, enabling quicker and \\nmore accurate detection of pathologies which is critical for diagnostics and treatment plan-\\nning. In industrial settings, YOLO has optimized quality control processes by identifying \\ndefects in manufacturing lines in real-time, thereby reducing waste and increasing produc -\\ntion efficiency. Additionally, in the retail sector, YOLO has supported inventory manage -\\nment through automated checkouts and stock monitoring, enhancing customer experience \\nand operational efficiency, whereas in agriculture, YOLO has played a key role to enhance \\ntimely crop stress detection, pest localization and precision crop management while improv-\\ning worker health and safety.\\n1 3\\nPage 65 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2d30f329-7ebc-443a-87dd-ea8366a61637', embedding=None, metadata={'page_label': '66', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\n7 Future directions in object detection with YOLO\\n7.1 YOLO deployment on edge and IoT devices\\nThe deployment of YOLO on edge devices unlocks several promising avenues for future \\nresearch and development. One potential direction involves enhancing the algorithm’s \\nefficiency and accuracy for even more constrained environments, such as ultra-low-power \\nMicrocontrollers and embedded systems. This can be achieved through further optimiza -\\ntion techniques, including model pruning, quantization, and the development of specialized \\nhardware accelerators. Additionally, integrating YOLO with advanced communication pro-\\ntocols, edge computing frameworks and IoT devices could facilitate more seamless collabo-\\nration between edge devices and centralized cloud services, enhancing the overall system \\nperformance and scalability. Exploring the integration of YOLO with other AI-driven func-\\ntionalities, such as anomaly detection and predictive analytics, may unlock new applications \\nin areas like healthcare, smart cities, and industrial automation. As edge computing contin-\\nues to evolve, the adaptation of YOLO to support federated learning paradigms could ensure \\nthe data privacy while enabling continuous learning and improvement of object detection \\nmodels. These future directions will not only expand the capabilities of YOLO but also con-\\ntribute significantly to the advancement of intelligent edge computing systems (Yang et al. \\n2022; Ghaziamin et al. 2024; Hussain et al. 2022; Zhang 2024).\\n7.2 YOLO and embodied artificial intelligence\\nEmbodied Artificial Intelligence (EAI) refers to AI systems integrated with physical entities \\nor bodies, enabling them to interact with the real world in a natural and human-like man -\\nner (Pfeifer and Iida 2004). Incorporating YOLO into these systems significantly enhances \\ntheir sensory capabilities, allowing for more efficient and accurate interaction with the \\nphysical environment. Applications of YOLO in EAI include autonomous vehicles, drones, \\nrobots (Sanket 2021), human-robot interaction (Wang et al. 2024). Additionally, it plays a \\nsignificant role in healthcare, particularly with robotic surgical assistants (Lakshmipathy \\net al. 2024), among other innovative uses (Li et al. 2024).\\n8 Expanding YOLO object detection into broader AI domains\\n8.1 YOLO and artificial general intelligence\\nArtificial General Intelligence (AGI) refers to an intelligent agent with human-level or higher \\nintelligence, capable of solving a variety of complex problems in diverse domains (Pande \\net al. 2024; Qu et al. 2024). In this context, an AGI system would need to integrate object \\ndetection capabilities, similar to those provided by YOLO, with other essential cognitive \\nfunctions, such as advanced natural language understanding, reasoning, and decision-mak -\\ning. This fusion will enable the system to handle a broad spectrum of tasks in real-time, \\nadapting to dynamic environments and complex scenarios effectively, thus advancing the \\ncurrent AI systems towards achieving a true AGI.\\n1 3\\n  274  Page 66 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0ff0bc52-5ac6-4e9e-9ded-2f063f81dc16', embedding=None, metadata={'page_label': '67', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\n8.2 YOLO integration with large language models\\nOne effective way to advance AGI capabilities is to integrate YOLO with Large Language \\nModels (LLMs) by seamlessly merging advanced visual data interpretation with sophis -\\nticated natural language understanding, reasoning, and contextual awareness. This fusion \\nwould allow the AGI to not only recognize and analyze objects in real-time but also engage \\nin meaningful interactions with stakeholders (or end users), making more informed deci -\\nsions and adapting to complex tasks with greater autonomy and precision. This synergy \\nwould enable AGI systems to operate in complex environments handling multi-modal inputs \\nsimultaneously, such as navigating through an environment using visual cues identified by \\nYOLO while interpreting and acting on spoken commands through capabilities provided by \\nLLMs (Rouhi et al. 2025; Sapkota et al. 2024e, b). Such integration is expected to lead to a \\nhighly versatile and intelligent system, capable of performing real-time, multi-faceted oper-\\nations across diverse application domains. By combining advanced visual recognition with \\nrobust language processing, it brings us a step closer to realizing true AGI, where the system \\ncan autonomously adapt, learn, and perform complex tasks with human-like flexibility and \\nreasoning. Future YOLO versions, such as YOLOv13, YOLOv14, YOLOv15, and beyond, \\nare anticipated to advance toward AGI, integrating enhanced reasoning, adaptability, and \\nautonomous learning capabilities beyond traditional object detection.\\n9 YOLO and environmental impact\\nTraining and retraining YOLO is extremely energy-intensive, leading to substantial energy \\nand water consumption, as well as significant carbon dioxide emissions. This environmental \\nimpact underscores concerns about the sustainability of AI development, emphasizing the \\nurgent need for more efficient practices to reduce the ecological footprint of large-scale \\nmodel training (Xu et al. 2024; Dhar 2020).\\n10 Conclusion\\nIn this comprehensive review, we explored the evolution of the YOLO models from the \\nmost recent YOLOv12 to the inaugural YOLOv1, including alternative versions of YOLO \\nas YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and Gold-YOLO. This retrospective \\nanalysis covered a decade of advancements, highlighting theapplied use of each version \\nand their respective impacts across five critical application areas: autonomous vehicles and \\ntraffic safety, healthcare and medical imaging, security and surveillance, manufacturing, and \\nagriculture. Our review outlined the significant enhancements in detection speed, accuracy, \\nand computational efficiency that each iteration brought, while also addressing the specific \\nchallenges and limitations faced by earlier versions. Furthermore, we identified gaps in the \\ncurrent capabilities of YOLO models and proposed potential directions for future research, \\nsuch as trade-off between detection speed versus accuracy, handling small and overlapping \\nObjects, and generalization across diverse datasets and domains. Predicting the trajectory \\nof YOLO’s development, we anticipate a shift towards multimodal data processing, lever -\\naging advancements in large language models and natural language processing to enhance \\n1 3\\nPage 67 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9da21b77-9012-44cb-8a14-b1c822a3b405', embedding=None, metadata={'page_label': '68', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nobject detection systems. This fusion is expected to broaden the utility of YOLO models, \\nenabling more sophisticated, context-aware applications that could revolutionize the inter -\\naction between AI systems and their environments using Generative AI and multi-modal \\nLLMs. Thus, this review not only serves as a detailed chronicle of YOLO’s evolution but \\nalso sets a prospective blueprint for its integration into the next generation of technological \\ninnovations.\\nAcknowledgements This work was supported in part by the National Science Foundation (NSF) and the \\nUnited States Department of Agriculture (USDA), National Institute of Food and Agriculture (NIFA), through \\nthe “Artificial Intelligence (AI) Institute for Agriculture” program under Award Numbers AWD003473 \\nand AWD004595, and USDA-NIFA Accession Number 1029004 for the project titled “Robotic Blossom \\nThinning with Soft Manipulators.” Additional support was provided through USDA-NIFA Grant Number \\n2024-67022-41788, Accession Number 1031712, under the project “Expanding UCF AI Research To Novel \\nAgricultural Engineering Applications (PARTNER).” Additionally, this work is partially supported by the \\nHong Kong Innovation and Technology Commission (InnoHK Project CIMDA) and by EcuTS2025 from \\nUFA-ESPE.\\nAuthor contributions Ranjan Sapkota: principal conceptualizer, research design, formal analysis, original \\ndraft preparation, manuscript writing, and editing. Marco Flores-Calero, Rizwan Qureshi, Chetan Badgu -\\njar, Upesh Nepal, Alwin Poulose, Peter Zeno, Uday Bhanu Prakash Vaddevolu, Sheheryar Khan, Maged \\nShoman, Hong Yan: methodology refinement, critical revisions, manuscript review, and editing. Manoj Kar-\\nkee: Funding and supervision, methodology refinement, critical revisions, manuscript review, and editing. \\nRanjan Sapkota and Manoj Karkee: Corresponding authors.\\nData availability No datasets were generated or analysed during the current study.\\nDeclarations\\nConflict of interest The authors declare no conflict of interest.\\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \\nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons \\nlicence, and indicate if changes were made. The images or other third party material in this article are \\nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. \\nIf material is not included in the article’s Creative Commons licence and your intended use is not permitted \\nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \\ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nReferences\\nLiu L, Ouyang W, Wang X, Fieguth P, Chen J, Liu X, Pietikäinen M (2020) Deep learning for generic object \\ndetection: a survey. Int J Comput Vision 128:261–318\\nBadgujar CM, Poulose A, Gan H (2024) Agricultural object detection with You Only Look Once (YOLO) \\nalgorithm: a bibliometric and systematic literature review. Comput Electron Agric 223:109090.  h t t p s : / / \\nd o i . o r g / 1 0 . 1 0 1 6 / j . c o m p a g . 2 0 2 4 . 1 0 9 0 9 0       \\nAhmad HM, Rahimi A (2022) Deep learning methods for object detection in smart manufacturing: a survey. \\nJ Manuf Syst 64:181–196\\nGheorghe C, Duguleana M, Boboc RG, Postelnicu CC (2024) Analyzing real-time object detection with \\nYOLO algorithm in automotive applications: a review. CMES - Comput Model Eng Sci 141(3):1939–\\n1981 https://doi.org/10.32604/cmes.2024.054735\\nArkin E, Yadikar N, Xu X, Aysa A, Ubul K (2023) A survey: object detection methods from CNN to trans -\\nformer. Multim Tools Appl 82(14):21353–21383\\n1 3\\n  274  Page 68 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='162cc7c6-c4a6-43da-a584-924ad372074b', embedding=None, metadata={'page_label': '69', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nFernandez RAS, Sanchez-Lopez JL, Sampedro C, Bavle H, Molina M, Campoy P (2016) Natural user inter-\\nfaces for human-drone multi-modal interaction. In: 2016 International Conference on Unmanned Air -\\ncraft Systems (ICUAS), pp. 1013–1022. IEEE\\nWang RJ, Li X, Ling CX (2018) PELEE: A real-time object detection system on mobile devices. Adv Neural \\nInform Process Syst 31\\nRen S, He K, Girshick R, Sun J (2015) Faster R-CNN: Towards real-time object detection with region pro -\\nposal networks. Adv Neural Inform Process Syst 28\\nTang J, Ye C, Zhou X, Xu L (2024) YOLO-fusion and internet of things: advancing object detection in smart \\ntransportation. Alex Eng J 107:1–12\\nChen H, Guan J (2022) Teacher-student behavior recognition in classroom teaching based on improved \\nYOLO-V4 and internet of things technology. Electronics 11(23):3998\\nRagab MG, Abdulkader SJ, Muneer A, Alqushaibi A, Sumiea EH, Qureshi R, Al-Selwi SM, Alhussian H \\n(2024) A comprehensive systematic review of YOLO for medical object detection (2018 to 2023). \\nIEEE Access\\nFlippo D, Gunturu S, Baldwin C, Badgujar C (2023) Tree trunk detection of eastern red cedar in rangeland \\nenvironment with deep learning technique. Croatian J For Eng 44(2):357–368.  h t t p s : / / d o i . o r g / 1 0 . 5 5 5 2 \\n/ c r o j f e . 2 0 2 3 . 2 0 1 2       \\nMalligere Shivanna V , Guo J-I (2024) Object detection, recognition, and tracking algorithms for ADASs—a \\nstudy on recent trends. Sensors. https://doi.org/10.3390/s24010249\\nFlores-Calero M, Astudillo CA, Guevara D, Maza J, Lita BS, Defaz B, Ante JS, Zabala-Blanco D, Armingol \\nMoreno JM (2024) Traffic sign detection and recognition using YOLO object detection algorithm: a \\nsystematic review. Mathematics. https://doi.org/10.3390/math12020297\\nGuerrero-Ibáñez J, Zeadally S, Contreras-Castillo J (2018) Sensor technologies for intelligent transportation \\nsystems. Sensors 18(4):1212\\nShoman M, Wang D, Aboah A, Abdel-Aty M (2024) Enhancing traffic safety with parallel dense video \\ncaptioning for end-to-end event analysis. In: Proceedings of the IEEE/CVF Conference on Computer \\nVision and Pattern Recognition (CVPR) Workshops, pp 7125–7133\\nHnewa M, Radha H (2023) Integrated multiscale domain adaptive YOLO. IEEE Trans Image Process \\n32:1857–1867\\nHussain R, Zeadally S (2018) Autonomous cars: Research results, issues, and future challenges. IEEE Com-\\nmun Surv Tutor 21(2):1275–1313\\nShoman M, Lanzaro G, Sayed T, Gargoum S (2024) Autonomous vehicle-pedestrian interaction modeling \\nplatform: a case study in four major cities. J Transport Eng Part A: Syst 150(9):04024045.  h t t p s : / / d o i . o \\nr g / 1 0 . 1 0 6 1 / J T E P B S . T E E N G - 8 0 9 7       \\nKaushal M, Khehra BS, Sharma A (2018) Soft computing based object detection and tracking approaches: \\nstate-of-the-art survey. Appl Soft Comput 70:423–464\\nXiang J, Fan H, Liao H, Xu J, Sun W, Yu S (2014) Moving object detection and shadow removing under \\nchanging illumination condition. Math Probl Eng 1:827461\\nXiao Y , Jiang A, Ye J, Wang M-W (2020) Making of night vision: object detection under low-illumination. \\nIEEE Access 8:123075–123086\\nSeoni S, Shahini A, Meiburger KM, Marzola F, Rotunno G, Acharya UR, Molinari F, Salvi M (2024) All you \\nneed is data preparation: a systematic review of image harmonization techniques in multi-center/device \\nstudies for medical support systems. Comput Methods Programs Biomed 108200\\nKhan SM, Shah M (2008) Tracking multiple occluding people by localizing on multiple scene planes. IEEE \\nTrans Pattern Anal Mach Intell 31(3):505–519\\nMostafa T, Chowdhury SJ, Rhaman MK, Alam MGR (2022) Occluded object detection for autonomous \\nvehicles employing YOLOV5, YOLOX and faster R-CNN. In: 2022 IEEE 13th Annual Information \\nTechnology, Electronics and Mobile Communication Conference (IEMCON), pp 0405–0410. IEEE\\nGupta A, Anpalagan A, Guan L, Khwaja AS (2021) Deep learning for object detection and scene perception \\nin self-driving cars: survey, challenges, and open issues. Array 10:100057\\nZou Z, Chen K, Shi Z, Guo Y , Ye J (2023) Object detection in 20 years: a survey. Proc IEEE 111(3):257–276\\nPark K, Patten T, Prankl J, Vincze M (2019) Multi-task template matching for object detection, segmentation \\nand pose estimation using depth images. In: 2019 International Conference on Robotics and Automation \\n(ICRA), pp 7207–7213. IEEE\\nLiu S, Liu D, Srivastava G, Połap D, Woźniak M (2021) Overview and methods of correlation filter algo -\\nrithms in object tracking. Compl Intell Syst 7:1895–1917\\nTeutsch M, Kruger W (2015) Robust and fast detection of moving vehicles in aerial videos using sliding \\nwindows. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Work-\\nshops, pp 26–34\\nLienhart R, Maydt J (2002) An extended set of haar-like features for rapid object detection. In: Proceedings. \\nInternational Conference on Image Processing, vol. 1, IEEE\\n1 3\\nPage 69 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7bb67a0e-a426-4ccc-93da-2a2b442a284a', embedding=None, metadata={'page_label': '70', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nJun-Feng G, Yu-Pin L (2009) A comprehensive study for asymmetric adaboost and its application in object \\ndetection. Acta Automatica Sinica 35(11):1403–1409\\nLi Q, Niaz U, Merialdo B (2012) An improved algorithm on viola-jones object detector. In: 2012 10th Inter-\\nnational Workshop on Content-Based Multimedia Indexing (CBMI), pp 1–6. IEEE\\nHu X-d, Wang X-q, Meng F-j, Hua X, Yan Y-j, Li Y-y, Huang J, Jiang X-l (2020) Gabor-CNN for object \\ndetection based on small samples. Def Technol 16(6):1116–1129\\nSurasak T, Takahiro I, Cheng C-h, Wang C-e, Sheng P-y (2018) Histogram of oriented gradients for human \\ndetection in video. In: 2018 5th International Conference on Business and Industrial Research (ICBIR), \\npp 172–176. IEEE\\nKaris MS, Razif NRA, Ali NM, Rosli MA, Aras MSM, Ghazaly MM (2016) Local binary pattern (lbp) with \\napplication to variant object detection: A survey and method. In: 2016 IEEE 12th International Collo -\\nquium on Signal Processing & Its Applications (CSPA), pp. 221–226. IEEE\\nMita T, Kaneko T, Hori O (2005) Joint haar-like features for face detection. In: Tenth IEEE International \\nConference on Computer Vision (ICCV’05) V olume 1, vol 2, pp 1619–1626. IEEE\\nYan J, Lei Z, Wen L, Li SZ (2014) The fastest deformable part model for object detection. In: Proceedings of \\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp 2497–2504\\nPiccinini P, Prati A, Cucchiara R (2012) Real-time object detection and localization with sift-based cluster -\\ning. Image Vis Comput 30(8):573–587\\nLi J, Zhang Y (2013) Learning surf cascade for fast and accurate object detection. In: Proceedings of the \\nIEEE Conference on Computer Vision and Pattern Recognition, pp 3468–3475\\nChiu H-J, Li T-HS, Kuo P-H (2020) Breast cancer-detection system using PCA, multilayer perceptron, trans-\\nfer learning, and support vector machine. IEEE Access 8:204309–204324\\nMienye ID, Sun Y (2022) A survey of ensemble learning: concepts, algorithms, applications, and prospects. \\nIEEE Access 10:99129–99149. https://doi.org/10.1109/ACCESS.2022.3207287\\nXiang Y , Mottaghi R, Savarese S (2014) Beyond pascal: a benchmark for 3D object detection in the wild. In: \\nIEEE Winter Conference on Applications of Computer Vision, pp 75–82. IEEE\\nKrizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with deep convolutional neural net -\\nworks. Adv Neural Inform Process Syst 25\\nXie X, Cheng G, Wang J, Yao X, Han J (2021) Oriented R-CNN for object detection. In: Proceedings of the \\nIEEE/CVF International Conference on Computer Vision, pp 3520–3529\\nTang S, Yuan Y (2015) Object detection based on convolutional neural network. In: International \\nConference-IEEE–2016\\nZhiqiang W, Jun L (2017) A review of object detection based on convolutional neural network. In: 2017 36th \\nChinese Control Conference (CCC), pp 11104–11109. IEEE\\nLi X, Song D, Dong Y (2020) Hierarchical feature fusion network for salient object detection. IEEE Trans \\nImage Process 29:9165–9175\\nCrawford E, Pineau J (2019) Spatially invariant unsupervised object detection with convolutional neural \\nnetworks. Proc AAAI Conf Artif Intell 33:3412–3420\\nTan M, Pang R, Le QV (2020) Efficientdet: Scalable and efficient object detection. In: Proceedings of the \\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 10781–10790\\nCarion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S (2020) End-to-end object detection \\nwith transformers. In: European Conference on Computer Vision, pp 213–229. Springer\\nLi Y , Mao H, Girshick R, He K (2022) Exploring plain vision transformer backbones for object detection. In: \\nEuropean Conference on Computer Vision, pp 280–296. Springer\\nGirshick R, Donahue J, Darrell T, Malik J, Mercan E (2014) R-CNN for object detection. In: IEEE Conference\\nBhat S, Shenoy KA, Jain MR, Manasvi K (2023) Detecting crops and weeds in fields using YOLOv6 and \\nFaster R-CNN object detection models. In: 2023 International Conference on Recent Advances in Infor-\\nmation Technology for Sustainable Development (ICRAIS), pp 43–48. IEEE\\nGirshick R (2015) Fast R-CNN. In: Proceedings of the IEEE International Conference on Computer Vision, \\npp 1440–1448\\nLiu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu C-Y , Berg AC (2016) Ssd: Single shot multibox \\ndetector. In: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, \\nOctober 11–14, 2016, Proceedings, Part I 14, pp 21–37. Springer\\nRedmon J, Divvala S, Girshick R, Farhadi A (2016) You only look once: Unified, real-time object detection. \\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 779–788\\nTian Y , Ye Q, Doermann D (2025) YOLOv12: Attention-centric real-time object detectors. Preprint at \\narXiv:2502.12524\\nWang C, Sun Q, Dong X, Chen J (2024) Automotive adhesive defect detection based on improved YOLOv8. \\nSignal, Image and Video Processing, pp 1–13\\n1 3\\n  274  Page 70 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3d0dd438-b3f2-4126-aa21-9c0d0a5f3752', embedding=None, metadata={'page_label': '71', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nShoman M, Ghoul T, Lanzaro G, Alsharif T, Gargoum S, Sayed T (2024) Enforcing traffic safety: a deep \\nlearning approach for detecting motorcyclists’ helmet violations using YOLOv8 and deep convolutional \\ngenerative adversarial network-generated images. Algorithms. https://doi.org/10.3390/a17050202\\nPatel GS, Desai AA, Kamble YY , Pujari GV , Chougule PA, Jujare V A (2023) Identification and separation of \\nmedicine through robot using YOLO and CNN algorithms for healthcare. In: 2023 International Confer-\\nence on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI), vol 1, pp 1–5. IEEE\\nLuo Y , Zhang Y , Sun X, Dai H, Chen X, et al (2021) Intelligent solutions in chest abnormality detection based \\non YOLOv5 and resnet50. J Healthcare Eng 2021\\nSalinas-Medina A, Neme A (2023) Enhancing hospital efficiency through web-deployed object detection: A \\nYOLOv8-based approach for automating healthcare operations. In: 2023 Mexican International Confer-\\nence on Computer Science (ENC), pp 1–6. IEEE\\nPham D-L, Chang T-W et al (2023) A YOLO-based real-time packaging defect detection system. Proc Com-\\nput Sci 217:886–894\\nKlarák J, Andok R, Malík P, Kuric I, Ritomskỳ M, Klačková I, Tsai H-Y (2024) From anomaly detection to \\ndefect classification. Sensors 24(2):429\\nArroyo MA, Ziad MTI, Kobayashi H, Yang J, Sethumadhavan S (2019) YOLO: frequently resetting cyber-\\nphysical systems for security. In: Autonomous Systems: Sensors, Processing, and Security for Vehicles \\nand Infrastructure 2019, vol 11009, pp 166–183. SPIE\\nBordoloi N, Talukdar AK, Sarma KK (2020) Suspicious activity detection from videos using YOLOv3. In: \\n2020 IEEE 17th India Council International Conference (INDICON), pp 1–5. IEEE\\nBadgujar CM, Poulose A, Gan H (2024) Agricultural object detection with you look only once (YOLO) algo-\\nrithm: A bibliometric and systematic literature review. Preprint at arXiv:2401.10379\\nLi J, Qiao Y , Liu S, Zhang J, Yang Z, Wang M (2022) An improved YOLOv5-based vegetable disease detec-\\ntion method. Comput Electron Agric 202:107345\\nFu L, Feng Y , Wu J, Liu Z, Gao F, Majeed Y , Al-Mallahi A, Zhang Q, Li R, Cui Y (2021) Fast and accurate \\ndetection of kiwifruit in orchard using improved YOLOv3-tiny model. Precision Agric 22:754–776\\nZhong Y , Gao J, Lei Q, Zhou Y (2018) A vision-based counting and recognition system for flying insects in \\nintelligent agriculture. Sensors 18(5):1489\\nWang Y , Yang L, Chen H, Hussain A, Ma C, Al-gabri M (2022) Mushroom-YOLO: A deep learning algo-\\nrithm for mushroom growth recognition based on improved YOLOv5 in agriculture 4.0. In: 2022 IEEE \\n20th International Conference on Industrial Informatics (INDIN), pp 239–244. IEEE\\nJiang K, Xie T, Yan R, Wen X, Li D, Jiang H, Jiang N, Feng L, Duan X, Wang J (2022) An attention \\nmechanism-improved YOLOv7 object detection algorithm for hemp duck count estimation. Agriculture \\n12(10):1659\\nChen G, Hou Y , Cui T, Li H, Shangguan F, Cao L (2023) YOLOv8-CML: A lightweight target detection \\nmethod for color-changing melon ripening in intelligent agriculture. ResearchSquare\\nYu X, Yin D, Xu H, Pinto Espinosa F, Schmidhalter U, Nie C, Bai Y , Sankaran S, Ming B, Cui N, et al (2024) \\nMaize tassel number and tasseling stage monitoring based on near-ground and UA V RGB images by \\nimproved YOLOv8. Precis Agric 1–39\\nJia L, Wang T, Chen Y , Zang Y , Li X, Shi H, Gao L (2023) MobileNet-CA-YOLO: an improved YOLOv7 \\nbased on the MobileNetV3 and attention mechanism for rice pests and diseases detection. Agriculture \\n13(7):1285\\nUmar M, Altaf S, Ahmad S, Mahmoud H, Mohamed ASN, Ayub R (2024) Precision agriculture through deep \\nlearning: tomato plant multiple diseases recognition with CNN and improved YOLOv7. IEEE Access\\nSapkota R, Ahmed D, Karkee M (2024) Comparing YOLOv8 and Mask R-CNN for instance segmentation \\nin complex orchard environments. Artif Intell Agric\\nBakirci M, Bayraktar I (2024) Transforming aircraft detection through Leo satellite imagery and YOLOv9 \\nfor improved aviation safety. In: 2024 26th International Conference on Digital Signal Processing and \\nIts Applications (DSPA), pp. 1–6. IEEE\\nPrinzi F, Insalaco M, Orlando A, Gaglio S, Vitabile S (2024) A YOLO-based model for breast cancer detec-\\ntion in mammograms. Cogn Comput 16(1):107–120\\nAly GH, Marey M, El-Sayed SA, Tolba MF (2021) YOLO based breast masses detection and classification \\nin full-field digital mammograms. Comput Methods Programs Biomed 200:105823\\nÜnver HM, Ayan E (2019) Skin lesion segmentation in dermoscopic images with combination of YOLO and \\ngrabcut algorithm. Diagnostics 9(3):72\\nTan L, Huangfu T, Wu L, Chen W (2021) Comparison of RetinaNet, SSD, and YOLO v3 for real-time pill \\nidentification. BMC Med Inform Decis Mak 21:1–11\\nSuksawatchon U, Srikamdee S, Suksawatchon J, Werapan W (2022) Shape recognition using unconstrained \\npill images based on deep convolution network. In: 2022 6th International Conference on Information \\nTechnology (InCIT), pp 309–313. IEEE\\n1 3\\nPage 71 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d6483242-2631-4093-a618-06623d85ca4f', embedding=None, metadata={'page_label': '72', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nPratibha K, Mishra M, Ramana G, Lourenço PB (2023) Deep learning-based YOLO network model for \\ndetecting surface cracks during structural health monitoring. In: International Conference on Structural \\nAnalysis of Historical Constructions, pp 179–187. Springer\\nFahim F, Hasan MS (2024) Enhancing the reliability of power grids: A YOLO based approach for insulator \\ndefect detection. e-Prime-Advances in Electrical Engineering, Electronics and Energy 9:100663\\nGorave A, Misra S, Padir O, Patil A, Ladole K (2020) Suspicious activity detection using live video analysis. \\nIn: Proceeding of International Conference on Computational Science and Applications: ICCSA 2019, \\npp 203–214. Springer\\nKolpe R, Ghogare S, Jawale M, William P, Pawar A (2022) Identification of face mask and social distancing \\nusing YOLO algorithm based on machine learning approach. In: 2022 6th International Conference on \\nIntelligent Computing and Control Systems (ICICCS), pp 1399–1403. IEEE\\nBashir S, Qureshi R, Shah A, Fan X, Alam T (2023) YOLOv5-m: A deep neural network for medical object \\ndetection in real-time. In: 2023 IEEE Symposium on Industrial Electronics & Applications (ISIEA), pp \\n1–6. IEEE\\nAjayi OG, Ashi J, Guda B (2023) Performance evaluation of YOLO v5 model for automatic crop and weed \\nclassification on UA V images. Smart Agric Technol 5:100231\\nMorbekar A, Parihar A, Jadhav R (2020) Crop disease detection using YOLO. In: 2020 International Confer-\\nence for Emerging Technology (INCET), pp 1–5. IEEE\\nLi D, Ahmed F, Wu N, Sethi AI (2022) YOLO-JD: A deep learning network for jute diseases and pests detec-\\ntion from images. Plants 11(7):937\\nCheeti S, Kumar GS, Priyanka JS, Firdous G, Ranjeeva PR (2021) Pest detection and classification using \\nYOLO and CNN. Ann Roman Soc Cell Biol 15295–15300\\nPham M-T, Courtrai L, Friguet C, Lefèvre S, Baussard A (2020) YOLO-Fine: One-stage detector of small \\nobjects under various backgrounds in remote sensing images. Remote Sens 12(15):2501\\nCheng L, Li J, Duan P, Wang M (2021) A small attentional YOLO model for landslide detection from satellite \\nremote sensing images. Landslides 18(8):2751–2765\\nChen C, Zheng Z, Xu T, Guo S, Feng S, Yao W, Lan Y (2023) YOLO-based UA V technology: a review of the \\nresearch and its applications. Drones 7(3):190\\nLuo X, Wu Y , Zhao L (2022) YOLOD: A target detection method for UA V aerial imagery. Remote Sens \\n14(14):3240\\nLi R, Yang J (2018) Improved YOLOv2 object detection model. In: 2018 6th International Conference on \\nMultimedia Computing and Systems (ICMCS), pp 1–6. IEEE\\nNakahara H, Yonekawa H, Fujii T, Sato S (2018) A lightweight YOLOv2: A binarized CNN with a parallel \\nsupport vector regression for an FPGA. In: Proceedings of the 2018 ACM/SIGDA International Sym -\\nposium on Field-programmable Gate Arrays, pp 31–40\\nKim K-J, Kim P-K, Chung Y-S, Choi D-H (2018) Performance enhancement of YOLOv3 by adding predic-\\ntion layers with spatial pyramid pooling for vehicle detection. In: 2018 15th IEEE International Confer-\\nence on Advanced Video and Signal Based Surveillance (A VSS), pp 1–6. IEEE\\nNepal U, Eslamiat H (2022) Comparing YOLOv3, YOLOv4 and YOLOv5 for autonomous landing spot \\ndetection in faulty UA Vs. Sensors 22(2):464\\nSozzi M, Cantalamessa S, Cogato A, Kayad A, Marinello F (2022) Automatic bunch detection in white grape \\nvarieties using YOLOv3, YOLOv4, and YOLOv5 deep learning algorithms. Agronomy 12(2):319\\nMohod N, Agrawal P, Madaan V (2022) YOLOv4 vs YOLOv5: Object detection on surveillance videos. In: \\nInternational Conference on Advanced Network Technologies and Intelligent Computing, pp 654–665. \\nSpringer\\nUltralytics (2020) https://github.com/ultralytics. Accessed 31 Dec 2024\\nLi C, Li L, Jiang H, Weng K, Geng Y , Li L, Ke Z, Li Q, Cheng M, Nie W, et al (2022) YOLOv6: A single-\\nstage object detection framework for industrial applications. Preprint at arXiv:2209.02976\\nWang C-Y , Bochkovskiy A, Liao H-YM (2022) YOLOv7: Trainable bag-of-freebies sets new state-of-the-art \\nfor real-time object detectors. Preprint at arXiv:2207.02696. Accessed 05 Jun 2024\\nWang C-Y , Bochkovskiy A, Liao H-YM (2023) YOLOv7: Trainable bag-of-freebies sets new state-of-the-art \\nfor real-time object detectors. In: Proceedings of the IEEE/CVF Conference on Computer Vision and \\nPattern Recognition, pp 7464–7475\\nJocher G, Chaurasia A, Qiu J (2023) Ultralytics, YOLOv 8. https://docs.ultralytics.com/models/YOLOv8/. \\nAccessed 31 Dec 2024\\nWang C-Y , Yeh I-H, Liao H-YM (2024) YOLOv9: Learning what you want to learn using programmable \\ngradient information. Preprint at arXiv:2402.13616\\nUltralytics (2023a) YOLOv 9. https://docs.ultralytics.com/models/YOLOv9/. Accessed 31 Dec 2024\\nUltralytics (2023b) YOLOv10: real-time end-to-end object detection.  h t t p s : / / d o c s . u l t r a l y t i c s . c o m / m o d e l s / Y \\nO L O v 1 0 /     . Accessed 31 Dec 2024\\n1 3\\n  274  Page 72 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c033ee4f-c84f-41ce-870e-5263957d9aa4', embedding=None, metadata={'page_label': '73', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nWang A, Chen H, Liu L, Chen K, Lin Z, Han J, Ding G (2024) YOLOv10: Real-time end-to-end object detec-\\ntion. Preprint at arXiv:2405.14458\\nMao H, Yang X, Dally WJ (2019) A delay metric for video object detection: What average precision fails \\nto tell. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp 573–582\\nChen B, Ghiasi G, Liu H, Lin TY . Kalenichenko D, Adam H, Le QV (2020) Mnasfpn: Learning latency-\\naware pyramid architecture for object detection on mobile devices. In: Proceedings of the IEEE/CVF \\nConference on Computer Vision and Pattern Recognition, pp 13607–13616\\nPestana D, Miranda PR, Lopes JD, Duarte RP, Véstias MP, Neto HC, De Sousa JT (2021) A full featured \\nconfigurable accelerator for object detection with YOLO. IEEE Access 9:75864–75877\\nZhou P, Ni B, Geng C, Hu J, Xu Y (2018) Scale-transferrable object detection. In: Proceedings of the IEEE \\nConference on Computer Vision and Pattern Recognition, pp 528–537\\nHall D, Dayoub F, Skinner J, Zhang H, Miller D, Corke P, Carneiro G, Angelova A, Sünderhauf N (2020) \\nProbabilistic object detection: Definition and evaluation. In: Proceedings of the IEEE/CVF Winter Con-\\nference on Applications of Computer Vision, pp 1031–1040\\nGoutte C, Gaussier E (2005) A probabilistic interpretation of precision, recall and f-score, with implication \\nfor evaluation. In: European Conference on Information Retrieval, pp 345–359. Springer\\nLiang Z, Zhang Z, Zhang M, Zhao X, Pu S (2021) Rangeioudet: Range image based real-time 3d object \\ndetector optimized by intersection over union. In: Proceedings of the IEEE/CVF Conference on Com -\\nputer Vision and Pattern Recognition, pp 7140–7149\\nJiang J, Xu H, Zhang S, Fang Y (2019) Object detection algorithm based on multiheaded attention. Appl Sci \\n9(9):1829\\nFu C-Y , Liu W, Ranga A, Tyagi A, Berg AC (2017) DSSD: Deconvolutional single shot detector. Preprint at \\narXiv:1701.06659\\nZhang S, Wen L, Bian X, Lei Z, Li SZ (2018) Single-shot refinement neural network for object detection. \\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 4203–4212\\nCui L, Ma R, Lv P, Jiang X, Gao Z, Zhou B, Xu M (2018) MDSSD: multi-scale deconvolutional single shot \\ndetector for small objects. Preprint at arXiv:1805.07009\\nLin T, Zhao X, Shou Z (2017) Single shot temporal action detection. In: Proceedings of the 25th ACM Inter-\\nnational Conference on Multimedia, pp 988–996\\nTang X, Du DK, He Z, Liu J (2018) Pyramidbox: a context-assisted single shot face detector. In: Proceedings \\nof the European Conference on Computer Vision (ECCV), pp 797–813\\nLi Z, Yang L, Zhou F (2017) FSSD: feature fusion single shot multibox detector. Preprint at arXiv:1712.00960\\nJiang P, Ergu D, Liu F, Cai Y , Ma B (2022) A review of YOLO algorithm developments. Proc Comput Sci \\n199:1066–1073\\nTerven J, Córdova-Esparza D-M, Romero-González J-A (2023) A comprehensive review of YOLO archi -\\ntectures in computer vision: from YOLOv1 to YOLOv8 and YOLO-NAS. Mach Learn Knowl Extract \\n5(4):1680–1716\\nHussain M (2024) YOLOv1 to v8: Unveiling each variant-a comprehensive review of YOLO. IEEE Access \\n12:42816–42833\\nHussain M (2023) YOLO-v1 to YOLO-v8, the rise of YOLO and its complementary nature toward digital \\nmanufacturing and industrial defect detection. Machines 11(7):677\\nWang C-Y , Liao H-YM, et al (2024) YOLOv1 to YOLOv10: The fastest and most accurate real-time object \\ndetection systems. APSIPA Transact Signal Inform Process 13(1)\\nJegham N, Koh CY , Abdelatti M, Hendawi A (2024) Evaluating the evolution of YOLO (you only look \\nonce) models: A comprehensive benchmark study of YOLO11 and its predecessors. Preprint at \\narXiv:2411.00201\\nWang C-Y , Liao H-YM, Wu Y-H, Chen P-Y , Hsieh J-W, Yeh I-H (2020) CSPNET: A new backbone that can \\nenhance learning capability of CNN. In: Proceedings of the IEEE/CVF Conference on Computer Vision \\nand Pattern Recognition Workshops, pp 390–391\\nWang C-Y , Liao H-YM, Yeh I-H (2022) Designing network design strategies through gradient path analysis. \\nPreprint at arXiv:2211.04800\\nSapkota R, Meng Z, Karkee M (2024) Synthetic meets authentic: leveraging LLM generated datasets for \\nYOLO11 and YOLOv10-based apple detection through machine vision sensors. Smart Agric Technol \\n9:100614\\nUltralytics (2024) YOLO11 NEW. https://docs.ultralytics.com/models/YOLO11. Accessed 31 Dec 2024\\nLiu S, Qi L, Qin H, Shi J, Jia J (2018) Path aggregation network for instance segmentation. In: Proceedings \\nof the IEEE Conference on Computer Vision and Pattern Recognition, pp 8759–8768\\nRothe R, Guillaumin M, Van Gool L (2015) Non-maximum suppression for object detection by passing \\nmessages between windows. In: Computer Vision–ACCV 2014: 12th Asian Conference on Computer \\nVision, Singapore, Singapore, November 1–5, 2014, Revised Selected Papers, Part I 12, pp 290–306. \\nSpringer\\n1 3\\nPage 73 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8bf60f07-d467-4b43-9b26-a2aed078c1fe', embedding=None, metadata={'page_label': '74', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nLi S, Li M, Li R, He C, Zhang L (2023) One-to-few label assignment for end-to-end dense detection. In: \\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 7350–7359\\nTian Y , Deng N, Xu J, Wen Z (2024) A fine-grained dataset for sewage outfalls objective detection in natural \\nenvironments. Sci Data 11(1):724\\nBhagat S, Kokare M, Haswani V , Hambarde P, Kamble R (2021) Wheatnet-lite: A novel light weight network \\nfor wheat head detection. In: Proceedings of the IEEE/CVF International Conference on Computer \\nVision, pp 1332–1341\\nHu Y , Tan W, Meng F, Liang Y (2023) A decoupled spatial-channel inverted bottleneck for image compres-\\nsion. In: 2023 IEEE International Conference on Image Processing (ICIP), pp 1740–1744. IEEE\\nYang G, Wang J, Nie Z, Yang H, Yu S (2023) A lightweight YOLOv8 tomato detection algorithm combining \\nfeature enhancement and attention. Agronomy 13(7):1824\\nLin T-Y , Maire M, Belongie S, Hays J, Perona P, Ramanan D, Dollár P, Zitnick CL (2014) Microsoft coco: \\nCommon objects in context. In: Computer Vision–ECCV 2014: 13th European Conference, Zurich, \\nSwitzerland, September 6-12, 2014, Proceedings, Part V 13, pp 740–755. Springer\\nJocher G, et al (2022) YOLOv8: a comprehensive improvement of the YOLO object detection series.  h t t p s : / \\n/ d o c s . u l t r a l y t i c s . c o m / Y O L O v 8 /     . Accessed 05 Jun 2024\\nTishby N, Zaslavsky N (2015) Deep learning and the information bottleneck principle. In: 2015 IEEE Infor-\\nmation Theory Workshop (ITW), pp. 1–5. IEEE\\nZhang B, Li J, Bai Y , Jiang Q, Yan B, Wang Z (2023) An improved microaneurysm detection model based on \\nSWINIR and YOLOv8. Bioengineering 10(12):1405\\nChien C-T, Ju R-Y , Chou K-Y , Chiang J-S (2024) YOLOv9 for fracture detection in pediatric wrist trauma \\nX-ray images. Preprint at arXiv:2403.11249\\nUltralytics: Home—docs.ultralytics.com. https://docs.ultralytics.com/. Accessed 28 May 2024\\nUltralytics: YOLOv8 object detection model: what is, how to use—roboflow.com.  h t t p s : / / r o b o fl  o w . c o m / m o \\nd e l / Y O L O v 8     . Accessed 28 May 2024\\nUltralytics: ultralytics YOLOv8 solutions: quick walkthrough—ultralytics.medium.com.  h t t p s :  / / u l t  r a l y t i  c s . m  \\ne d i u m  . c o m /  u l t r a l  y t i c  s - Y O L  O v 8 - s  o l u t i o  n s - q  u i c k -  w a l k t  h r o u g h  - b 8 0  2 f d 6 d a 5 d 7. Accessed 28 May 2024\\nDu S, Zhang B, Zhang P, Xiang P (2021) An improved bounding box regression loss function based on CIOU \\nloss for multi-scale object detection. In: 2021 IEEE 2nd International Conference on Pattern Recogni -\\ntion and Machine Learning (PRML), pp 92–98. IEEE\\nXu S, Wang X, Lv W, Chang Q, Cui C, Deng K, Wang G, Dang Q, Wei S, Du Y , et al (2022) PP-YOLOE: An \\nevolved version of YOLO. Preprint at arXiv:2203.16250\\nYue X, Qi K, Na X, Zhang Y , Liu Y , Liu C (2023) Improved YOLOv8-seg network for instance segmentation \\nof healthy and diseased tomato plants in the growth stage. Agriculture 13(8):1643\\nZhu X, Lyu S, Wang X, Zhao Q (2021) TPH-YOLOv5: Improved YOLOv5 based on transformer prediction \\nhead for object detection on drone-captured scenarios. In: Proceedings of the IEEE/CVF International \\nConference on Computer Vision, pp 2778–2788\\nWoo S, Park J, Lee J-Y , Kweon IS (2018) CBAM: Convolutional block attention module. In: Proceedings of \\nthe European Conference on Computer Vision (ECCV), pp 3–19\\nBai Z, Pei X, Qiao Z, Wu G, Bai Y (2024) Improved YOLOv7 target detection algorithm based on UA V aerial \\nphotography. Drones 8(3):104\\nSirisha U, Praveen SP, Srinivasu PN, Barsocchi P, Bhoi AK (2023) Statistical analysis of design aspects of \\nvarious YOLO-based deep learning models for object detection. Int J Comput Intell Syst 16(1):126\\nWang K, Liew JH, Zou Y , Zhou D, Feng J (2019) PANET: Few-shot image semantic segmentation with \\nprototype alignment. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, \\npp 9197–9206\\nZhang Z, Lu X, Cao G, Yang Y , Jiao L, Liu F (2021) VIT-YOLO: Transformer-based YOLO for object detec-\\ntion. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp 2799–2808\\nUltralytics: Comprehensive Guide to Ultralytics YOLOv5—docs.ultralytics.com.  h t t p s : / / d o c s . u l t r a l y t i c s . c o \\nm / Y O L O v 5 /     . Accessed 28 May 2024\\nUltralytics: GitHub-ultralytics/YOLOv5: YOLOv5 in PyTorch [CDATA[>]] > ONNX >[CDATA[>]] \\nCoreML >[CDATA[>]] TFLite—github.com. https://github.com/ultralytics/YOLOv5. Accessed 28 \\nMay 2024\\nBochkovskiy A, Wang C-Y , Liao H-YM (2020) YOLOv4: Optimal speed and accuracy of object detection. \\nPreprint at arXiv:2004.10934\\nMahasin M, Dewi IA (2022) Comparison of CSPDarkNet53, CSPResNeXt-50, and EfficientNet-B0 back -\\nbones on YOLO V4 as object detector. Int J Eng Sci Inform Technol 2(3):64–72\\nRedmon J, Farhadi A (2018) YOLOv3: An incremental improvement. Preprint at arXiv:1804.02767\\nMisra D (2019) Mish: A self regularized non-monotonic activation function. Preprint at arXiv:1908.08681\\n1 3\\n  274  Page 74 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c744c30c-bb07-463b-b561-3d98918cf3cf', embedding=None, metadata={'page_label': '75', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nYun S, Han D, Oh SJ, Chun S, Choe J, Yoo Y (2019) Cutmix: Regularization strategy to train strong classi-\\nfiers with localizable features. In: Proceedings of the IEEE/CVF International Conference on Computer \\nVision, pp 6023–6032\\nGhiasi G, Lin T-Y , Le QV (2018) Dropblock: a regularization method for convolutional networks. Adv Neu-\\nral Inform Process Syst 31\\nMüller R, Kornblith S, Hinton GE (2019) When does label smoothing help? Adv Neural Inform Process \\nSyst 32\\nZhang Z, He T, Zhang H, Zhang Z, Xie J, Li M (2019) Bag of freebies for training object detection neural \\nnetworks. Preprint at arXiv:1902.04103\\nRedmon J, Farhadi A (2017) YOLO9000: better, faster, stronger. In: Proceedings of the IEEE Conference on \\nComputer Vision and Pattern Recognition, pp 7263–7271\\nRedmon J, Divvala S, Girshick R, Farhadi A (2016) You only look once: unified, real-time object detection. \\nPreprint at arXiv:1506.02640. Accessed 05 Jun 2024\\nRen P, Xiao Y , Chang X, Huang P-Y , Li Z, Chen X, Wang X (2021) A comprehensive survey of neural archi-\\ntecture search: challenges and solutions. ACM Comput Surv (CSUR) 54(4):1–34\\nMithun M, Jawhar SJ (2024) Detection and classification on MRI images of brain tumor using YOLO NAS \\ndeep learning model. J Radiat Res Appl Sci 17(4):101113\\nGe Z (2021) YOLOx: Exceeding YOLO series in 2021. Preprint at arXiv:2107.08430\\nZhang Y , Zhang W, Yu J, He L, Chen J, He Y (2022) Complete and accurate holly fruits counting using \\nYOLOx object detection. Comput Electron Agric 198:107062\\nLiu J, Sun W (2022) YOLOx-based ship target detection for shore-based monitoring. In: Proceedings of the \\n2022 5th International Conference on Signal Processing and Machine Learning, pp 234–241\\nAshraf I, Hur S, Kim G, Park Y (2024) Analyzing performance of YOLOx for detecting vehicles in bad \\nweather conditions. Sensors 24(2):522\\nChang H-S, Wang C-Y , Wang RR, Chou G, Liao H-YM (2023) YOLOr-based multi-task learning. Preprint \\nat arXiv:2309.16921\\nAndrei-Alexandru T, Cosmin C, Ioan S, Adrian-Alexandru T, Henrietta DE (2022) Novel ceramic plate \\ndefect detection using YOLO-r. In: 2022 14th International Conference on Electronics, Computers and \\nArtificial Intelligence (ECAI), pp 1–6. IEEE\\nSun H, Lu D, Li X, Tan J, Zhao J, Hou D (2024) Research on multi-apparent defects detection of concrete \\nbridges based on YOLOr. Structures 65:106735\\nXu X, Jiang Y , Chen W, Huang Y , Zhang Y , Sun X (2022) Damo-YOLO: a report on real-time object detec-\\ntion design. Preprint at arXiv:2211.15444\\nWang C, He W, Nie Y , Guo J, Liu C, Wang Y , Han K (2024) Gold-YOLO: Efficient object detector via gather-\\nand-distribute mechanism. Adv Neural Inform Process Syst 36\\nVijayakumar A, Vairavasundaram S (2024) YOLO-based object detection models: a review and its applica-\\ntions. Multim Tools Appl 1–40\\nAlibabaei K, Gaspar PD, Lima TM, Campos RM, Girão I, Monteiro J, Lopes CM (2022) A review of the \\nchallenges of using deep learning algorithms to support decision-making in agricultural activities. \\nRemote Sens 14(3):638\\nWang C, Liu B, Liu L, Zhu Y , Hou J, Liu P, Li X (2021) A review of deep learning used in the hyperspectral \\nimage analysis for agriculture. Artif Intell Rev 54(7):5205–5253\\nBenjumea A, Teeti I, Cuzzolin F, Bradley A (2021) YOLO-z: Improving small object detection in YOLOv5 \\nfor autonomous vehicles. Preprint at arXiv:2112.11798\\nSarda A, Dixit S, Bhan A (2021) Object detection for autonomous driving using YOLO [you only look once] \\nalgorithm. In: 2021 Third International Conference on Intelligent Communication Technologies and \\nVirtual Mobile Networks (ICICV), pp 1370–1374. IEEE\\nCai Y , Luan T, Gao H, Wang H, Chen L, Li Y , Sotelo MA, Li Z (2021) YOLOv4-5D: An effective and effi-\\ncient object detector for autonomous driving. IEEE Trans Instrum Meas 70:1–13\\nZhao J, Hao S, Dai C, Zhang H, Zhao L, Ji Z, Ganchev I (2022) Improved vision-based vehicle detection and \\nclassification by optimized YOLOv4. IEEE Access 10:8590–8603\\nWoo J, Baek J-H, Jo S-H, Kim SY , Jeong J-H (2022) A study on object detection performance of YOLOv4 \\nfor autonomous driving of tram. Sensors 22(22):9026\\nYe C, Wang Y , Wang Y , Tie M (2022) Steering angle prediction YOLOv5-based end-to-end adaptive neural \\nnetwork control for autonomous vehicles. Proc Institut Mech Eng Part D 236(9):1991–2011\\nJia X, Tong Y , Qiao H, Li M, Tong J, Liang B (2023) Fast and accurate object detector for autonomous driv-\\ning based on improved YOLOv5. Sci Rep 13(1):9711\\nChen Z, Wang X, Zhang W, Yao G, Li D, Zeng L (2023) Autonomous parking space detection for electric \\nvehicles based on improved YOLOv5-OBB algorithm. World Electric Vehicle J 14(10):276\\nLiu X, Yan WQ (2022) Vehicle-related distance estimation using customized YOLOv7. In: International \\nConference on Image and Vision Computing New Zealand, pp 91–103. Springer\\n1 3\\nPage 75 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d6a340d8-e77a-4de7-be40-9457064a34d8', embedding=None, metadata={'page_label': '76', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nMehla N, Ishita Talukdar R, Sharma DK (2023) Object detection in autonomous maritime vehicles: Com -\\nparison between YOLO v8 and efficientdet. In: International Conference on Data Science and Network \\nEngineering, pp 125–141. Springer\\nKumar D, Muhammad N (2023) Object detection in adverse weather for autonomous driving through data \\nmerging and YOLOv8. Sensors 23(20):8471\\nOh G, Lim S (2023) One-stage brake light status detection based on YOLOv8. Sensors 23(17):7436\\nAfdhal A, Saddami K, Sugiarto S, Fuadi Z, Nasaruddin N (2023) Real-time object detection performance \\nof YOLOv8 models for self-driving cars in a mixed traffic environment. In: 2023 2nd International \\nConference on Computer System, Information Technology, and Electrical Engineering (COSITE), pp. \\n260–265. IEEE\\nWang H, Liu C, Cai Y , Chen L, Li Y (2024) YOLOv8-QSD: An improved small object detection algorithm \\nfor autonomous vehicles based on YOLOv8. IEEE Transactions on Instrumentation and Measurement\\nBakirci M, Dmytrovych P, Bayraktar I, Anatoliyovych O (2024) Multi-class vehicle detection and classifi -\\ncation with YOLO11 on uav-captured aerial imagery. In: 2024 IEEE 7th International Conference on \\nActual Problems of Unmanned Aerial Vehicles Development (APUA VD), pp 191–196. IEEE\\nLi Y , Leong W, Zhang H (2024) YOLOv10-based real-time pedestrian detection for autonomous vehicles. \\nIn: 2024 IEEE 8th International Conference on Signal and Image Processing Applications (ICSIPA), \\npp 1–6. IEEE\\nArifando R, Eto S, Tibyani T, Wada C (2025) Improved YOLOv10 for visually impaired: balancing model \\naccuracy and efficiency in the case of public transportation. Informatics 12:7\\nWibowo A, Trilaksono BR, Hidayat EMI, Munir R (2023) Object detection in dense and mixed traffic for \\nautonomous vehicles with modified YOLO. IEEE Access 11:134866–134877\\nHung WCW, Zakaria MA, Ishak M, Heerwan P (2022) Object tracking for autonomous vehicle using YOLO \\nv3. In: Enabling Industry 4.0 Through Advances in Mechatronics: Selected Articles from iM3F 2021, \\nMalaysia, pp 265–273. Springer\\nZaghari N, Fathy M, Jameii SM, Shahverdy M (2021) The improvement in obstacle detection in autonomous \\nvehicles using YOLO non-maximum suppression fuzzy algorithm. J Supercomput 77(11):13421–13446\\nAli Y , Haque MM, Mannering F (2023) A Bayesian generalised extreme value model to estimate real-time \\npedestrian crash risks at signalised intersections using artificial intelligence-based video analytics. Anal \\nMethods Accident Res 38:100264\\nHussain F, Ali Y , Li Y , Haque MM (2024) Revisiting the hybrid approach of anomaly detection and extreme \\nvalue theory for estimating pedestrian crashes using traffic conflicts obtained from artificial intelli -\\ngence-based video analytics. Accident Analysis & Prevention 199:107517\\nGhaziamin P, Bajaj K, Bouguila N, Patterson Z (2024) A privacy-preserving edge computing solution for \\nreal-time passenger counting at bus stops using overhead fisheye camera. In: 2024 IEEE 18th Interna -\\ntional Conference on Semantic Computing (ICSC), pp 25–32. IEEE\\nZhang S, Abdel-Aty M, Yuan J, Li P (2020) Prediction of pedestrian crossing intentions at intersections based \\non long short-term memory recurrent neural network. Transp Res Rec 2674(4):57–65\\nYang HF, Ling Y , Kopca C, Ricord S, Wang Y (2022) Cooperative traffic signal assistance system for non-\\nmotorized users and disabilities empowered by computer vision and edge artificial intelligence. Trans \\nRes Part C: Emerg Technol 145:103896\\nJiao D, Fei T (2023) Pedestrian walking speed monitoring at street scale by an in-flight drone. Peer J Comput \\nSci 9:1226\\nWang Y , Jia Y , Chen W, Wang T, Zhang A (2024) Examining safe spaces for pedestrians and e-bicyclists at \\nurban crosswalks: an analysis based on drone-captured video. Accident Anal Prevent 194:107365\\nZhou W, Liu Y , Zhao L, Xu S, Wang C (2023) Pedestrian crossing intention prediction from surveillance \\nvideos for over-the-horizon safety warning. IEEE Transactions on Intelligent Transportation Systems\\nXiao X, Feng X (2023) Multi-object pedestrian tracking using improved YOLOv8 and oc-sort. Sensors, \\n23(20) https://doi.org/10.3390/s23208439\\nLi S, Wang S, Wang P (2023) A small object detection algorithm for traffic signs based on improved YOLOv7. \\nSensors. https://doi.org/10.3390/s23167145\\nMahaur B, Mishra KK (2023) Small-object detection based on YOLOv5 in autonomous driving systems. \\nPattern Recogn Lett 168:115–122. https://doi.org/10.1016/j.patrec.2023.03.009\\nZhang H, Qin L, Li J, Guo Y , Zhou Y , Zhang J, Xu Z (2020) Real-time detection method for small traffic signs \\nbased on YOLOv3. IEEE Access 8:64145–64156. https://doi.org/10.1109/ACCESS.2020.2984554\\nLi M, Zhang L, Li L, W S, (2022) YOLO-based traffic sign recognition algorithm. Comput Intell Neurosci. \\nhttps://doi.org/10.1155/2022/2682921\\nZhang J, Huang M, Jin X, Li X (2017) A real-time Chinese traffic sign detection algorithm based on modified \\nYOLOv2. Algorithms https://doi.org/10.3390/a10040127\\nBai W, Zhao J, Dai C, Zhang H, Zhao L, Ji Z, Ganchev I (2023) Two novel models for traffic sign detection \\nbased on YOLOv5s. Axioms. https://doi.org/10.3390/axioms12020160\\n1 3\\n  274  Page 76 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ac4d0804-1183-4522-b778-15842db99c96', embedding=None, metadata={'page_label': '77', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nSoylu E, Soylu T (2024) A performance comparison of YOLOv8 models for traffic sign detection in the \\nRobotAXI-full scale autonomous vehicle competition. Multim Tools Appl 83(8):25005–25035\\nZhang M (2024) Research on traffic sign detection based on improved YOLOv9. In: Kolivand, H., Moshayedi, \\nA.J. (eds.) Fourth International Conference on Computer Graphics, Image, and Virtualization (ICCGIV \\n2024), vol 13288, p 132880. SPIE. https://doi.org/10.1117/12.3045722. International Society for Optics \\nand Photonics\\nKaraca H, Atasoy NA (2025) Fine-grained classification of military aircraft using pre-trained deep learning \\nmodels and YOLO11. Curr Trends Comput 2(2):150–171\\nKaur R, Singh J (2022) Local regression based real-time traffic sign detection using YOLOv6. In: 2022 \\n4th International Conference on Advances in Computing, Communication Control and Networking \\n(ICAC3N), pp 522–526. IEEE\\nDewi C, Chen R-C, Jiang X, Yu H (2022) Deep convolutional neural network for enhancing traffic sign rec-\\nognition developed on YOLO v4. Multim Tools Appl 81(26):37821–37845\\nPandey S, Chen K-F, Dam EB (2023) Comprehensive multimodal segmentation in medical imaging: Com -\\nbining YOLOv8 with SAM and HQ-SAM models. In: Proceedings of the IEEE/CVF International Con-\\nference on Computer Vision, pp 2592–2598\\nJu R-Y , Cai W (2023) Fracture detection in pediatric wrist trauma x-ray images using YOLOv8 algorithm. \\nSci Rep 13(1):20077\\nInui A, Mifune Y , Nishimoto H, Mukohara S, Fukuda S, Kato T, Furukawa T, Tanaka S, Kusunose M, Taki-\\ngami S et al (2023) Detection of elbow OCD in the ultrasound image by artificial intelligence using \\nYOLOv8. Appl Sci 13(13):7623\\nWu B, Pang C, Zeng X, Hu X (2022) ME-YOLO: Improved YOLOv5 for detecting medical personal protec-\\ntive equipment. Appl Sci 12(23):11978\\nZhao X, Wang Q, Zhang M, Wei Z, Ku R, Zhang Z, Yu Y , Zhang B, Liu Y , Wang C (2024) CSFF-YOLOv5: \\nImproved YOLOv5 based on channel split and feature fusion in femoral neck fracture detection. Inter-\\nnet Things 26:101190\\nGoel L, Patel P (2024) Improving YOLOv6 using advanced PSO optimizer for weight selection in lung can-\\ncer detection and classification. Multim Tools Appl 1–34\\nNorkobil Saydirasulovich S, Abdusalomov A, Jamil MK, Nasimov R, Kozhamzharova D, Cho Y-I (2023) \\nA YOLOv6-based improved fire detection approach for smart city environments. Sensors 23(6):3161\\nZou J, Arshad MR (2024) Detection of whole body bone fractures based on improved YOLOv7. Biomed \\nSignal Process Control 91:105995\\nRazaghi M, Komleh HE, Dehghani F, Shahidi Z (2024) Innovative diagnosis of dental diseases using YOLO \\nv8 deep learning model. In: 2024 13th Iranian/3rd International Machine Vision and Image Processing \\nConference (MVIP), pp 1–5. IEEE\\nPham T-L, Le V-H (2024) Ovarian tumors detection and classification from ultrasound images based on \\nYOLOv8. J Adv Inform Technol 15(2)\\nKrishnamurthy V , Balasubramanian S, Kanmani RS, Srividhya S, Deepika J, Nimeshika GN (2023) Endo -\\nscopic surgical operation and object detection using custom architecture models. In: International Con-\\nference on Human-Centric Smart Computing, pp 637–654. Springer\\nPalanivel N, Deivanai S, Sindhuja B, et al (2023) The art of YOLOv8 algorithm in cancer diagnosis using \\nmedical imaging. In: 2023 International Conference on System, Computation, Automation and Net -\\nworking (ICSCAN), pp 1–6. IEEE\\nKaraköse M, Yetış H, Çeçen M (2024) A new approach for effective medical deepfake detection in medical \\nimages. IEEE Access\\nBhojane R, Chourasia S, Laddha SV , Ochawar RS (2023) Liver lesion detection from mr t1 in-phase and \\nout-phase fused images and CT images using YOLOv8. In: International Conference on Data Science \\nand Applications, pp 121–135. Springer\\nAkdoğan S, Öziç MÜ, Tassoker M (2025) Development of an ai-supported clinical tool for assessing man -\\ndibular third molar tooth extraction difficulty using panoramic radiographs and YOLO11 sub-models. \\nDiagnostics 15(4):462\\nAli BS, Nasir H, Khan A, Ashraf M, Akbar SM (2024) A machine learning-based model for the detection of \\nskin cancer using YOLOv10. In: 2024 IEEE 8th International Conference on Signal and Image Process-\\ning Applications (ICSIPA), pp 1–6. IEEE\\nJulia R, Prince S, Bini D (2024) Medical image analysis of masses in mammography using deep learning \\nmodel for early diagnosis of cancer tissues. In: Computational Intelligence and Modelling Techniques \\nfor Disease Detection in Mammogram Images, pp 75–89. Elsevier\\nSalahin SS, Ullaa MS, Ahmed S, Mohammed N, Farook TH, Dudley J (2023) One-stage methods of com -\\nputer vision object detection to classify carious lesions from smartphone imaging. Oral 3(2):176–190\\nDoniyorjon M, Madinakhon R, Shakhnoza M, Cho Y-I (2022) An improved method of polyp detection using \\ncustom YOLOv4-tiny. Appl Sci 12(21):10856\\n1 3\\nPage 77 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ac36ca9a-a75d-44cb-bd58-afdf6240c2c5', embedding=None, metadata={'page_label': '78', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nDing B, Zhang Z, Liang Y , Wang W, Hao S, Meng Z, Guan L, Hu Y , Guo B, Zhao R, et al (2021) Detection \\nof dental caries in oral photographs taken by mobile phones based on the YOLOv3 algorithm. Ann \\nTransl Med 9(21)\\nWang L, Yang S, Yang S, Zhao C, Tian G, Gao Y , Chen Y , Lu Y (2019) Automatic thyroid nodule recognition \\nand diagnosis in ultrasound imaging with the YOLOv2 neural network. World J Surg Oncol 17:1–9\\nKwaśniewska A, Rumiński J, Czuszyński K, Szankin M (2018) Real-time facial features detection from low \\nresolution thermal images with deep classification models. J Med Imaging Health Inform 8(5):979–987\\nMajeed F, Khan FZ, Nazir M, Iqbal Z, Alhaisoni M, Tariq U, Khan MA, Kadry S (2022) Investigating the \\nefficiency of deep learning based security system in a real-time environment using YOLOv5. Sustain \\nEnergy Technol Assess 53:102603\\nAboah A, Shoman M, Mandal V , Davami S, Adu-Gyamfi Y , Sharma A (2021) A vision-based system for \\ntraffic anomaly detection using deep learning and decision trees. In: 2021 IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition Workshops (CVPRW), pp 4202–4207.  h t t p s :  / / d o i  . o r g / 1  0 . 1 1  \\n0 9 / C V  P R W 5 3  0 9 8 . 2 0  2 1 . 0  0 4 7 5\\nAFFES N, KTARI J, BEN AMOR N, FRIKHA T, HAMAM H (2023) Comparison of YOLOV5, YOLOV6, \\nYOLOV7 and YOLOV8 for intelligent video surveillance. J Inform Assur Secur 18(5)\\nCao F, Ma S (2023) Enhanced campus security target detection using a refined YOLOv7 approach. Traite -\\nment du Signal 40(5)\\nChatterjee N, Singh A V , Agarwal R (2024) You only look once (YOLOv8) based intrusion detection system \\nfor physical security and surveillance. In: 2024 11th International Conference on Reliability, Infocom \\nTechnologies and Optimization (Trends and Future Directions)(ICRITO), pp 1–5. IEEE\\nSandhya, Kashyap A (2024) Real-time object-removal tampering localization in surveillance videos by \\nemploying YOLO-v8. J For Sci\\nTran DQ, Aboah A, Jeon Y , Shoman M, Park M, Park S (2024) Low-light image enhancement framework \\nfor improved object detection in fisheye lens datasets. In: Proceedings of the IEEE/CVF Conference on \\nComputer Vision and Pattern Recognition (CVPR) Workshops, pp 7056–7065\\nBakirci M, Bayraktar I (2024) Boosting aircraft monitoring and security through ground surveillance opti -\\nmization with YOLOv9. In: 2024 12th International Symposium on Digital Forensics and Security \\n(ISDFS), pp 1–6. IEEE\\nBakirci M, Bayraktar I (2024) YOLOv9-enabled vehicle detection for urban security and forensics applica -\\ntions. In: 2024 12th International Symposium on Digital Forensics and Security (ISDFS), pp 1–6. IEEE\\nChakraborty S, Zahir S, Orchi NT, Hafiz MFB, Shamsuddoha A, Dipto SM (2024) Violence detection: A \\nmulti-model approach towards automated video surveillance and public safety. In: 2024 International \\nConference on Advances in Computing, Communication, Electrical, and Smart Systems (iCACCESS), \\npp 1–6. IEEE\\nChen G, Du W, Xu T, Wang S, Qi X, Wang Y (2024) Investigating enhanced YOLOv8 model applications \\nfor large-scale security risk management and drone-based low-altitude law enforcement. Highlights Sci \\nEng Technol 98:390–396\\nPashayev F, Babayeva L, Isgandarova Z, Kalejahi BK (2023) Face recognition in smart cameras by YOLO8. \\nKhazar J Sci Technol (KJSAT) 67\\nKaç SB, Eken S, Balta DD, Balta M, İskefiyeli M, Özçelik İ (2024) Image-based security techniques for \\nwater critical infrastructure surveillance. Appl Soft Comput 111730\\nGao Q, Deng H, Zhang G (2024) A contraband detection scheme in X-ray security images based on improved \\nYOLOv8s network model. Sensors 24(4):1158\\nAntony JC, Chowdary CLS, Murali E, Mayan A, et al (2024) Advancing crowd management through innova-\\ntive surveillance using YOLOv8 and bytetrack. In: 2024 International Conference on Wireless Com -\\nmunications Signal Processing and Networking (WiSPNET), pp 1–6. IEEE\\nZhang D (2024) A YOLO-based approach for fire and smoke detection in IoT surveillance systems. Int J Adv \\nComput Sci Appl 15(1)\\nKhin PP, Htaik NM (2024) Gun detection: A comparative study of retinanet, efficientdet and YOLOv8 on \\ncustom dataset. In: 2024 IEEE Conference on Computer Applications (ICCA), pp 1–7. IEEE\\nNkuzo L, Sibiya M, Markus ED (2023) A comprehensive analysis of real-time car safety belt detection using \\nthe YOLOv7 algorithm. Algorithms 16(9):400\\nChang R, Zhang B, Zhu Q, Zhao S, Yan K, Yang Y , et al (2023) FFA-YOLOv7: Improved YOLOv7 based \\non feature fusion and attention mechanism for wearing violation detection in substation construction \\nsafety. J Electrical Comput Eng 2023\\nBakirci M, Bayraktar I (2024) Assessment of YOLO11 for ship detection in sar imagery under open ocean \\nand coastal challenges. In: 2024 21st International Conference on Electrical Engineering, Computing \\nScience and Automatic Control (CCE), pp 1–6. IEEE\\n1 3\\n  274  Page 78 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='24cd1dd3-e8f8-4b0b-a392-4928388edb26', embedding=None, metadata={'page_label': '79', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nŽigulić N, Glučina M, Frank D, Lorencin I, Šverko Z, Matika D (2024) Deploying YOLOv10 for affordable \\nreal-time handgun detection. In: 2024 IEEE 22nd Jubilee International Symposium on Intelligent Sys -\\ntems and Informatics (SISY), pp 000283–000288. IEEE\\nHan L, Ma C, Liu Y , Jia J, Sun J (2023) SC-YOLOv8: a security check model for the inspection of prohibited \\nitems in X-ray images. Electronics 12(20):4208\\nYuan J, Zhang N, Xie Y , Gao X (2022) Detection of prohibited items based upon x-ray images and improved \\nYOLOv7. In: Journal of Physics: Conference Series, vol. 2390. Guangzhou, China, p. 012114.  h t t p s :  / / \\nd o i  . o r g / 1  0 . 1 0  8 8 / 1 7  4 2 - 6 5  9 6 / 2 3 9  0 / 1 /  0 1 2 1 1 4. 3rd International Conference on Advanced Materials and \\nIntelligent Manufacturing (ICAMIM 2022).  h t t p s :   /  / i o p s c i e n c  e . i  o  p . o   r g / a r t  i c  l e   / 1 0 .  1  0 8 8 /  1 7 4 2 -  6 5 9 6  / 2  3 9 \\n0 / 1 / 0 1 2 1 1 4\\nAwang S, Rokei MQR, Sulaiman J (2023) Suspicious activity trigger system using YOLOv6 convolutional \\nneural network. In: 2023 International Conference on Artificial Intelligence in Information and Com -\\nmunication (ICAIIC), pp 527–532. IEEE\\nXiao Y , Chang A, Wang Y , Huang Y , Yu J, Huo L (2022) Real-time object detection for substation security \\nearly-warning with deep neural network based on YOLO-v5. In: 2022 IEEE IAS Global Conference on \\nEmerging Technologies (GlobConET), pp 45–50. IEEE\\nWang G, Ding H, Duan M, Pu Y , Yang Z, Li H (2023) Fighting against terrorism: a real-time CCTV autono-\\nmous weapons detection based on improved YOLO v4. Digit Signal Process 132:103790\\nKashika P, Venkatapur RB (2022) Automatic tracking of objects using improvised YOLOv3 algorithm and \\nalarm human activities in case of anomalies. Int J Inf Technol 14(6):2885–2891\\nMohandoss T, Rangaraj J (2024) Multi-object detection using enhanced YOLOv2 and Lunet algorithms in \\nsurveillance videos. e-Prime-Adv Electrical Eng Electron Energy 8:100535\\nSmiley CJ (2015) From silence to propagation: understanding the relationship between “stop snitchin’’ and \\n“YOLO’’. Deviant Behav 36(1):1–16\\nPendse R, Rajput H, Saraf S, Sarwate A, Jadhav J et al (2023) Defect detection in manufacturing using \\nYOLOv7. IJRAR-Int J Res Anal Rev (IJRAR) 10(2):179–185\\nYi F, Zhang H, Yang J, He L, Mohamed ASA, Gao S (2024) YOLOv7-SIAMFF: Industrial defect detection \\nalgorithm based on improved YOLOv7. Comput Electr Eng 114:109090\\nWang H, Xu X, Liu Y , Lu D, Liang B, Tang Y (2023) Real-time defect detection for metal components: a \\nfusion of enhanced Canny–Devernay and YOLOv6 algorithms. Appl Sci 13(12):6898\\nLudwika AS, Rifai AP (2024) Deep learning for detection of proper utilization and adequacy of personal \\nprotective equipment in manufacturing teaching laboratories. Safety 10(1):26\\nBeak S, Han Y-H, Moon Y , Lee J, Jeong J (2023) YOLOv7-based anomaly detection using intensity and ng \\ntypes in labeling in cosmetic manufacturing processes. Processes 11(8):2266\\nZhao H, Wang X, Sun J, Wang Y , Chen Z, Wang J, Xu X (2024) Artificial intelligence powered real-time \\nquality monitoring for additive manufacturing in construction. Constr Build Mater 429:135894\\nLiu Z, Ye K (2023) YOLO-IMF: an improved YOLOv8 algorithm for surface defect detection in industrial \\nmanufacturing field. In: International Conference on Metaverse, pp 15–28. Springer\\nWen Y , Wang L (2024) YOLO-SD: simulated feature fusion for few-shot industrial defect detection based on \\nYOLOv8 and stable diffusion. Int J Mach Learn Cybernet 1–13\\nKarna N, Putra MAP, Rachmawati SM, Abisado M, Sampedro GA (2023) Towards accurate fused deposition \\nmodeling 3d printer fault detection using improved YOLOv8 with hyperparameter optimization. IEEE \\nAccess\\nLi W, Solihin MI, Nugroho HA (2024) Rca: YOLOv8-based surface defects detection on the inner wall of \\ncylindrical high-precision parts. Arab J Sci Eng 1–19\\nHu Y , Wang J, Wang X, Sun Y , Yu H, Zhang J (2024) Real-time evaluation of the blending uniformity of \\nindustrially produced gravelly soil based on Cond-YOLOv8-seg. J Ind Inf Integr 39:100603\\nYang S, Zhang Z, Wang B, Wu J (2024) Dcs-YOLOv8: An improved steel surface defect detection algorithm \\nbased on YOLOv8. In: Proceedings of the 2024 7th International Conference on Image and Graphics \\nProcessing, pp 39–46\\nWang X, Gao H, Jia Z, Li Z (2023) BL-YOLOv8: An improved road defect detection model based on \\nYOLOv8. Sensors 23(20):8361\\nLuo B, Kou Z, Han C, Wu J (2023) A “hardware-friendly’’ foreign object identification method for belt con-\\nveyors based on improved YOLOv8. Appl Sci 13(20):11464\\nWu Q, Kuang X, Tang X, Guo D, Luo Z (2023) Industrial equipment object detection based on improved \\nYOLOv7. In: International Conference on Computer, Artificial Intelligence, and Control Engineering \\n(CAICE 2023), vol 12645, pp 600–608. SPIE\\nKim O, Han Y , Jeong J (2022) Real-time inspection system based on Moire pattern and YOLOv7 for coated \\nhigh-reflective injection molding product. WSEAS Trans Comput Res 10:120–125\\nChen J, Bai S, Wan G, Li Y (2023) Research on YOLOv7-based defect detection method for automotive run-\\nning lights. Syst Sci Control Eng 11(1):2185916\\n1 3\\nPage 79 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4f644c97-2991-41d9-8bee-5e683c400508', embedding=None, metadata={'page_label': '80', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='R. Sapkota et al.\\nHussain M, Al-Aqrabi H, Munawar M, Hill R, Alsboui T (2022) Domain feature mapping with YOLOv7 for \\nautomated edge-based pallet racking inspections. Sensors 22(18):6927\\nZhu B, Xiao G, Zhang Y , Gao H (2023) Multi-classification recognition and quantitative characterization of \\nsurface defects in belt grinding based on YOLOv7. Measurement 216:112937\\nBanduka N, Tomić K, Živadinović J, Mladineo M (2024) Automated dual-side leather defect detection and \\nclassification using YOLOv11: a case study in the finished leather industry. Processes 12(12):2892\\nLiao L, Song C, Wu S, Fu J (2025) A novel YOLOv10-based algorithm for accurate steel surface defect \\ndetection. Sensors 25(3):769\\nHuang G, Huang Y , Li H, Guan Z, Li X, Zhang G, Li W, Zheng X (2024) An improved YOLOv9 and its \\napplications for detecting flexible circuit boards connectors. Int J Comput Intell Syst 17(1):261\\nGupta C, Gill NS, Gulia P, Chatterjee JM (2023) A novel finetuned YOLOv6 transfer learning model for real-\\ntime object detection. J Real-Time Image Proc 20(3):42\\nZendehdel N, Chen H, Leu MC (2023) Real-time tool detection in smart manufacturing using you-only-look-\\nonce (YOLO) v5. Manuf Lett 35:1052–1059\\nJiang D, Wang H, Lu Y (2024) An efficient automobile assembly state monitoring system based on channel-\\npruned YOLOv4 algorithm. Int J Comput Integr Manuf 37(3):372–382\\nYan J, Wang Z (2022) YOLO v3+ vgg16-based automatic operations monitoring and analysis in a manufac-\\nturing workshop under industry 4.0. J Manuf Syst 63:134–142\\nArima K, Nagata F, Shimizu T, Otsuka A, Kato H, Watanabe K, Habib MK (2023) Improvements of detection \\naccuracy and its confidence of defective areas by YOLOv2 using a data set augmentation method. Artif \\nLife Robot 28(3):625–631\\nBadgujar CM, Armstrong PR, Gerken AR, Pordesimo LO, Campbell JF (2023) Real-time stored product \\ninsect detection and identification using deep learning: System integration and extensibility to mobile \\nplatforms. J Stored Products Res 104:102196. https://doi.org/10.1016/j.jspr.2023.102196. Accessed 05 \\nJun 2024\\nShoman M, Aboah A, Morehead A, Duan Y , Daud A, Adu-Gyamfi Y (2022) A region-based deep learning \\napproach to automated retail checkout. In: Proceedings of the IEEE/CVF Conference on Computer \\nVision and Pattern Recognition (CVPR) Workshops, pp 3210–3215\\nBist RB, Subedi S, Yang X, Chai L (2023) A novel YOLOv6 object detector for monitoring piling behavior \\nof cage-free laying hens. AgriEngineering 5(2):905–923\\nKumar P, Kumar N (2023) Drone-based apple detection: Finding the depth of apples using YOLOv7 archi -\\ntecture with multi-head attention mechanism. Smart Agric Technol 5:100311\\nZhang L, Ding G, Li C, Li D (2023) DCF-YOLOv8: An improved algorithm for aggregating low-level fea -\\ntures to detect agricultural pests and diseases. Agronomy 13(8):2012\\nSharma A, Kumar V , Longchamps L (2024) Comparative performance of YOLOv8, YOLOv9, YOLOv10, \\nYOLOv11 and Faster R-CNN models for detection of multiple weed species. Smart Agric Technol. \\nhttps://doi.org/10.1016/j.atech.2024.100648\\nJunior LCM, Ulson JAC (2021) Real time weed detection using computer vision and deep learning. In: 2021 \\n14th IEEE International Conference on Industry Applications (INDUSCON), pp 1131–1137. IEEE\\nKhalid M, Sarfraz MS, Iqbal U, Aftab MU, Niedbała G, Rauf HT (2023) Real-time plant health detection \\nusing deep convolutional neural networks. Agriculture 13(2):510\\nGallo I, Rehman AU, Dehkordi RH, Landro N, La Grassa R, Boschetti M (2023) Deep object detection of \\ncrop weeds: performance of YOLOv7 on a real case dataset from UA V images. Remote Sens 15(2):539\\nVaidya S, Kavthekar S, Joshi A (2023) Leveraging YOLOv7 for plant disease detection. In: 2023 4th Interna-\\ntional Conference on Innovative Trends in Information Technology (ICITIIT), pp 1–6. IEEE\\nZayani HM, Ammar I, Ghodhbani R, Maqbool A, Saidani T, Slimane JB, Kachoukh A, Kouki M, Kallel M, \\nAlsuwaylimi AA et al (2024) Deep learning for tomato disease detection with YOLOv8. Eng Technol \\nAppl Sci Res 14(2):13584–13591\\nMa B, Hua Z, Wen Y , Deng H, Zhao Y , Pu L, Song H (2024) Using an improved lightweight YOLOv8 model \\nfor real-time detection of multi-stage apple fruit in complex orchard environments. Artif Intell Agric\\nJunos MH, Mohd Khairuddin AS, Thannirmalai S, Dahari M (2021) An optimized YOLO-based object \\ndetection model for crop harvesting system. IET Image Proc 15(9):2112–2125\\nZhao H, Tang Z, Li Z, Dong Y , Si Y , Lu M, Panoutsos G (2024) Real-time object detection and robotic \\nmanipulation for agriculture using a YOLO-based learning approach. Preprint at arXiv:2401.15785\\nChen W, Zhang J, Guo B, Wei Q, Zhu Z (2021) An apple detection method based on des-YOLO v4 algorithm \\nfor harvesting robots in complex environment. Math Probl Eng 2021:1–12\\nNergiz M, (2023) Enhancing strawberry harvesting efficiency through YOLO-v7 object detection assess -\\nment. Turk J Sci Technol 18(2):519–533\\nWang C, Han Q, Li C, Li J, Kong D, Wang F, Zou X (2024) Assisting the planning of harvesting plans for \\nlarge strawberry fields through image-processing method based on deep learning. Agriculture 14(4):560\\n1 3\\n  274  Page 80 of 83', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0809452e-31a0-4ac8-bd4b-bd312749a5b9', embedding=None, metadata={'page_label': '81', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\nChen W, Lu S, Liu B, Chen M, Li G, Qian T (2022) CitrusYOLO: a algorithm for citrus detection under \\norchard environment based on YOLOv4. Multimed Tools Appl 81(22):31363–31389\\nMirhaji H, Soleymani M, Asakereh A, Mehdizadeh SA (2021) Fruit detection and load estimation of an \\norange orchard using the YOLO models through simple approaches in different imaging and illumina -\\ntion conditions. Comput Electron Agric 191:106533\\nSapkota R, Ahmed D, Churuvija M, Karkee M (2024) Immature green apple detection and sizing in commer-\\ncial orchards using YOLOv8 and shape fitting techniques. IEEE Access 12:43436–43452\\nWu D, Lv S, Jiang M, Song H (2020) Using channel pruning-based YOLO v4 deep learning algorithm for \\nthe real-time and accurate detection of apple flowers in natural environments. Comput Electron Agric \\n178:105742\\nWang J, Gao Z, Zhang Y , Zhou J, Wu J, Li P (2021) Real-time detection and location of potted flowers based \\non a zed camera and a YOLO v4-tiny deep learning algorithm. Horticulturae 8(1):21\\nKhanal SR, Sapkota R, Ahmed D, Bhattarai U, Karkee M (2023) Machine vision system for early-stage \\napple flowers and flower clusters detection for precision thinning and pollination. IFAC-PapersOnLine \\n56(2):8914–8919\\nXiao F, Wang H, Xu Y , Zhang R (2023) Fruit detection and recognition based on deep learning for automatic \\nharvesting: an overview and review. Agronomy 13(6):1625\\nYijing W, Yi Y , Xue-fen W, Jian C, Xinyun L (2021) Fig fruit recognition method based on YOLO v4 deep \\nlearning. In: 2021 18th International Conference on Electrical Engineering/Electronics, Computer, \\nTelecommunications and Information Technology (ECTI-CON), pp 303–306. IEEE\\nZhang Y , Li L, Chun C, Wen Y , Xu G (2024) Multi-scale feature adaptive fusion model for real-time detection \\nin complex citrus orchard environments. Comput Electron Agric 219:108836\\nZhou J, Zhang Y , Wang J (2023) A dragon fruit picking detection method based on YOLOv7 and PSP-ellipse. \\nSensors 23(8):3803\\nXiuyan G, Zhang Y (2023) Detection of fruit using YOLOv8-based single stage detectors. Int J Adv Comput \\nSci Appl 14(12)\\nHe Z, Karkee M, Upadhayay P (2021) Detection of strawberries with varying maturity levels for robotic har-\\nvesting using YOLOv4. In: 2021 ASABE Annual International Virtual Meeting, p. 1. American Society \\nof Agricultural and Biological Engineers\\nBoudaa B, Abada K, Aichouche WA, Belakermi AN (2024) Advancing plant diseases detection with pre-\\ntrained YOLO models. In: 2024 6th International Conference on Pattern Analysis and Intelligent Sys -\\ntems (PAIS), pp 1–6. IEEE\\nAndreyanov N, Shleymovich M, Sytnik A (2022) Driver assistance system for agricultural machinery for \\nobstacles detection based on deep neural networks. In: 2022 International Conference on Industrial \\nEngineering, Applications and Manufacturing (ICIEAM), pp 880–885. IEEE\\nJung T-H, Cates B, Choi I-K, Lee S-H, Choi J-M (2020) Multi-camera-based person recognition system for \\nautonomous tractors. Designs 4(4):54\\nXu S, Rai R (2024) Vision-based autonomous navigation stack for tractors operating in peach orchards. \\nComput Electron Agric 217:108558\\nSapkota R, Karkee M (2025) Improved YOLOv12 with LLM-generated synthetic data for enhanced apple \\ndetection and benchmarking against YOLOv11 and YOLOv10. Preprint at arXiv:2503.00057\\nSapkota R, Meng Z, Churuvija M, Du X, Ma Z, Karkee M (2024) Comprehensive performance evaluation \\nof YOLO11, YOLOv10, YOLOv9 and YOLOv8 on detecting and counting fruitlet in complex orchard \\nenvironments. Preprint at arXiv:2407.12040\\nMeng Z, Du X, Sapkota R, Ma Z, Cheng H (2025) YOLOv10-pose and YOLOv9-pose: Real-time strawberry \\nstalk pose detection models. Comput Ind 165:104231\\nSapkota R, Karkee M (2024) Comparing YOLOv11 and YOLOv8 for instance segmentation of occluded and \\nnon-occluded immature green fruits in complex orchard environment. Preprint at arXiv:2410.19869\\nV o H-T, Mui KC, Thien NN, Tien PP (2024) Automating tomato ripeness classification and counting with \\nYOLOv9. Int J Adv Comput Sci Appl 15(4)\\nZhao J, Qu J (2019) A detection method for tomato fruit common physiological diseases based on YOLOv2. \\nIn: 2019 10th International Conference on Information Technology in Medicine and Education (ITME), \\npp 559–563. IEEE\\nDiwan T, Anirudh G, Tembhurne JV (2023) Object detection using YOLO: challenges, architectural succes-\\nsors, datasets and applications. Multimed Tools Appl 82(6):9243–9275\\nJi S-J, Ling Q-H, Han F (2023) An improved algorithm for small object detection based on YOLO v4 and \\nmulti-scale contextual information. Comput Electr Eng 105:108490\\nFang W, Wang L, Ren P (2019) Tinier-YOLO: a real-time object detection method for constrained environ-\\nments. IEEE Access 8:1935–1944\\nYe R, Gao Q, Qian Y , Sun J, Li T (2024) Improved YOLOv8 and Sahi model for the collaborative detection \\nof small targets at the micro scale: a case study of pest detection in tea. Agronomy 14(5):1034\\n1 3\\nPage 81 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6ef9d3ce-f1ba-4540-bd26-26f7f9714a9d', embedding=None, metadata={'page_label': '82', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"R. Sapkota et al.\\nOlorunshola OE, Irhebhude ME, Evwiekpaefe AE (2023) A comparative study of YOLOv5 and YOLOv7 \\nobject detection algorithms. J Comput Soc Inform 2(1):1–12\\nLi N, Wang M, Yang G, Li B, Yuan B, Xu S (2023) Dens-YOLOv6: A small object detection model for gar-\\nbage detection on water surface. Multimed Tools Appl 1–21\\nJung H-K, Choi G-S (2022) Improved YOLOv5: efficient object detection using drone images under various \\nconditions. Appl Sci 12(14):7255\\nWang A, Peng T, Cao H, Xu Y , Wei X, Cui B (2022) TIA-YOLOv5: an improved YOLOv5 network for real-\\ntime detection of crop and weed in the field. Front Plant Sci 13:1091655\\nWu T-H, Wang T-W, Liu Y-Q (2021) Real-time vehicle and distance detection based on improved YOLO v5 \\nnetwork. In: 2021 3rd World Symposium on Artificial Intelligence (WSAI), pp 24–28. IEEE\\nAli MAM, Aly T, Raslan AT, Gheith M, Amin EA (2024) Advancing crowd object detection: a review of \\nYOLO, CNN and vits hybrid approach. J Intell Learn Syst Appl 16(3):175–221\\nPfeifer R, Iida F (2004) Embodied artificial intelligence: Trends and challenges. Lecture Notes Comput Sci \\n1–26\\nSanket NJ (2021) Active vision based embodied-AI design for nano-UA V autonomy. PhD thesis, University \\nof Maryland, College Park\\nWang T, Zheng P, Li S, Wang L (2024) Multimodal human-robot interaction for human-centric smart manu-\\nfacturing: a survey. Adv Intell Syst 6(3):2300359\\nLakshmipathy A, Vardhineedi M, Sekharamahanthi VRP, Patel DD, Saini S, Mohammed S (2024) Medicap-\\ntion: Integrating YOLO-driven computer vision and NLP for advanced pharmaceutical package recog-\\nnition and annotation. Authorea Preprints\\nLi C, Zhang R, Wong J, Gokmen C, Srivastava S, Martín-Martín R, Wang C, Levine G, Ai W, Martinez B, \\net al (2024) Behavior-1k: A human-centered, embodied ai benchmark with 1000 everyday activities and \\nrealistic simulation. Preprint at arXiv:2403.09227\\nPande AK, Brantley P, Tanveer MH, V oicu RC (2024) From AI to AGI-the evolution of real-time systems \\nwith GPT integration. In: SoutheastCon 2024, pp 699–707. IEEE\\nQu Y , Wei C, Du P, Che W, Zhang C, Ouyang W, Bian Y , Xu F, Hu B, Du K, Wu H, Liu J, Liu Q (2024) Inte-\\ngration of cognitive tasks into artificial general intelligence test for large models. iScience 27\\nRouhi A, Patiño D, Han DK (2025) Enhancing object detection by leveraging large language models for con-\\ntextual knowledge. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial \\nIntelligence and Lecture Notes in Bioinformatics) 15317 LNCS, 299–314,  h t t p s : / / d o i . o r g / 1 0 . 1 0 0 7 / 9 7 \\n8 - 3 - 0 3 1 - 7 8 4 4 7 - 7 _ 2 0       \\nSapkota R, Paudel A, Karkee M (2024) Zero-shot automatic annotation and instance segmentation using \\nLLM-generated datasets: Eliminating field imaging and manual annotation for deep learning model \\ndevelopment. Preprint at arXiv:2411.11285\\nXu R, Ji K, Yuan Z, Wang C, Xia Y (2024) Exploring the evolution trend of China’s digital carbon footprint: \\na simulation based on system dynamics approach. Sustainability (Switzerland).  h t t p s : / / d o i . o r g / 1 0 . 3 3 9 \\n0 / s u 1 6 1 0 4 2 3 0       \\nDhar P (2020) The carbon impact of artificial intelligence. Nat Mach Intell.  h t t p s : / / d o i . o r g / 1 0 . 1 0 3 8 / s 4 2 2 5 \\n6 - 0 2 0 - 0 2 1 9 - 9       \\nPublisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \\ninstitutional affiliations.\\nAuthors and Affiliations\\nRanjan\\xa0Sapkota1 \\xa0· Marco\\xa0Flores-Calero2\\xa0· Rizwan\\xa0Qureshi3\\xa0· Chetan\\xa0Badgujar4\\xa0· \\nUpesh\\xa0Nepal5\\xa0· Alwin\\xa0Poulose6\\xa0· Peter\\xa0Zeno7\\xa0· Uday Bhanu Prakash\\xa0Vaddevolu8\\xa0· \\nSheheryar\\xa0Khan9\\xa0· Maged\\xa0Shoman10\\xa0· Hong\\xa0Yan11,12\\xa0· Manoj\\xa0Karkee1,13\\n \\r Ranjan Sapkota\\nrs2672@cornell.edu\\n \\r Manoj Karkee\\nmk2684@cornell.edu\\n1 Biological & Environmental Engineering, Cornell University, Ithaca, New York, USA\\n1 3\\n  274  Page 82 of 83\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8f7f1545-be05-48d9-a9c6-865a34352c51', embedding=None, metadata={'page_label': '83', 'file_name': 'YOLO.pdf', 'file_path': 'C:\\\\Users\\\\namnh\\\\Downloads\\\\data\\\\YOLO.pdf', 'file_type': 'application/pdf', 'file_size': 8735084, 'creation_date': '2025-10-02', 'last_modified_date': '2025-10-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='YOLO advances to its genesis: a decadal and comprehensive review of…\\n2 Department of Electrical, Electronics and Telecommunications, Universidad de las Fuerzas \\nArmadas ESPE, Av. General Rumiñahui s/n, Sangolquí 171-5-231B, Ecuador\\n3 Center for Research in Computer Vision, University of Central Florida, Orlando, FL, USA\\n4 Biosystems Engineering and Soil Sciences, The University of Tennessee, Knoxville,  \\nTN 37996, USA\\n5 Cooper Machine Company, Inc., Wadley, GA 30477, USA\\n6 School of Data Science, Indian Institute of Science Education and Research \\nThiruvananthapuram (IISER TVM), Thiruvananthapuram, Kerala 695551, India\\n7 ZenoRobotics, LLC, Billings, MT 59106, USA\\n8 Biological and Agricultural Engineering, Texas A&M University, College Station, TX  \\n77840, USA\\n9 School of Professional Education and Executive Development, The Hong Kong Polytechnic \\nUniversity, Hong Kong 999077, SAR China, China\\n10 University of Tennessee, Knoxville, TN, USA\\n11 Department of Electrical Engineering, City University of Hong Kong, Kowloon,  \\nHong Kong 999077, SAR China, China\\n12 Center for Intelligent Multidimensional Data Analysis, CIMDA, Hong Kong Science Park, \\nHong Kong, SAR China, China\\n13 Biological Systems Engineering, Washington State University, Prosser Campus, Pullman, WA, \\nUSA\\n1 3\\nPage 83 of 83   274 ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f610ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing nodes: 100%|██████████| 98/98 [00:00<00:00, 1116.69it/s]\n",
      "\n",
      "\u001b[A2025-10-02 10:56:35,435 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-10-02 10:56:35,435 - INFO - Retrying request to /embeddings in 0.416948 seconds\n",
      "2025-10-02 10:56:36,658 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-10-02 10:56:36,662 - INFO - Retrying request to /embeddings in 0.907045 seconds\n",
      "2025-10-02 10:56:37,876 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 401 Unauthorized\"\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************w90A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\core\\indices\\base.py:122\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mid_, doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[0;32m    115\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[0;32m    116\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     transformations,\n\u001b[0;32m    118\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    120\u001b[0m )\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    123\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    124\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m    125\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m    126\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    127\u001b[0m     transformations\u001b[38;5;241m=\u001b[39mtransformations,\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    129\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:75\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m resolve_embed_model(\n\u001b[0;32m     71\u001b[0m     embed_model \u001b[38;5;129;01mor\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39membed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     76\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     77\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     78\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m     79\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m     80\u001b[0m     objects\u001b[38;5;241m=\u001b[39mobjects,\n\u001b[0;32m     81\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m     82\u001b[0m     transformations\u001b[38;5;241m=\u001b[39mtransformations,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     84\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\core\\indices\\base.py:79\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[1;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m---> 79\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_index_from_nodes(\n\u001b[0;32m     80\u001b[0m         nodes \u001b[38;5;241m+\u001b[39m objects,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:309\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[1;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content_nodes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(nodes):\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome nodes are missing content, skipping them...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index_from_nodes(content_nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:278\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[1;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m     run_async_tasks(tasks)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_nodes_to_index(\n\u001b[0;32m    279\u001b[0m         index_struct,\n\u001b[0;32m    280\u001b[0m         nodes,\n\u001b[0;32m    281\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_progress,\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:231\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[1;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[1;32m--> 231\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:138\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[1;34m(self, nodes, show_progress)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    128\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[0;32m    129\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    130\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\core\\indices\\utils.py:176\u001b[0m, in \u001b[0;36membed_nodes\u001b[1;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[1;32m--> 176\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[0;32m    181\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    332\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    337\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    338\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\core\\base\\embeddings\\base.py:473\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[1;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    469\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[0;32m    470\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[0;32m    471\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_cache:\n\u001b[1;32m--> 473\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_text_embeddings_cached(cur_batch)\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:472\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_retryable_get_embeddings\u001b[39m():\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_embeddings(\n\u001b[0;32m    466\u001b[0m         client,\n\u001b[0;32m    467\u001b[0m         texts,\n\u001b[0;32m    468\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_engine,\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retryable_get_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\tenacity\\__init__.py:338\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    336\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    337\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\tenacity\\__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\tenacity\\__init__.py:400\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\tenacity\\__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:465\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings\u001b[1;34m()\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_retryable_get_embeddings\u001b[39m():\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_embeddings(\n\u001b[0;32m    466\u001b[0m         client,\n\u001b[0;32m    467\u001b[0m         texts,\n\u001b[0;32m    468\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_engine,\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs,\n\u001b[0;32m    470\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:172\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(client, list_of_text, engine, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch size should not be larger than 2048.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m list_of_text \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[1;32m--> 172\u001b[0m data \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlist_of_text, model\u001b[38;5;241m=\u001b[39mengine, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [d\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    127\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\namnh\\miniconda3\\envs\\ChatBoxAI\\lib\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************w90A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc5a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatBoxAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
